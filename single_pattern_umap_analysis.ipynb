{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Cognitive Pattern UMAP + HDBSCAN Analysis\n",
    "\n",
    "This notebook performs UMAP dimensionality reduction and HDBSCAN clustering analysis on a single cognitive pattern.\n",
    "The key improvements over the previous analysis:\n",
    "- **Clustering within states only**: Separate clustering for positive, negative, and transition states\n",
    "- **Single cognitive pattern focus**: Analyze one pattern at a time (e.g., 'Executive Fatigue & Avolition')\n",
    "- **Token sampling options**: All tokens vs last token only\n",
    "- **Sample size options**: All samples vs single sample\n",
    "\n",
    "## Analysis Variants:\n",
    "1. All token positions + All samples\n",
    "2. Last token only + All samples  \n",
    "3. All token positions + Single sample\n",
    "4. Last token only + Single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import umap\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "import webbrowser\n",
    "import os\n",
    "import hdbscan\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure Plotly for browser display\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"browser\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Single Cognitive Pattern UMAP + HDBSCAN Analysis\n",
      "============================================================\n",
      "Loading activation data and metadata...\n",
      "Loaded data for 520 examples\n",
      "Device: mps\n",
      "Negative activations keys: ['negative_layer_17', 'negative_layer_21', 'enriched_metadata']...\n",
      "Positive activations keys: ['positive_layer_17', 'positive_layer_21', 'enriched_metadata']...\n",
      "Transition activations keys: ['transition_layer_17', 'transition_layer_21', 'enriched_metadata']...\n"
     ]
    }
   ],
   "source": [
    "print(\"🔍 Single Cognitive Pattern UMAP + HDBSCAN Analysis\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Loading activation data and metadata...\")\n",
    "\n",
    "# Set device and paths (same as original notebook)\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "base_path = Path(\"/Users/ivanculo/Desktop/Projects/turn_point\")\n",
    "activations_dir = base_path / \"activations\"\n",
    "\n",
    "# Load data (same as original notebook)\n",
    "negative_activations = torch.load(activations_dir / \"activations_8ff00d963316212d.pt\", map_location=device)\n",
    "positive_activations = torch.load(activations_dir / \"activations_e5ad16e9b3c33c9b.pt\", map_location=device)\n",
    "transition_activations = torch.load(activations_dir / \"activations_332f24de2a3f82ff.pt\", map_location=device)\n",
    "\n",
    "with open(base_path / \"data\" / \"final\" / \"enriched_metadata.json\", 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(f\"Loaded data for {len(metadata)} examples\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Negative activations keys: {list(negative_activations.keys())[:3]}...\")\n",
    "print(f\"Positive activations keys: {list(positive_activations.keys())[:3]}...\")\n",
    "print(f\"Transition activations keys: {list(transition_activations.keys())[:3]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Available Cognitive Patterns:\n",
      "  Executive Fatigue & Avolition: 40 examples\n",
      "  Persistent Suicidal Ideation Focus: 40 examples\n",
      "  Self-Critical Rumination: 40 examples\n",
      "  Conflict-Focused Self-Reflection: 40 examples\n",
      "  Disorganized Thought & Derealization: 40 examples\n",
      "  Somatic–Emotional Self-Monitoring: 40 examples\n",
      "  Identity-Focused Life Narrative: 40 examples\n",
      "  Overwhelmed Narrative Processing: 40 examples\n",
      "  Overload with Entrapment Themes: 40 examples\n",
      "  Existential Overload & Worthlessness: 40 examples\n",
      "  Hopelessness-Driven Cognitive Exhaustion: 40 examples\n",
      "  Suicidal Planning & Rationalization: 40 examples\n",
      "  Fragmented Overwhelm & Exhaustion: 40 examples\n",
      "\n",
      "🎯 Selected pattern: Conflict-Focused Self-Reflection\n",
      "   Number of examples: 40\n",
      "   Analyzing layer: 17\n"
     ]
    }
   ],
   "source": [
    "# Create pattern indices mapping\n",
    "pattern_indices = {}\n",
    "for i, entry in enumerate(metadata):\n",
    "    pattern_name = entry['bad_good_narratives_match']['cognitive_pattern_name_from_bad_good']\n",
    "    if pattern_name not in pattern_indices:\n",
    "        pattern_indices[pattern_name] = []\n",
    "    pattern_indices[pattern_name].append(i)\n",
    "\n",
    "# Set layer to analyze (fixed to layer 17, same as original notebook)\n",
    "layer = 17\n",
    "\n",
    "print(\"\\n📊 Available Cognitive Patterns:\")\n",
    "for pattern, indices in pattern_indices.items():\n",
    "    print(f\"  {pattern}: {len(indices)} examples\")\n",
    "\n",
    "# Select pattern to analyze\n",
    "selected_pattern = 'Conflict-Focused Self-Reflection'  # Change this to analyze different patterns\n",
    "print(f\"\\n🎯 Selected pattern: {selected_pattern}\")\n",
    "print(f\"   Number of examples: {len(pattern_indices[selected_pattern])}\")\n",
    "print(f\"   Analyzing layer: {layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_single_pattern_analysis(\n",
    "    pattern_name,\n",
    "    all_tokens=True,\n",
    "    all_samples=True,\n",
    "    single_sample_idx=0,\n",
    "    min_cluster_size=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform UMAP + HDBSCAN analysis on a single cognitive pattern\n",
    "    \n",
    "    Args:\n",
    "        pattern_name: Name of cognitive pattern to analyze\n",
    "        all_tokens: If True, use all token positions. If False, use last token only\n",
    "        all_samples: If True, use all samples. If False, use single sample\n",
    "        single_sample_idx: Which sample to use if all_samples=False\n",
    "        min_cluster_size: Minimum cluster size for HDBSCAN\n",
    "    \"\"\"\n",
    "    \n",
    "    if pattern_name not in pattern_indices:\n",
    "        print(f\"❌ Pattern '{pattern_name}' not found\")\n",
    "        return None\n",
    "    \n",
    "    # Configuration string for titles\n",
    "    tokens_str = \"AllTokens\" if all_tokens else \"LastToken\"\n",
    "    samples_str = \"AllSamples\" if all_samples else f\"Sample{single_sample_idx}\"\n",
    "    config_str = f\"{tokens_str}_{samples_str}\"\n",
    "    \n",
    "    print(f\"\\n🚀 Analysis Configuration: {config_str}\")\n",
    "    print(f\"   Pattern: {pattern_name}\")\n",
    "    print(f\"   Layer: {layer} (fixed)\")\n",
    "    print(f\"   Token sampling: {'All tokens' if all_tokens else 'Last token only'}\")\n",
    "    print(f\"   Sample selection: {'All samples' if all_samples else f'Single sample {single_sample_idx}'}\")\n",
    "    print(f\"   Min cluster size: {min_cluster_size}\")\n",
    "    \n",
    "    # Get indices for this pattern\n",
    "    indices = pattern_indices[pattern_name]\n",
    "    if not all_samples:\n",
    "        if single_sample_idx >= len(indices):\n",
    "            print(f\"❌ Sample index {single_sample_idx} out of range (max: {len(indices)-1})\")\n",
    "            return None\n",
    "        indices = [indices[single_sample_idx]]\n",
    "    \n",
    "    print(f\"   Using {len(indices)} sample(s)\")\n",
    "    \n",
    "    # Load activations for this pattern (layer 17 only)\n",
    "    neg_data = negative_activations[f'negative_layer_{layer}'][indices]\n",
    "    pos_data = positive_activations[f'positive_layer_{layer}'][indices] \n",
    "    trans_data = transition_activations[f'transition_layer_{layer}'][indices]\n",
    "    \n",
    "    print(f\"   Data shapes - Neg: {neg_data.shape}, Pos: {pos_data.shape}, Trans: {trans_data.shape}\")\n",
    "    \n",
    "    # Prepare data based on token sampling strategy\n",
    "    def prepare_state_data(data, state_name):\n",
    "        if all_tokens:\n",
    "            # Use all tokens from all samples\n",
    "            flat_data = data.reshape(-1, data.shape[-1])\n",
    "            print(f\"     {state_name}: {data.shape} -> {flat_data.shape} (all tokens)\")\n",
    "        else:\n",
    "            # Use only last token from each sample\n",
    "            last_tokens = data[:, -1, :]\n",
    "            flat_data = last_tokens.reshape(-1, last_tokens.shape[-1])\n",
    "            print(f\"     {state_name}: {data.shape} -> {flat_data.shape} (last tokens only)\")\n",
    "        return flat_data.cpu().numpy()  # Move to CPU before converting to numpy\n",
    "    \n",
    "    # Prepare data for each state\n",
    "    print(f\"\\n📊 Preparing data for clustering...\")\n",
    "    neg_flat = prepare_state_data(neg_data, \"Negative\")\n",
    "    pos_flat = prepare_state_data(pos_data, \"Positive\")\n",
    "    trans_flat = prepare_state_data(trans_data, \"Transition\")\n",
    "    \n",
    "    return perform_clustering_and_visualization(\n",
    "        neg_flat, pos_flat, trans_flat, \n",
    "        pattern_name, config_str, min_cluster_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering_and_visualization(\n",
    "    neg_flat, pos_flat, trans_flat, \n",
    "    pattern_name, config_str, min_cluster_size\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform HDBSCAN clustering within each state separately and create UMAP visualization\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n🔬 HDBSCAN Clustering Analysis (within states only)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    states_data = {\n",
    "        'Negative': neg_flat,\n",
    "        'Positive': pos_flat,\n",
    "        'Transition': trans_flat\n",
    "    }\n",
    "    \n",
    "    # Standardize data\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Perform HDBSCAN clustering on each state separately\n",
    "    clustering_results = {}\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    all_colors = []\n",
    "    all_detailed_labels = []\n",
    "    \n",
    "    # Color schemes for each state\n",
    "    color_schemes = {\n",
    "        'Negative': ['#8B0000', '#DC143C', '#B22222', '#FF6B6B', '#FF8E8E'],\n",
    "        'Positive': ['#006400', '#228B22', '#32CD32', '#7CFC00', '#ADFF2F'], \n",
    "        'Transition': ['#4B0082', '#8A2BE2', '#9370DB', '#BA55D3', '#DDA0DD']\n",
    "    }\n",
    "    \n",
    "    for state_name, data in states_data.items():\n",
    "        print(f\"\\n🎯 Clustering {state_name} state ({data.shape[0]} samples)\")\n",
    "        \n",
    "        if data.shape[0] < min_cluster_size:\n",
    "            print(f\"   ⚠️  Skipping {state_name} - insufficient samples ({data.shape[0]} < {min_cluster_size})\")\n",
    "            continue\n",
    "        \n",
    "        # Standardize data\n",
    "        data_scaled = scaler.fit_transform(data)\n",
    "        \n",
    "        # HDBSCAN clustering\n",
    "        clusterer = hdbscan.HDBSCAN(\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            min_samples=max(5, min_cluster_size // 4),\n",
    "            cluster_selection_epsilon=0.0,\n",
    "            metric='euclidean'\n",
    "        )\n",
    "        \n",
    "        cluster_labels = clusterer.fit_predict(data_scaled)\n",
    "        \n",
    "        # Analyze clustering results\n",
    "        unique_labels, counts = np.unique(cluster_labels, return_counts=True)\n",
    "        n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
    "        n_noise = np.sum(cluster_labels == -1)\n",
    "        \n",
    "        print(f\"   Found {n_clusters} clusters, {n_noise} noise points ({n_noise/len(cluster_labels)*100:.1f}%)\")\n",
    "        \n",
    "        for label, count in zip(unique_labels, counts):\n",
    "            pct = count / len(cluster_labels) * 100\n",
    "            if label == -1:\n",
    "                print(f\"     Noise: {count:4d} samples ({pct:5.1f}%)\")\n",
    "            else:\n",
    "                print(f\"     Cluster {label}: {count:4d} samples ({pct:5.1f}%)\")\n",
    "        \n",
    "        # Store clustering results\n",
    "        clustering_results[state_name] = {\n",
    "            'data': data,\n",
    "            'data_scaled': data_scaled,\n",
    "            'cluster_labels': cluster_labels,\n",
    "            'clusterer': clusterer,\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_noise': n_noise\n",
    "        }\n",
    "        \n",
    "        # UMAP embedding for this state\n",
    "        print(f\"   Computing UMAP embedding...\")\n",
    "        n_neighbors = min(15, max(2, len(data) // 10))\n",
    "        \n",
    "        umap_reducer = umap.UMAP(\n",
    "            n_components=2,\n",
    "            n_neighbors=n_neighbors,\n",
    "            min_dist=0.1,\n",
    "            random_state=42,\n",
    "            n_jobs=1\n",
    "        )\n",
    "        \n",
    "        embedding = umap_reducer.fit_transform(data_scaled)\n",
    "        all_embeddings.append(embedding)\n",
    "        \n",
    "        # Create labels and colors\n",
    "        state_colors = color_schemes[state_name]\n",
    "        for i, label in enumerate(cluster_labels):\n",
    "            if label == -1:\n",
    "                all_labels.append(f\"{state_name}_Noise\")\n",
    "                all_colors.append('#808080')  # Gray for noise\n",
    "                all_detailed_labels.append(f\"{state_name} Noise\")\n",
    "            else:\n",
    "                all_labels.append(f\"{state_name}_Cluster_{label}\")\n",
    "                all_colors.append(state_colors[label % len(state_colors)])\n",
    "                all_detailed_labels.append(f\"{state_name} Cluster {label}\")\n",
    "    \n",
    "    # Combine all embeddings\n",
    "    if not all_embeddings:\n",
    "        print(\"❌ No valid clusterings found\")\n",
    "        return None\n",
    "    \n",
    "    combined_embedding = np.vstack(all_embeddings)\n",
    "    \n",
    "    print(f\"\\n🗺️  Creating visualization with {len(combined_embedding)} total points...\")\n",
    "    \n",
    "    # Create visualization\n",
    "    results = {\n",
    "        'embedding': combined_embedding,\n",
    "        'labels': all_labels,\n",
    "        'colors': all_colors,\n",
    "        'detailed_labels': all_detailed_labels,\n",
    "        'clustering_results': clustering_results,\n",
    "        'pattern_name': pattern_name,\n",
    "        'config_str': config_str\n",
    "    }\n",
    "    \n",
    "    create_visualization(results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualization(results):\n",
    "    \"\"\"\n",
    "    Create interactive UMAP visualization with HDBSCAN clustering results\n",
    "    \"\"\"\n",
    "    \n",
    "    embedding = results['embedding']\n",
    "    labels = results['labels']\n",
    "    colors = results['colors']\n",
    "    detailed_labels = results['detailed_labels']\n",
    "    pattern_name = results['pattern_name']\n",
    "    config_str = results['config_str']\n",
    "    \n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Group points by label for legend organization\n",
    "    unique_labels = list(set(labels))\n",
    "    unique_labels.sort()\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        mask = [l == label for l in labels]\n",
    "        if not any(mask):\n",
    "            continue\n",
    "            \n",
    "        indices = [i for i, m in enumerate(mask) if m]\n",
    "        \n",
    "        # Extract state and cluster info for display\n",
    "        parts = label.split('_')\n",
    "        state = parts[0]\n",
    "        \n",
    "        if 'Noise' in label:\n",
    "            display_name = f\"{state} (Noise)\"\n",
    "            marker_symbol = 'x'\n",
    "            marker_size = 3\n",
    "        else:\n",
    "            cluster_id = parts[-1]\n",
    "            display_name = f\"{state} C{cluster_id}\"\n",
    "            marker_symbol = 'circle'\n",
    "            marker_size = 4\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=embedding[indices, 0],\n",
    "            y=embedding[indices, 1],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                color=colors[indices[0]],\n",
    "                size=marker_size,\n",
    "                opacity=0.7,\n",
    "                symbol=marker_symbol,\n",
    "                line=dict(width=0.5, color='white')\n",
    "            ),\n",
    "            name=display_name,\n",
    "            hovertemplate=f'<b>{display_name}</b><br>UMAP 1: %{{x:.2f}}<br>UMAP 2: %{{y:.2f}}<extra></extra>'\n",
    "        ))\n",
    "    \n",
    "    # Update layout\n",
    "    title = f'{pattern_name} - {config_str} UMAP + HDBSCAN'\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=title,\n",
    "            x=0.5,\n",
    "            font=dict(size=16)\n",
    "        ),\n",
    "        xaxis_title='UMAP Dimension 1',\n",
    "        yaxis_title='UMAP Dimension 2',\n",
    "        width=1000,\n",
    "        height=800,\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=1.01\n",
    "        ),\n",
    "        margin=dict(r=150)\n",
    "    )\n",
    "    \n",
    "    # Save and display\n",
    "    safe_pattern = pattern_name.replace(' ', '_').replace('&', 'and')\n",
    "    filename = f\"single_pattern_{safe_pattern}_{config_str}_{hash(str(embedding.tolist())) % 10000}.html\"\n",
    "    \n",
    "    fig.write_html(filename, auto_open=False)\n",
    "    \n",
    "    print(f\"\\n📊 Visualization saved: {filename}\")\n",
    "    print(f\"   Opening in browser...\")\n",
    "    \n",
    "    try:\n",
    "        webbrowser.open(f'file://{os.path.abspath(filename)}', new=2)\n",
    "    except Exception as e:\n",
    "        print(f\"   Could not open browser: {e}\")\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_analysis_summary(results):\n",
    "    \"\"\"\n",
    "    Print summary of clustering analysis results\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n📋 Analysis Summary: {results['pattern_name']} - {results['config_str']}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    clustering_results = results['clustering_results']\n",
    "    total_points = len(results['labels'])\n",
    "    \n",
    "    print(f\"Total data points analyzed: {total_points}\")\n",
    "    \n",
    "    for state_name, cluster_info in clustering_results.items():\n",
    "        n_clusters = cluster_info['n_clusters']\n",
    "        n_noise = cluster_info['n_noise']\n",
    "        n_total = len(cluster_info['cluster_labels'])\n",
    "        \n",
    "        print(f\"\\n{state_name} State:\")\n",
    "        print(f\"  • {n_total} total points\")\n",
    "        print(f\"  • {n_clusters} clusters found\")\n",
    "        print(f\"  • {n_noise} noise points ({n_noise/n_total*100:.1f}%)\")\n",
    "        print(f\"  • {n_total - n_noise} points in clusters ({(n_total-n_noise)/n_total*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n💡 Key Insights:\")\n",
    "    print(f\"  • Clustering performed WITHIN each cognitive state separately\")\n",
    "    print(f\"  • HDBSCAN automatically determines optimal number of clusters\")\n",
    "    print(f\"  • Noise points represent outliers that don't fit clear patterns\")\n",
    "    print(f\"  • Cluster separation indicates distinct neural activation patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 1: All Token Positions + All Samples\n",
    "\n",
    "This analysis uses all token positions from all samples of the selected cognitive pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🔍 ANALYSIS 1: All Token Positions + All Samples\n",
      "================================================================================\n",
      "\n",
      "🚀 Analysis Configuration: AllTokens_AllSamples\n",
      "   Pattern: Conflict-Focused Self-Reflection\n",
      "   Layer: 17 (fixed)\n",
      "   Token sampling: All tokens\n",
      "   Sample selection: All samples\n",
      "   Min cluster size: 30\n",
      "   Using 40 sample(s)\n",
      "   Data shapes - Neg: torch.Size([40, 208, 2304]), Pos: torch.Size([40, 261, 2304]), Trans: torch.Size([40, 311, 2304])\n",
      "\n",
      "📊 Preparing data for clustering...\n",
      "     Negative: torch.Size([40, 208, 2304]) -> torch.Size([8320, 2304]) (all tokens)\n",
      "     Positive: torch.Size([40, 261, 2304]) -> torch.Size([10440, 2304]) (all tokens)\n",
      "     Transition: torch.Size([40, 311, 2304]) -> torch.Size([12440, 2304]) (all tokens)\n",
      "\n",
      "🔬 HDBSCAN Clustering Analysis (within states only)\n",
      "============================================================\n",
      "\n",
      "🎯 Clustering Negative state (8320 samples)\n",
      "   Found 6 clusters, 2161 noise points (26.0%)\n",
      "     Noise: 2161 samples ( 26.0%)\n",
      "     Cluster 0:   37 samples (  0.4%)\n",
      "     Cluster 1:   42 samples (  0.5%)\n",
      "     Cluster 2:   58 samples (  0.7%)\n",
      "     Cluster 3:   40 samples (  0.5%)\n",
      "     Cluster 4:   40 samples (  0.5%)\n",
      "     Cluster 5: 5942 samples ( 71.4%)\n",
      "   Computing UMAP embedding...\n",
      "\n",
      "🎯 Clustering Positive state (10440 samples)\n",
      "   Found 14 clusters, 3285 noise points (31.5%)\n",
      "     Noise: 3285 samples ( 31.5%)\n",
      "     Cluster 0:   30 samples (  0.3%)\n",
      "     Cluster 1:   34 samples (  0.3%)\n",
      "     Cluster 2:   33 samples (  0.3%)\n",
      "     Cluster 3:   72 samples (  0.7%)\n",
      "     Cluster 4:   45 samples (  0.4%)\n",
      "     Cluster 5:   50 samples (  0.5%)\n",
      "     Cluster 6:   80 samples (  0.8%)\n",
      "     Cluster 7:   31 samples (  0.3%)\n",
      "     Cluster 8:   44 samples (  0.4%)\n",
      "     Cluster 9:   40 samples (  0.4%)\n",
      "     Cluster 10:   82 samples (  0.8%)\n",
      "     Cluster 11:   40 samples (  0.4%)\n",
      "     Cluster 12: 6525 samples ( 62.5%)\n",
      "     Cluster 13:   49 samples (  0.5%)\n",
      "   Computing UMAP embedding...\n",
      "\n",
      "🎯 Clustering Transition state (12440 samples)\n",
      "   Found 4 clusters, 3621 noise points (29.1%)\n",
      "     Noise: 3621 samples ( 29.1%)\n",
      "     Cluster 0:   40 samples (  0.3%)\n",
      "     Cluster 1:   40 samples (  0.3%)\n",
      "     Cluster 2: 8708 samples ( 70.0%)\n",
      "     Cluster 3:   31 samples (  0.2%)\n",
      "   Computing UMAP embedding...\n",
      "\n",
      "🗺️  Creating visualization with 31200 total points...\n",
      "\n",
      "📊 Visualization saved: single_pattern_Conflict-Focused_Self-Reflection_AllTokens_AllSamples_4643.html\n",
      "   Opening in browser...\n",
      "\n",
      "📋 Analysis Summary: Conflict-Focused Self-Reflection - AllTokens_AllSamples\n",
      "============================================================\n",
      "Total data points analyzed: 31200\n",
      "\n",
      "Negative State:\n",
      "  • 8320 total points\n",
      "  • 6 clusters found\n",
      "  • 2161 noise points (26.0%)\n",
      "  • 6159 points in clusters (74.0%)\n",
      "\n",
      "Positive State:\n",
      "  • 10440 total points\n",
      "  • 14 clusters found\n",
      "  • 3285 noise points (31.5%)\n",
      "  • 7155 points in clusters (68.5%)\n",
      "\n",
      "Transition State:\n",
      "  • 12440 total points\n",
      "  • 4 clusters found\n",
      "  • 3621 noise points (29.1%)\n",
      "  • 8819 points in clusters (70.9%)\n",
      "\n",
      "💡 Key Insights:\n",
      "  • Clustering performed WITHIN each cognitive state separately\n",
      "  • HDBSCAN automatically determines optimal number of clusters\n",
      "  • Noise points represent outliers that don't fit clear patterns\n",
      "  • Cluster separation indicates distinct neural activation patterns\n"
     ]
    }
   ],
   "source": [
    "# Analysis 1: All tokens, all samples\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🔍 ANALYSIS 1: All Token Positions + All Samples\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_all_tokens_all_samples = perform_single_pattern_analysis(\n",
    "    pattern_name=selected_pattern,\n",
    "    all_tokens=True,\n",
    "    all_samples=True,\n",
    "    min_cluster_size=30\n",
    ")\n",
    "\n",
    "print_analysis_summary(results_all_tokens_all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "💾 SAVING & ANALYZING ANALYSIS 1 RESULTS\n",
      "================================================================================\n",
      "🔬 Running cluster analysis on Analysis 1 results...\n",
      "Loading activation data and metadata...\n",
      "Loaded data for 520 examples\n",
      "Found 13 cognitive patterns\n",
      "Using device: mps\n",
      "\n",
      "================================================================================\n",
      "🔬 COMPLETE CLUSTER ANALYSIS PIPELINE\n",
      "================================================================================\n",
      "\n",
      "🔍 Analyzing cluster activations for Conflict-Focused Self-Reflection\n",
      "   Layer: 17\n",
      "\n",
      "📊 Analyzing Negative state clusters:\n",
      "   Cluster 0: 37 points, magnitude: 268.09\n",
      "   Cluster 1: 42 points, magnitude: 246.57\n",
      "   Cluster 2: 58 points, magnitude: 254.22\n",
      "   Cluster 3: 40 points, magnitude: 569.35\n",
      "   Cluster 4: 40 points, magnitude: 3502.02\n",
      "   Cluster 5: 5942 points, magnitude: 0.51\n",
      "\n",
      "📊 Analyzing Positive state clusters:\n",
      "   Cluster 0: 30 points, magnitude: 248.16\n",
      "   Cluster 1: 34 points, magnitude: 342.39\n",
      "   Cluster 2: 33 points, magnitude: 265.45\n",
      "   Cluster 3: 72 points, magnitude: 286.24\n",
      "   Cluster 4: 45 points, magnitude: 278.84\n",
      "   Cluster 5: 50 points, magnitude: 268.29\n",
      "   Cluster 6: 80 points, magnitude: 239.17\n",
      "   Cluster 7: 31 points, magnitude: 255.86\n",
      "   Cluster 8: 44 points, magnitude: 243.12\n",
      "   Cluster 9: 40 points, magnitude: 637.42\n",
      "   Cluster 10: 82 points, magnitude: 251.44\n",
      "   Cluster 11: 40 points, magnitude: 3502.02\n",
      "   Cluster 12: 6525 points, magnitude: 0.00\n",
      "   Cluster 13: 49 points, magnitude: 211.11\n",
      "\n",
      "📊 Analyzing Transition state clusters:\n",
      "   Cluster 0: 40 points, magnitude: 629.30\n",
      "   Cluster 1: 40 points, magnitude: 3502.02\n",
      "   Cluster 2: 8708 points, magnitude: 0.00\n",
      "   Cluster 3: 31 points, magnitude: 214.53\n",
      "\n",
      "🎯 Finding top 3 highest activating clusters per state:\n",
      "   Negative state:\n",
      "     #1: Cluster 4 (size: 40, magnitude: 3502.02)\n",
      "     #2: Cluster 3 (size: 40, magnitude: 569.35)\n",
      "     #3: Cluster 0 (size: 37, magnitude: 268.09)\n",
      "   Positive state:\n",
      "     #1: Cluster 11 (size: 40, magnitude: 3502.02)\n",
      "     #2: Cluster 9 (size: 40, magnitude: 637.42)\n",
      "     #3: Cluster 1 (size: 34, magnitude: 342.39)\n",
      "   Transition state:\n",
      "     #1: Cluster 1 (size: 40, magnitude: 3502.02)\n",
      "     #2: Cluster 0 (size: 40, magnitude: 629.30)\n",
      "     #3: Cluster 3 (size: 31, magnitude: 214.53)\n",
      "\n",
      "📋 Extracting activation IDs for cluster filtering:\n",
      "   Pattern: Conflict-Focused Self-Reflection\n",
      "   Layer: 17\n",
      "   Token strategy: All tokens\n",
      "\n",
      "   Negative state:\n",
      "     Data shape: torch.Size([40, 208, 2304])\n",
      "     Cluster 4: 40 activations\n",
      "     Cluster 3: 40 activations\n",
      "     Cluster 0: 37 activations\n",
      "   Total Negative activations: 117\n",
      "\n",
      "   Positive state:\n",
      "     Data shape: torch.Size([40, 261, 2304])\n",
      "     Cluster 11: 40 activations\n",
      "     Cluster 9: 40 activations\n",
      "     Cluster 1: 34 activations\n",
      "   Total Positive activations: 114\n",
      "\n",
      "   Transition state:\n",
      "     Data shape: torch.Size([40, 311, 2304])\n",
      "     Cluster 1: 40 activations\n",
      "     Cluster 0: 40 activations\n",
      "     Cluster 3: 31 activations\n",
      "   Total Transition activations: 111\n",
      "\n",
      "🔧 Creating activation filter structure:\n",
      "   Negative: 40 unique samples, 117 total activations\n",
      "   Positive: 40 unique samples, 114 total activations\n",
      "   Transition: 40 unique samples, 111 total activations\n",
      "\n",
      "🎯 TOP CLUSTERS BY ACTIVATION MAGNITUDE:\n",
      "\n",
      "Negative state:\n",
      "  #1: Cluster 4 - Magnitude: 3502.02, Size: 40 points\n",
      "  #2: Cluster 3 - Magnitude: 569.35, Size: 40 points\n",
      "  #3: Cluster 0 - Magnitude: 268.09, Size: 37 points\n",
      "\n",
      "Positive state:\n",
      "  #1: Cluster 11 - Magnitude: 3502.02, Size: 40 points\n",
      "  #2: Cluster 9 - Magnitude: 637.42, Size: 40 points\n",
      "  #3: Cluster 1 - Magnitude: 342.39, Size: 34 points\n",
      "\n",
      "Transition state:\n",
      "  #1: Cluster 1 - Magnitude: 3502.02, Size: 40 points\n",
      "  #2: Cluster 0 - Magnitude: 629.30, Size: 40 points\n",
      "  #3: Cluster 3 - Magnitude: 214.53, Size: 31 points\n",
      "\n",
      "📋 ACTIVATION FILTERS CREATED:\n",
      "  Negative: 117 activations from 40 unique samples\n",
      "  Positive: 114 activations from 40 unique samples\n",
      "  Transition: 111 activations from 40 unique samples\n",
      "\n",
      "💾 Cluster analysis saved to: cluster_analysis_Conflict-Focused_Self-Reflection_Analysis1.pkl\n",
      "✅ Analysis 1 results processed and saved!\n"
     ]
    }
   ],
   "source": [
    "# 💾 Save and Analyze Analysis 1 Results\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"💾 SAVING & ANALYZING ANALYSIS 1 RESULTS\")  \n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Import the cluster analysis utilities\n",
    "from cluster_analysis_utils import analyze_clustering_from_notebook_results\n",
    "\n",
    "# Set up paths for the cluster analyzer\n",
    "base_path = Path(\"/Users/ivanculo/Desktop/Projects/turn_point\")\n",
    "activations_dir = base_path / \"activations\"\n",
    "metadata_path = base_path / \"data\" / \"final\" / \"enriched_metadata.json\"\n",
    "\n",
    "# Analyze the clustering results from Analysis 1\n",
    "print(\"🔬 Running cluster analysis on Analysis 1 results...\")\n",
    "cluster_analysis = analyze_clustering_from_notebook_results(\n",
    "    results_all_tokens_all_samples,\n",
    "    activations_dir=activations_dir,\n",
    "    metadata_path=metadata_path,\n",
    "    top_k=3  # Get top 3 highest activating clusters per state\n",
    ")\n",
    "\n",
    "print(\"\\n🎯 TOP CLUSTERS BY ACTIVATION MAGNITUDE:\")\n",
    "for state, clusters in cluster_analysis['top_clusters'].items():\n",
    "    print(f\"\\n{state} state:\")\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        print(f\"  #{i+1}: Cluster {cluster['cluster_id']} - \"\n",
    "              f\"Magnitude: {cluster['activation_magnitude']:.2f}, \"\n",
    "              f\"Size: {cluster['size']} points\")\n",
    "\n",
    "print(f\"\\n📋 ACTIVATION FILTERS CREATED:\")\n",
    "for state, filter_info in cluster_analysis['activation_filter'].items():\n",
    "    print(f\"  {state}: {filter_info['activation_count']} activations from \"\n",
    "          f\"{len(filter_info['sample_ids'])} unique samples\")\n",
    "\n",
    "# Save the analysis results for later use\n",
    "import pickle\n",
    "analysis_filename = f\"cluster_analysis_{selected_pattern.replace(' ', '_').replace('&', 'and')}_Analysis1.pkl\"\n",
    "with open(analysis_filename, 'wb') as f:\n",
    "    pickle.dump(cluster_analysis, f)\n",
    "    \n",
    "print(f\"\\n💾 Cluster analysis saved to: {analysis_filename}\")\n",
    "print(\"✅ Analysis 1 results processed and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 2: Last Token Only + All Samples\n",
    "\n",
    "This analysis uses only the last token from each sample (final processing state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🔍 ANALYSIS 2: Last Token Only + All Samples\n",
      "================================================================================\n",
      "\n",
      "🚀 Analysis Configuration: LastToken_AllSamples\n",
      "   Pattern: Executive Fatigue & Avolition\n",
      "   Layer: 17 (fixed)\n",
      "   Token sampling: Last token only\n",
      "   Sample selection: All samples\n",
      "   Min cluster size: 10\n",
      "   Using 40 sample(s)\n",
      "   Data shapes - Neg: torch.Size([40, 208, 2304]), Pos: torch.Size([40, 261, 2304]), Trans: torch.Size([40, 311, 2304])\n",
      "\n",
      "📊 Preparing data for clustering...\n",
      "     Negative: torch.Size([40, 208, 2304]) -> torch.Size([40, 2304]) (last tokens only)\n",
      "     Positive: torch.Size([40, 261, 2304]) -> torch.Size([40, 2304]) (last tokens only)\n",
      "     Transition: torch.Size([40, 311, 2304]) -> torch.Size([40, 2304]) (last tokens only)\n",
      "\n",
      "🔬 HDBSCAN Clustering Analysis (within states only)\n",
      "============================================================\n",
      "\n",
      "🎯 Clustering Negative state (40 samples)\n",
      "   Found 0 clusters, 40 noise points (100.0%)\n",
      "     Noise:   40 samples (100.0%)\n",
      "   Computing UMAP embedding...\n",
      "\n",
      "🎯 Clustering Positive state (40 samples)\n",
      "   Found 0 clusters, 40 noise points (100.0%)\n",
      "     Noise:   40 samples (100.0%)\n",
      "   Computing UMAP embedding...\n",
      "\n",
      "🎯 Clustering Transition state (40 samples)\n",
      "   Found 0 clusters, 40 noise points (100.0%)\n",
      "     Noise:   40 samples (100.0%)\n",
      "   Computing UMAP embedding...\n",
      "\n",
      "🗺️  Creating visualization with 120 total points...\n",
      "\n",
      "📊 Visualization saved: single_pattern_Executive_Fatigue_and_Avolition_LastToken_AllSamples_7044.html\n",
      "   Opening in browser...\n",
      "\n",
      "📋 Analysis Summary: Executive Fatigue & Avolition - LastToken_AllSamples\n",
      "============================================================\n",
      "Total data points analyzed: 120\n",
      "\n",
      "Negative State:\n",
      "  • 40 total points\n",
      "  • 0 clusters found\n",
      "  • 40 noise points (100.0%)\n",
      "  • 0 points in clusters (0.0%)\n",
      "\n",
      "Positive State:\n",
      "  • 40 total points\n",
      "  • 0 clusters found\n",
      "  • 40 noise points (100.0%)\n",
      "  • 0 points in clusters (0.0%)\n",
      "\n",
      "Transition State:\n",
      "  • 40 total points\n",
      "  • 0 clusters found\n",
      "  • 40 noise points (100.0%)\n",
      "  • 0 points in clusters (0.0%)\n",
      "\n",
      "💡 Key Insights:\n",
      "  • Clustering performed WITHIN each cognitive state separately\n",
      "  • HDBSCAN automatically determines optimal number of clusters\n",
      "  • Noise points represent outliers that don't fit clear patterns\n",
      "  • Cluster separation indicates distinct neural activation patterns\n"
     ]
    }
   ],
   "source": [
    "# Analysis 2: Last token only, all samples\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🔍 ANALYSIS 2: Last Token Only + All Samples\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_last_token_all_samples = perform_single_pattern_analysis(\n",
    "    pattern_name=selected_pattern,\n",
    "    all_tokens=False,\n",
    "    all_samples=True,\n",
    "    min_cluster_size=10  # Smaller cluster size since we have fewer points\n",
    ")\n",
    "\n",
    "print_analysis_summary(results_last_token_all_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 3: All Token Positions + Single Sample\n",
    "\n",
    "This analysis focuses on just one example to see token-level progression within a single narrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering_and_visualization_umap_first(\n",
    "    neg_flat, pos_flat, trans_flat, \n",
    "    pattern_name, config_str, min_cluster_size\n",
    "):\n",
    "    \"\"\"\n",
    "    Alternative approach: Perform 3D UMAP first, then HDBSCAN clustering on the 3D embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n🔬 UMAP-First Analysis: 3D UMAP → HDBSCAN Clustering\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"⚡ NEW APPROACH: Dimensionality reduction first, then clustering in 3D space\")\n",
    "    \n",
    "    states_data = {\n",
    "        'Negative': neg_flat,\n",
    "        'Positive': pos_flat,\n",
    "        'Transition': trans_flat\n",
    "    }\n",
    "    \n",
    "    # Standardize data\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Perform 3D UMAP first, then HDBSCAN clustering on 3D embeddings\n",
    "    clustering_results = {}\n",
    "    all_embeddings_2d = []  # For final 2D visualization\n",
    "    all_embeddings_3d = []  # Store 3D embeddings\n",
    "    all_labels = []\n",
    "    all_colors = []\n",
    "    all_detailed_labels = []\n",
    "    \n",
    "    # Color schemes for each state\n",
    "    color_schemes = {\n",
    "        'Negative': ['#8B0000', '#DC143C', '#B22222', '#FF6B6B', '#FF8E8E'],\n",
    "        'Positive': ['#006400', '#228B22', '#32CD32', '#7CFC00', '#ADFF2F'], \n",
    "        'Transition': ['#4B0082', '#8A2BE2', '#9370DB', '#BA55D3', '#DDA0DD']\n",
    "    }\n",
    "    \n",
    "    for state_name, data in states_data.items():\n",
    "        print(f\"\\n🎯 Processing {state_name} state ({data.shape[0]} samples)\")\n",
    "        \n",
    "        if data.shape[0] < min_cluster_size:\n",
    "            print(f\"   ⚠️  Skipping {state_name} - insufficient samples ({data.shape[0]} < {min_cluster_size})\")\n",
    "            continue\n",
    "        \n",
    "        # Step 1: Standardize data\n",
    "        data_scaled = scaler.fit_transform(data)\n",
    "        \n",
    "        # Step 2: UMAP to 3D FIRST\n",
    "        print(f\"   🗺️  Computing 3D UMAP embedding...\")\n",
    "        n_neighbors = min(15, max(2, len(data) // 10))\n",
    "        \n",
    "        umap_3d = umap.UMAP(\n",
    "            n_components=3,  # 3D embedding\n",
    "            n_neighbors=n_neighbors,\n",
    "            min_dist=0.1,\n",
    "            random_state=42,\n",
    "            n_jobs=1\n",
    "        )\n",
    "        \n",
    "        embedding_3d = umap_3d.fit_transform(data_scaled)\n",
    "        print(f\"   ✅ 3D UMAP: {data_scaled.shape} → {embedding_3d.shape}\")\n",
    "        \n",
    "        # Step 3: HDBSCAN clustering on 3D embeddings\n",
    "        print(f\"   🔍 HDBSCAN clustering on 3D embedding...\")\n",
    "        clusterer = hdbscan.HDBSCAN(\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            min_samples=max(5, min_cluster_size // 4),\n",
    "            cluster_selection_epsilon=0.0,\n",
    "            metric='euclidean'\n",
    "        )\n",
    "        \n",
    "        cluster_labels = clusterer.fit_predict(embedding_3d)  # Cluster on 3D UMAP!\n",
    "        \n",
    "        # Analyze clustering results\n",
    "        unique_labels, counts = np.unique(cluster_labels, return_counts=True)\n",
    "        n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
    "        n_noise = np.sum(cluster_labels == -1)\n",
    "        \n",
    "        print(f\"   Found {n_clusters} clusters, {n_noise} noise points ({n_noise/len(cluster_labels)*100:.1f}%)\")\n",
    "        \n",
    "        for label, count in zip(unique_labels, counts):\n",
    "            pct = count / len(cluster_labels) * 100\n",
    "            if label == -1:\n",
    "                print(f\"     Noise: {count:4d} samples ({pct:5.1f}%)\")\n",
    "            else:\n",
    "                print(f\"     Cluster {label}: {count:4d} samples ({pct:5.1f}%)\")\n",
    "        \n",
    "        # Step 4: Create 2D UMAP for visualization (using same random state for consistency)\n",
    "        print(f\"   📊 Creating 2D UMAP for visualization...\")\n",
    "        umap_2d = umap.UMAP(\n",
    "            n_components=2,\n",
    "            n_neighbors=n_neighbors,\n",
    "            min_dist=0.1,\n",
    "            random_state=42,  # Same seed for consistency\n",
    "            n_jobs=1\n",
    "        )\n",
    "        \n",
    "        embedding_2d = umap_2d.fit_transform(data_scaled)\n",
    "        all_embeddings_2d.append(embedding_2d)\n",
    "        all_embeddings_3d.append(embedding_3d)\n",
    "        \n",
    "        # Store clustering results\n",
    "        clustering_results[state_name] = {\n",
    "            'data': data,\n",
    "            'data_scaled': data_scaled,\n",
    "            'embedding_3d': embedding_3d,\n",
    "            'embedding_2d': embedding_2d,\n",
    "            'cluster_labels': cluster_labels,\n",
    "            'clusterer': clusterer,\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_noise': n_noise,\n",
    "            'umap_3d': umap_3d,\n",
    "            'umap_2d': umap_2d\n",
    "        }\n",
    "        \n",
    "        # Create labels and colors based on 3D clustering\n",
    "        state_colors = color_schemes[state_name]\n",
    "        for i, label in enumerate(cluster_labels):\n",
    "            if label == -1:\n",
    "                all_labels.append(f\"{state_name}_Noise\")\n",
    "                all_colors.append('#808080')  # Gray for noise\n",
    "                all_detailed_labels.append(f\"{state_name} Noise\")\n",
    "            else:\n",
    "                all_labels.append(f\"{state_name}_Cluster_{label}\")\n",
    "                all_colors.append(state_colors[label % len(state_colors)])\n",
    "                all_detailed_labels.append(f\"{state_name} Cluster {label}\")\n",
    "    \n",
    "    # Combine all embeddings\n",
    "    if not all_embeddings_2d:\n",
    "        print(\"❌ No valid clusterings found\")\n",
    "        return None\n",
    "    \n",
    "    combined_embedding_2d = np.vstack(all_embeddings_2d)\n",
    "    combined_embedding_3d = np.vstack(all_embeddings_3d)\n",
    "    \n",
    "    print(f\"\\n🗺️  Creating visualizations with {len(combined_embedding_2d)} total points...\")\n",
    "    \n",
    "    # Create results with both 2D and 3D embeddings\n",
    "    results = {\n",
    "        'embedding_2d': combined_embedding_2d,\n",
    "        'embedding_3d': combined_embedding_3d,\n",
    "        'labels': all_labels,\n",
    "        'colors': all_colors,\n",
    "        'detailed_labels': all_detailed_labels,\n",
    "        'clustering_results': clustering_results,\n",
    "        'pattern_name': pattern_name,\n",
    "        'config_str': config_str + \"_UMAPFirst\"\n",
    "    }\n",
    "    \n",
    "    # Create both 2D and 3D visualizations\n",
    "    create_umap_first_visualizations(results)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_umap_first_visualizations(results):\n",
    "    \"\"\"\n",
    "    Create both 2D and 3D interactive visualizations for UMAP-first approach\n",
    "    \"\"\"\n",
    "    \n",
    "    embedding_2d = results['embedding_2d']\n",
    "    embedding_3d = results['embedding_3d']\n",
    "    labels = results['labels']\n",
    "    colors = results['colors']\n",
    "    detailed_labels = results['detailed_labels']\n",
    "    pattern_name = results['pattern_name']\n",
    "    config_str = results['config_str']\n",
    "    \n",
    "    # Create subplots: 2D and 3D side by side\n",
    "    from plotly.subplots import make_subplots\n",
    "    import plotly.graph_objects as go\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        column_widths=[0.5, 0.5],\n",
    "        specs=[[{\"type\": \"scatter\"}, {\"type\": \"scatter3d\"}]],\n",
    "        subplot_titles=('2D UMAP Visualization', '3D UMAP (Clustering Space)')\n",
    "    )\n",
    "    \n",
    "    # Group points by label for legend organization\n",
    "    unique_labels = list(set(labels))\n",
    "    unique_labels.sort()\n",
    "    \n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = [l == label for l in labels]\n",
    "        if not any(mask):\n",
    "            continue\n",
    "            \n",
    "        indices = [idx for idx, m in enumerate(mask) if m]\n",
    "        \n",
    "        # Extract state and cluster info for display\n",
    "        parts = label.split('_')\n",
    "        state = parts[0]\n",
    "        \n",
    "        if 'Noise' in label:\n",
    "            display_name = f\"{state} (Noise)\"\n",
    "            marker_symbol = 'x'\n",
    "            marker_size_2d = 3\n",
    "            marker_size_3d = 2\n",
    "        else:\n",
    "            cluster_id = parts[-1]\n",
    "            display_name = f\"{state} C{cluster_id}\"\n",
    "            marker_symbol = 'circle'\n",
    "            marker_size_2d = 4\n",
    "            marker_size_3d = 3\n",
    "        \n",
    "        # Add 2D scatter plot\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=embedding_2d[indices, 0],\n",
    "                y=embedding_2d[indices, 1],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    color=colors[indices[0]],\n",
    "                    size=marker_size_2d,\n",
    "                    opacity=0.7,\n",
    "                    symbol=marker_symbol,\n",
    "                    line=dict(width=0.5, color='white')\n",
    "                ),\n",
    "                name=display_name,\n",
    "                legendgroup=display_name,\n",
    "                hovertemplate=f'<b>{display_name}</b><br>UMAP 1: %{{x:.2f}}<br>UMAP 2: %{{y:.2f}}<extra></extra>'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Add 3D scatter plot  \n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=embedding_3d[indices, 0],\n",
    "                y=embedding_3d[indices, 1], \n",
    "                z=embedding_3d[indices, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    color=colors[indices[0]],\n",
    "                    size=marker_size_3d,\n",
    "                    opacity=0.7,\n",
    "                    symbol=marker_symbol,\n",
    "                    line=dict(width=0.5, color='white')\n",
    "                ),\n",
    "                name=display_name,\n",
    "                legendgroup=display_name,\n",
    "                showlegend=False,  # Don't duplicate legend\n",
    "                hovertemplate=f'<b>{display_name}</b><br>UMAP 1: %{{x:.2f}}<br>UMAP 2: %{{y:.2f}}<br>UMAP 3: %{{z:.2f}}<extra></extra>'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    title = f'{pattern_name} - {config_str}<br><sub>Left: 2D Visualization | Right: 3D Clustering Space</sub>'\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=title,\n",
    "            x=0.5,\n",
    "            font=dict(size=14)\n",
    "        ),\n",
    "        width=1600,\n",
    "        height=700,\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\", \n",
    "            x=1.02\n",
    "        ),\n",
    "        margin=dict(r=200)\n",
    "    )\n",
    "    \n",
    "    # Update 2D subplot axes\n",
    "    fig.update_xaxes(title_text=\"UMAP Dimension 1\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"UMAP Dimension 2\", row=1, col=1)\n",
    "    \n",
    "    # Update 3D subplot axes\n",
    "    fig.update_scenes(\n",
    "        xaxis_title=\"UMAP Dimension 1\",\n",
    "        yaxis_title=\"UMAP Dimension 2\", \n",
    "        zaxis_title=\"UMAP Dimension 3\",\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Save and display\n",
    "    safe_pattern = pattern_name.replace(' ', '_').replace('&', 'and')\n",
    "    filename = f\"umap_first_{safe_pattern}_{config_str}_{hash(str(embedding_2d.tolist())) % 10000}.html\"\n",
    "    \n",
    "    fig.write_html(filename, auto_open=False)\n",
    "    \n",
    "    print(f\"\\n📊 UMAP-First Visualization saved: {filename}\")\n",
    "    print(f\"   🎯 Key difference: Clustering was performed on 3D UMAP embeddings, not original high-dim data\")\n",
    "    print(f\"   📈 Left plot: 2D visualization | Right plot: 3D clustering space\")\n",
    "    print(f\"   Opening in browser...\")\n",
    "    \n",
    "    try:\n",
    "        import webbrowser\n",
    "        import os\n",
    "        webbrowser.open(f'file://{os.path.abspath(filename)}', new=2)\n",
    "    except Exception as e:\n",
    "        print(f\"   Could not open browser: {e}\")\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_single_pattern_analysis_umap_first(\n",
    "    pattern_name,\n",
    "    all_tokens=True,\n",
    "    all_samples=True,\n",
    "    single_sample_idx=0,\n",
    "    min_cluster_size=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Alternative analysis: UMAP first (to 3D), then HDBSCAN clustering on 3D embeddings\n",
    "    \n",
    "    Args:\n",
    "        pattern_name: Name of cognitive pattern to analyze\n",
    "        all_tokens: If True, use all token positions. If False, use last token only\n",
    "        all_samples: If True, use all samples. If False, use single sample\n",
    "        single_sample_idx: Which sample to use if all_samples=False\n",
    "        min_cluster_size: Minimum cluster size for HDBSCAN\n",
    "    \"\"\"\n",
    "    \n",
    "    if pattern_name not in pattern_indices:\n",
    "        print(f\"❌ Pattern '{pattern_name}' not found\")\n",
    "        return None\n",
    "    \n",
    "    # Configuration string for titles\n",
    "    tokens_str = \"AllTokens\" if all_tokens else \"LastToken\"\n",
    "    samples_str = \"AllSamples\" if all_samples else f\"Sample{single_sample_idx}\"\n",
    "    config_str = f\"{tokens_str}_{samples_str}\"\n",
    "    \n",
    "    print(f\"\\n🚀 UMAP-First Analysis Configuration: {config_str}\")\n",
    "    print(f\"   🔄 NEW APPROACH: 3D UMAP → HDBSCAN clustering (vs original: HDBSCAN → 2D UMAP)\")\n",
    "    print(f\"   Pattern: {pattern_name}\")\n",
    "    print(f\"   Layer: {layer} (fixed)\")\n",
    "    print(f\"   Token sampling: {'All tokens' if all_tokens else 'Last token only'}\")\n",
    "    print(f\"   Sample selection: {'All samples' if all_samples else f'Single sample {single_sample_idx}'}\")\n",
    "    print(f\"   Min cluster size: {min_cluster_size}\")\n",
    "    \n",
    "    # Get indices for this pattern\n",
    "    indices = pattern_indices[pattern_name]\n",
    "    if not all_samples:\n",
    "        if single_sample_idx >= len(indices):\n",
    "            print(f\"❌ Sample index {single_sample_idx} out of range (max: {len(indices)-1})\")\n",
    "            return None\n",
    "        indices = [indices[single_sample_idx]]\n",
    "    \n",
    "    print(f\"   Using {len(indices)} sample(s)\")\n",
    "    \n",
    "    # Load activations for this pattern (layer 17 only)\n",
    "    neg_data = negative_activations[f'negative_layer_{layer}'][indices]\n",
    "    pos_data = positive_activations[f'positive_layer_{layer}'][indices] \n",
    "    trans_data = transition_activations[f'transition_layer_{layer}'][indices]\n",
    "    \n",
    "    print(f\"   Data shapes - Neg: {neg_data.shape}, Pos: {pos_data.shape}, Trans: {trans_data.shape}\")\n",
    "    \n",
    "    # Prepare data based on token sampling strategy\n",
    "    def prepare_state_data(data, state_name):\n",
    "        if all_tokens:\n",
    "            # Use all tokens from all samples\n",
    "            flat_data = data.reshape(-1, data.shape[-1])\n",
    "            print(f\"     {state_name}: {data.shape} -> {flat_data.shape} (all tokens)\")\n",
    "        else:\n",
    "            # Use only last token from each sample\n",
    "            last_tokens = data[:, -1, :]\n",
    "            flat_data = last_tokens.reshape(-1, last_tokens.shape[-1])\n",
    "            print(f\"     {state_name}: {data.shape} -> {flat_data.shape} (last tokens only)\")\n",
    "        return flat_data.cpu().numpy()  # Move to CPU before converting to numpy\n",
    "    \n",
    "    # Prepare data for each state\n",
    "    print(f\"\\n📊 Preparing data for UMAP-first analysis...\")\n",
    "    neg_flat = prepare_state_data(neg_data, \"Negative\")\n",
    "    pos_flat = prepare_state_data(pos_data, \"Positive\")\n",
    "    trans_flat = prepare_state_data(trans_data, \"Transition\")\n",
    "    \n",
    "    return perform_clustering_and_visualization_umap_first(\n",
    "        neg_flat, pos_flat, trans_flat, \n",
    "        pattern_name, config_str, min_cluster_size\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 Alternative Approach: UMAP-First Analysis\n",
    "\n",
    "This section tests a different approach: **3D UMAP first, then HDBSCAN clustering on the 3D embeddings**.\n",
    "\n",
    "### Key Differences:\n",
    "- **Original approach**: HDBSCAN clustering in 2304-dimensional space → 2D UMAP for visualization\n",
    "- **New approach**: 3D UMAP dimensionality reduction → HDBSCAN clustering in 3D space → 2D UMAP for comparison\n",
    "\n",
    "### Why This Might Be Better:\n",
    "1. **UMAP preserves local structure** better than raw high-dimensional clustering\n",
    "2. **3D clustering space** is more manageable than 2304D but richer than 2D\n",
    "3. **May reveal different cluster patterns** that are lost in high-dimensional noise\n",
    "4. **More interpretable clustering** in the UMAP-transformed space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n================================================================================\n",
      "🔄 UMAP-FIRST ANALYSIS: All Token Positions + All Samples\n",
      "================================================================================\n",
      "\n",
      "🚀 UMAP-First Analysis Configuration: AllTokens_AllSamples\n",
      "   🔄 NEW APPROACH: 3D UMAP → HDBSCAN clustering (vs original: HDBSCAN → 2D UMAP)\n",
      "   Pattern: Executive Fatigue & Avolition\n",
      "   Layer: 17 (fixed)\n",
      "   Token sampling: All tokens\n",
      "   Sample selection: All samples\n",
      "   Min cluster size: 30\n",
      "   Using 40 sample(s)\n",
      "   Data shapes - Neg: torch.Size([40, 208, 2304]), Pos: torch.Size([40, 261, 2304]), Trans: torch.Size([40, 311, 2304])\n",
      "\n",
      "📊 Preparing data for UMAP-first analysis...\n",
      "     Negative: torch.Size([40, 208, 2304]) -> torch.Size([8320, 2304]) (all tokens)\n",
      "     Positive: torch.Size([40, 261, 2304]) -> torch.Size([10440, 2304]) (all tokens)\n",
      "     Transition: torch.Size([40, 311, 2304]) -> torch.Size([12440, 2304]) (all tokens)\n",
      "\n",
      "🔬 UMAP-First Analysis: 3D UMAP → HDBSCAN Clustering\n",
      "============================================================\n",
      "⚡ NEW APPROACH: Dimensionality reduction first, then clustering in 3D space\n",
      "\n",
      "🎯 Processing Negative state (8320 samples)\n",
      "   🗺️  Computing 3D UMAP embedding...\n",
      "   ✅ 3D UMAP: (8320, 2304) → (8320, 3)\n",
      "   🔍 HDBSCAN clustering on 3D embedding...\n",
      "   Found 17 clusters, 97 noise points (1.2%)\n",
      "     Noise:   97 samples (  1.2%)\n",
      "     Cluster 0:   40 samples (  0.5%)\n",
      "     Cluster 1:   32 samples (  0.4%)\n",
      "     Cluster 2:   34 samples (  0.4%)\n",
      "     Cluster 3:   71 samples (  0.9%)\n",
      "     Cluster 4:   88 samples (  1.1%)\n",
      "     Cluster 5:   45 samples (  0.5%)\n",
      "     Cluster 6:   37 samples (  0.4%)\n",
      "     Cluster 7:  143 samples (  1.7%)\n",
      "     Cluster 8:   45 samples (  0.5%)\n",
      "     Cluster 9:   66 samples (  0.8%)\n",
      "     Cluster 10: 7332 samples ( 88.1%)\n",
      "     Cluster 11:   48 samples (  0.6%)\n",
      "     Cluster 12:   35 samples (  0.4%)\n",
      "     Cluster 13:   45 samples (  0.5%)\n",
      "     Cluster 14:   38 samples (  0.5%)\n",
      "     Cluster 15:   57 samples (  0.7%)\n",
      "     Cluster 16:   67 samples (  0.8%)\n",
      "   📊 Creating 2D UMAP for visualization...\n",
      "\n",
      "🎯 Processing Positive state (10440 samples)\n",
      "   🗺️  Computing 3D UMAP embedding...\n",
      "   ✅ 3D UMAP: (10440, 2304) → (10440, 3)\n",
      "   🔍 HDBSCAN clustering on 3D embedding...\n",
      "   Found 49 clusters, 479 noise points (4.6%)\n",
      "     Noise:  479 samples (  4.6%)\n",
      "     Cluster 0:   38 samples (  0.4%)\n",
      "     Cluster 1:   39 samples (  0.4%)\n",
      "     Cluster 2:   33 samples (  0.3%)\n",
      "     Cluster 3:   31 samples (  0.3%)\n",
      "     Cluster 4:   30 samples (  0.3%)\n",
      "     Cluster 5:   57 samples (  0.5%)\n",
      "     Cluster 6:   35 samples (  0.3%)\n",
      "     Cluster 7:   52 samples (  0.5%)\n",
      "     Cluster 8:   31 samples (  0.3%)\n",
      "     Cluster 9:   53 samples (  0.5%)\n",
      "     Cluster 10:   51 samples (  0.5%)\n",
      "     Cluster 11:   88 samples (  0.8%)\n",
      "     Cluster 12:   33 samples (  0.3%)\n",
      "     Cluster 13:  218 samples (  2.1%)\n",
      "     Cluster 14:   75 samples (  0.7%)\n",
      "     Cluster 15:   32 samples (  0.3%)\n",
      "     Cluster 16:   88 samples (  0.8%)\n",
      "     Cluster 17:   31 samples (  0.3%)\n",
      "     Cluster 18:   33 samples (  0.3%)\n",
      "     Cluster 19:   34 samples (  0.3%)\n",
      "     Cluster 20:   50 samples (  0.5%)\n",
      "     Cluster 21:   46 samples (  0.4%)\n",
      "     Cluster 22:   38 samples (  0.4%)\n",
      "     Cluster 23:   99 samples (  0.9%)\n",
      "     Cluster 24:   61 samples (  0.6%)\n",
      "     Cluster 25:   46 samples (  0.4%)\n",
      "     Cluster 26:   43 samples (  0.4%)\n",
      "     Cluster 27:   32 samples (  0.3%)\n",
      "     Cluster 28:   54 samples (  0.5%)\n",
      "     Cluster 29:   38 samples (  0.4%)\n",
      "     Cluster 30:   44 samples (  0.4%)\n",
      "     Cluster 31:   49 samples (  0.5%)\n",
      "     Cluster 32:   35 samples (  0.3%)\n",
      "     Cluster 33:   47 samples (  0.5%)\n",
      "     Cluster 34:   86 samples (  0.8%)\n",
      "     Cluster 35:   33 samples (  0.3%)\n",
      "     Cluster 36:   61 samples (  0.6%)\n",
      "     Cluster 37:   47 samples (  0.5%)\n",
      "     Cluster 38:   62 samples (  0.6%)\n",
      "     Cluster 39:   58 samples (  0.6%)\n",
      "     Cluster 40:   68 samples (  0.7%)\n",
      "     Cluster 41:   52 samples (  0.5%)\n",
      "     Cluster 42:   60 samples (  0.6%)\n",
      "     Cluster 43: 7445 samples ( 71.3%)\n",
      "     Cluster 44:   32 samples (  0.3%)\n",
      "     Cluster 45:   58 samples (  0.6%)\n",
      "     Cluster 46:   40 samples (  0.4%)\n",
      "     Cluster 47:   34 samples (  0.3%)\n",
      "     Cluster 48:   61 samples (  0.6%)\n",
      "   📊 Creating 2D UMAP for visualization...\n",
      "\n",
      "🎯 Processing Transition state (12440 samples)\n",
      "   🗺️  Computing 3D UMAP embedding...\n",
      "   ✅ 3D UMAP: (12440, 2304) → (12440, 3)\n",
      "   🔍 HDBSCAN clustering on 3D embedding...\n",
      "   Found 9 clusters, 132 noise points (1.1%)\n",
      "     Noise:  132 samples (  1.1%)\n",
      "     Cluster 0:   37 samples (  0.3%)\n",
      "     Cluster 1:   33 samples (  0.3%)\n",
      "     Cluster 2:   40 samples (  0.3%)\n",
      "     Cluster 3:   59 samples (  0.5%)\n",
      "     Cluster 4:   64 samples (  0.5%)\n",
      "     Cluster 5:   62 samples (  0.5%)\n",
      "     Cluster 6:  142 samples (  1.1%)\n",
      "     Cluster 7:   77 samples (  0.6%)\n",
      "     Cluster 8: 11794 samples ( 94.8%)\n",
      "   📊 Creating 2D UMAP for visualization...\n",
      "\n",
      "🗺️  Creating visualizations with 31200 total points...\n",
      "\n",
      "📊 UMAP-First Visualization saved: umap_first_Executive_Fatigue_and_Avolition_AllTokens_AllSamples_UMAPFirst_1435.html\n",
      "   🎯 Key difference: Clustering was performed on 3D UMAP embeddings, not original high-dim data\n",
      "   📈 Left plot: 2D visualization | Right plot: 3D clustering space\n",
      "   Opening in browser...\n",
      "\n",
      "📋 Analysis Summary: Executive Fatigue & Avolition - AllTokens_AllSamples_UMAPFirst\n",
      "============================================================\n",
      "Total data points analyzed: 31200\n",
      "\n",
      "Negative State:\n",
      "  • 8320 total points\n",
      "  • 17 clusters found\n",
      "  • 97 noise points (1.2%)\n",
      "  • 8223 points in clusters (98.8%)\n",
      "\n",
      "Positive State:\n",
      "  • 10440 total points\n",
      "  • 49 clusters found\n",
      "  • 479 noise points (4.6%)\n",
      "  • 9961 points in clusters (95.4%)\n",
      "\n",
      "Transition State:\n",
      "  • 12440 total points\n",
      "  • 9 clusters found\n",
      "  • 132 noise points (1.1%)\n",
      "  • 12308 points in clusters (98.9%)\n",
      "\n",
      "💡 Key Insights:\n",
      "  • Clustering performed WITHIN each cognitive state separately\n",
      "  • HDBSCAN automatically determines optimal number of clusters\n",
      "  • Noise points represent outliers that don't fit clear patterns\n",
      "  • Cluster separation indicates distinct neural activation patterns\n"
     ]
    }
   ],
   "source": [
    "# Test UMAP-First Approach: All tokens, all samples\n",
    "print(\"\\\\n\" + \"=\" * 80)\n",
    "print(\"🔄 UMAP-FIRST ANALYSIS: All Token Positions + All Samples\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_umap_first = perform_single_pattern_analysis_umap_first(\n",
    "    pattern_name=selected_pattern,\n",
    "    all_tokens=True,\n",
    "    all_samples=True,\n",
    "    min_cluster_size=30\n",
    ")\n",
    "\n",
    "print_analysis_summary(results_umap_first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_clustering_approaches(original_results, umap_first_results):\n",
    "    \"\"\"\n",
    "    Compare the clustering results between original and UMAP-first approaches\n",
    "    \"\"\"\n",
    "    \n",
    "    if not original_results or not umap_first_results:\n",
    "        print(\"❌ Cannot compare - missing results\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\" * 80)\n",
    "    print(\"📊 CLUSTERING APPROACH COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"Pattern: {original_results['pattern_name']}\")\n",
    "    print(f\"Configuration: All Tokens + All Samples\\\\n\")\n",
    "    \n",
    "    # Extract clustering info for both approaches\n",
    "    orig_clustering = original_results['clustering_results']\n",
    "    umap_clustering = umap_first_results['clustering_results']\n",
    "    \n",
    "    print(f\"{'State':<12} {'Approach':<15} {'Clusters':<9} {'Noise %':<8} {'Largest Cluster %':<18}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for state in ['Negative', 'Positive', 'Transition']:\n",
    "        if state in orig_clustering:\n",
    "            orig_info = orig_clustering[state]\n",
    "            orig_noise_pct = orig_info['n_noise'] / len(orig_info['cluster_labels']) * 100\n",
    "            \n",
    "            # Find largest cluster\n",
    "            labels, counts = np.unique(orig_info['cluster_labels'], return_counts=True)\n",
    "            non_noise_counts = counts[labels != -1] if -1 in labels else counts\n",
    "            largest_orig_pct = max(non_noise_counts) / len(orig_info['cluster_labels']) * 100 if len(non_noise_counts) > 0 else 0\n",
    "            \n",
    "            print(f\"{state:<12} {'Original':<15} {orig_info['n_clusters']:<9} {orig_noise_pct:<8.1f} {largest_orig_pct:<18.1f}\")\n",
    "        \n",
    "        if state in umap_clustering:\n",
    "            umap_info = umap_clustering[state]\n",
    "            umap_noise_pct = umap_info['n_noise'] / len(umap_info['cluster_labels']) * 100\n",
    "            \n",
    "            # Find largest cluster\n",
    "            labels, counts = np.unique(umap_info['cluster_labels'], return_counts=True)\n",
    "            non_noise_counts = counts[labels != -1] if -1 in labels else counts\n",
    "            largest_umap_pct = max(non_noise_counts) / len(umap_info['cluster_labels']) * 100 if len(non_noise_counts) > 0 else 0\n",
    "            \n",
    "            print(f\"{'':<12} {'UMAP-First':<15} {umap_info['n_clusters']:<9} {umap_noise_pct:<8.1f} {largest_umap_pct:<18.1f}\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"🔍 Key Insights:\")\n",
    "    print(\"  • Original: Clusters in full 2304D space, then visualizes with UMAP\")\n",
    "    print(\"  • UMAP-First: Reduces to 3D with UMAP, then clusters in 3D space\")\n",
    "    print(\"  • Lower noise % suggests cleaner clustering\")\n",
    "    print(\"  • More balanced cluster sizes suggest better separation\")\n",
    "    print(\"  • Different approaches may reveal different organizational patterns\")\n",
    "\n",
    "# Compare the approaches if both results exist\n",
    "if 'results_all_tokens_all_samples' in locals() and 'results_umap_first' in locals():\n",
    "    compare_clustering_approaches(results_all_tokens_all_samples, results_umap_first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🔍 ANALYSIS 3: All Token Positions + Single Sample\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'perform_single_pattern_analysis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🔍 ANALYSIS 3: All Token Positions + Single Sample\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m results_all_tokens_single_sample = \u001b[43mperform_single_pattern_analysis\u001b[49m(\n\u001b[32m      7\u001b[39m     pattern_name=selected_pattern,\n\u001b[32m      8\u001b[39m     all_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      9\u001b[39m     all_samples=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     10\u001b[39m     single_sample_idx=\u001b[32m0\u001b[39m,\n\u001b[32m     11\u001b[39m     min_cluster_size=\u001b[32m5\u001b[39m  \u001b[38;5;66;03m# Much smaller cluster size for single sample\u001b[39;00m\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m print_analysis_summary(results_all_tokens_single_sample)\n",
      "\u001b[31mNameError\u001b[39m: name 'perform_single_pattern_analysis' is not defined"
     ]
    }
   ],
   "source": [
    "# Analysis 3: All tokens, single sample\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🔍 ANALYSIS 3: All Token Positions + Single Sample\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_all_tokens_single_sample = perform_single_pattern_analysis(\n",
    "    pattern_name=selected_pattern,\n",
    "    all_tokens=True,\n",
    "    all_samples=False,\n",
    "    single_sample_idx=0,\n",
    "    min_cluster_size=5  # Much smaller cluster size for single sample\n",
    ")\n",
    "\n",
    "print_analysis_summary(results_all_tokens_single_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 4: Last Token Only + Single Sample\n",
    "\n",
    "This analysis looks at the final processing state of each cognitive state within a single example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis 4: Last token only, single sample\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🔍 ANALYSIS 4: Last Token Only + Single Sample\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_last_token_single_sample = perform_single_pattern_analysis(\n",
    "    pattern_name=selected_pattern,\n",
    "    all_tokens=False,\n",
    "    all_samples=False,\n",
    "    single_sample_idx=0,\n",
    "    min_cluster_size=2  # Very small cluster size for single sample last tokens only\n",
    ")\n",
    "\n",
    "print_analysis_summary(results_last_token_single_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis\n",
    "\n",
    "Compare the results across different analysis approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_analyses(analysis_results):\n",
    "    \"\"\"\n",
    "    Compare the results from different analysis approaches\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"📊 COMPARATIVE ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for name, results in analysis_results.items():\n",
    "        if results is None:\n",
    "            continue\n",
    "            \n",
    "        clustering_results = results['clustering_results']\n",
    "        total_points = len(results['labels'])\n",
    "        \n",
    "        # Aggregate statistics\n",
    "        total_clusters = sum(info['n_clusters'] for info in clustering_results.values())\n",
    "        total_noise = sum(info['n_noise'] for info in clustering_results.values())\n",
    "        total_clustered = total_points - total_noise\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'analysis': name,\n",
    "            'total_points': total_points,\n",
    "            'total_clusters': total_clusters,\n",
    "            'clustered_points': total_clustered,\n",
    "            'noise_points': total_noise,\n",
    "            'cluster_rate': total_clustered / total_points * 100 if total_points > 0 else 0\n",
    "        })\n",
    "    \n",
    "    # Print comparison table\n",
    "    print(f\"{'Analysis':<25} {'Points':<8} {'Clusters':<9} {'Clustered':<10} {'Noise':<8} {'Cluster %':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for data in comparison_data:\n",
    "        print(f\"{data['analysis']:<25} {data['total_points']:<8} {data['total_clusters']:<9} \"\n",
    "              f\"{data['clustered_points']:<10} {data['noise_points']:<8} {data['cluster_rate']:<10.1f}%\")\n",
    "    \n",
    "    print(f\"\\n💡 Interpretation Guidelines:\")\n",
    "    print(f\"  • All Tokens analyses show sequential processing patterns\")\n",
    "    print(f\"  • Last Token analyses focus on final cognitive states\")\n",
    "    print(f\"  • All Samples analyses reveal population-level patterns\")\n",
    "    print(f\"  • Single Sample analyses show individual example dynamics\")\n",
    "    print(f\"  • Higher cluster rates suggest more structured activation patterns\")\n",
    "    print(f\"  • Noise points indicate unique or transitional activation states\")\n",
    "\n",
    "# Collect all analysis results for comparison\n",
    "all_analyses = {\n",
    "    'All Tokens + All Samples': results_all_tokens_all_samples,\n",
    "    'Last Token + All Samples': results_last_token_all_samples,  \n",
    "    'All Tokens + Single Sample': results_all_tokens_single_sample,\n",
    "    'Last Token + Single Sample': results_last_token_single_sample\n",
    "}\n",
    "\n",
    "compare_analyses(all_analyses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a comprehensive analysis framework for single cognitive patterns using UMAP + HDBSCAN.\n",
    "\n",
    "### Key Improvements:\n",
    "1. **Proper clustering scope**: Clustering performed within each cognitive state (positive/negative/transition) separately\n",
    "2. **Single pattern focus**: Analysis limited to one cognitive pattern at a time for cleaner interpretation\n",
    "3. **Flexible token sampling**: Options for all tokens vs. last token analysis\n",
    "4. **Sample size control**: Analysis of all samples vs. single sample for different research questions\n",
    "\n",
    "### Usage Notes:\n",
    "- Change `selected_pattern` variable to analyze different cognitive patterns\n",
    "- Adjust `min_cluster_size` parameters based on your data size and desired granularity\n",
    "- The generated HTML files can be opened in any web browser for interactive exploration\n",
    "- Each analysis answers different research questions about neural activation patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
