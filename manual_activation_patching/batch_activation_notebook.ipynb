{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Activation Patching Experiments\n",
    "\n",
    "This notebook allows you to experiment with batch activation extraction and aggregation. Instead of using activations from a single text, you can:\n",
    "\n",
    "1. **Extract activations from multiple positive texts**\n",
    "2. **Aggregate them using different methods** (mean, median, max, etc.)\n",
    "3. **Use the aggregated activation for patching**\n",
    "4. **Compare different aggregation strategies**\n",
    "\n",
    "This approach may provide more robust and generalizable activation patterns for patching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Add TransformerLens to path\n",
    "sys.path.append('/home/koalacrown/Desktop/Code/Projects/turnaround/turn_point/third_party/TransformerLens')\n",
    "\n",
    "# Import our activation patcher\n",
    "sys.path.append('/home/koalacrown/Desktop/Code/Projects/turnaround/turn_point/manual_activation_patching')\n",
    "from activation_patcher import ActivationPatcher\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "# random.seed(42)\n",
    "# np.random.seed(42)\n",
    "# torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Model and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Choose your model here - change this and re-run to experiment with different models\nMODEL_NAME = \"gpt2-small\"  # Change to: gpt2-medium, EleutherAI/gpt-neo-125m, etc.\n\n# Initialize the activation patcher\npatcher = ActivationPatcher(MODEL_NAME)\n\n# Load the positive patterns dataset\ndataset_path = \"/home/koalacrown/Desktop/Code/Projects/turnaround/turn_point/data/final/positive_patterns.jsonl\"\npatterns = patcher.load_dataset(dataset_path)\n\nprint(f\"Loaded {len(patterns)} positive thought patterns\")\nprint(f\"Model info: {patcher.get_model_info()}\")\n\n# Ensure model starts in clean state\npatcher.reset_hooks()\nprint(\"âœ… Model initialized and hooks reset to clean state\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Available Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported Models:\n",
      "==================================================\n",
      "\n",
      "GPT2:\n",
      "  - gpt2-small (small)\n",
      "  - gpt2-medium (medium)\n",
      "  - gpt2-large (large)\n",
      "  - gpt2-xl (xl)\n",
      "\n",
      "GPTJ:\n",
      "  - EleutherAI/gpt-j-6b (6b)\n",
      "\n",
      "GPT-NEO:\n",
      "  - EleutherAI/gpt-neo-125m (125m)\n",
      "  - EleutherAI/gpt-neo-1.3b (1.3b)\n",
      "  - EleutherAI/gpt-neo-2.7b (2.7b)\n",
      "\n",
      "OPT:\n",
      "  - facebook/opt-125m (125m)\n",
      "  - facebook/opt-1.3b (1.3b)\n",
      "  - facebook/opt-2.7b (2.7b)\n",
      "  - facebook/opt-6.7b (6.7b)\n",
      "\n",
      "PYTHIA:\n",
      "  - EleutherAI/pythia-70m (70m)\n",
      "  - EleutherAI/pythia-160m (160m)\n",
      "  - EleutherAI/pythia-410m (410m)\n",
      "  - EleutherAI/pythia-1b (1b)\n",
      "  - EleutherAI/pythia-1.4b (1.4b)\n",
      "  - EleutherAI/pythia-2.8b (2.8b)\n",
      "\n",
      "GEMMA:\n",
      "  - google/gemma-2b (2b)\n",
      "  - google/gemma-7b (7b)\n"
     ]
    }
   ],
   "source": [
    "# List all supported models\n",
    "ActivationPatcher.list_supported_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select patterns by cognitive type or randomly\n",
    "def filter_patterns_by_type(patterns, pattern_type=None, max_count=20):\n",
    "    \"\"\"Filter patterns by cognitive type or return random selection.\"\"\"\n",
    "    if pattern_type:\n",
    "        filtered = [p for p in patterns if pattern_type.lower() in p.get('cognitive_pattern_type', '').lower()]\n",
    "        print(f\"Found {len(filtered)} patterns matching '{pattern_type}'\")\n",
    "    else:\n",
    "        filtered = patterns.copy()\n",
    "        random.shuffle(filtered)\n",
    "        print(f\"Using random selection from {len(filtered)} patterns\")\n",
    "    \n",
    "    return filtered[:max_count]\n",
    "\n",
    "# Available cognitive pattern types in the dataset:\n",
    "pattern_types = set([p.get('cognitive_pattern_type', '') for p in patterns])\n",
    "print(\"Available cognitive pattern types:\")\n",
    "for ptype in sorted(pattern_types):\n",
    "    if ptype:\n",
    "        count = len([p for p in patterns if p.get('cognitive_pattern_type', '') == ptype])\n",
    "        print(f\"  - {ptype} ({count} examples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for batch experiments\n",
    "BATCH_SIZE = 15  # Number of texts to use for batch activation extraction\n",
    "PATTERN_TYPE = None  # Set to specific type like \"rumination\" or None for random\n",
    "\n",
    "# Select patterns for batch\n",
    "selected_patterns = filter_patterns_by_type(patterns, PATTERN_TYPE, BATCH_SIZE)\n",
    "\n",
    "# Extract texts\n",
    "batch_texts = [p['positive_thought_pattern'] for p in selected_patterns]\n",
    "\n",
    "print(f\"\\nSelected {len(batch_texts)} texts for batch processing:\")\n",
    "for i, text in enumerate(batch_texts[:5]):  # Show first 5\n",
    "    print(f\"{i+1}. {text[:100]}...\")\n",
    "if len(batch_texts) > 5:\n",
    "    print(f\"... and {len(batch_texts) - 5} more\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ðŸ”„ RESET MODEL HOOKS - Essential for batch experiments\n# Run this before starting new batch experiments to avoid interference\n\npatcher.reset_hooks()\n\nprint(\"ðŸ”„ Model hooks reset - Ready for clean batch experiments!\")\nprint(\"ðŸ’¡ Tip: Run this cell between different batch configurations\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ðŸ”„ Reset Model State\n\nImportant: Reset hooks before running batch experiments to ensure clean state:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Different Aggregation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test different aggregation methods with NEW MULTI-LAYER FUNCTIONALITY\naggregation_methods = [\"mean\", \"median\", \"max\", \"random\"]\ncorrupted_prompt = \"I feel completely overwhelmed and stuck, unable to\"\ntarget_words = [\"positive\", \"solutions\", \"growth\", \"hope\", \"progress\"]\n\nprint(f\"Testing aggregation methods with:\")\nprint(f\"- Batch size: {len(batch_texts)}\")\nprint(f\"- Corrupted prompt: {corrupted_prompt}\")\nprint(f\"- Target words: {target_words}\")\nprint(\"\\n\" + \"=\"*100)\n\naggregation_results = {}\n\nfor method in aggregation_methods:\n    print(f\"\\n--- Testing {method.upper()} aggregation ---\")\n    \n    try:\n        predicted_token, generated_text = patcher.batch_patch_and_generate(\n            clean_texts=batch_texts,\n            corrupted_text=corrupted_prompt,\n            capture_layer_idx=-1,     # Updated parameter name\n            patch_layer_idx=-1,       # Explicit patch layer specification\n            aggregation=method,\n            target_words=target_words,\n            num_placeholder_tokens=5,\n            max_new_tokens=60\n        )\n        \n        aggregation_results[method] = {\n            'success': True,\n            'predicted_token': predicted_token,\n            'generated_text': generated_text\n        }\n        \n        print(f\"\\nâœ“ SUCCESS with {method} aggregation:\")\n        print(f\"Generated: {generated_text}\")\n        \n    except Exception as e:\n        print(f\"âœ— Error with {method} aggregation: {e}\")\n        aggregation_results[method] = {\n            'success': False,\n            'error': str(e),\n            'generated_text': None\n        }\n    \n    print(\"-\" * 80)\n\n# Memory cleanup\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Batch Size Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test different batch sizes\nbatch_sizes = [1, 3, 5, 10, 15]\ncorrupted_prompt = \"My thoughts are spiraling and I can't seem to\"\naggregation_method = \"mean\"\n\nprint(f\"Testing batch sizes with {aggregation_method} aggregation\")\nprint(f\"Corrupted prompt: {corrupted_prompt}\")\nprint(\"\\n\" + \"=\"*100)\n\nbatch_size_results = {}\n\nfor batch_size in batch_sizes:\n    if batch_size > len(batch_texts):\n        print(f\"Skipping batch size {batch_size} (not enough texts available)\")\n        continue\n        \n    print(f\"\\n--- Testing batch size: {batch_size} ---\")\n    \n    subset_texts = batch_texts[:batch_size]\n    \n    try:\n        predicted_token, generated_text = patcher.batch_patch_and_generate(\n            clean_texts=subset_texts,\n            corrupted_text=corrupted_prompt,\n            capture_layer_idx=-1,     # Updated parameter name\n            patch_layer_idx=-1,       # Updated parameter name\n            aggregation=aggregation_method,\n            target_words=None,\n            num_placeholder_tokens=5,\n            max_new_tokens=55\n        )\n        \n        batch_size_results[batch_size] = {\n            'success': True,\n            'generated_text': generated_text,\n            'predicted_token': predicted_token\n        }\n        \n        print(f\"\\nâœ“ SUCCESS with batch size {batch_size}:\")\n        print(f\"Generated: {generated_text}\")\n        \n    except Exception as e:\n        print(f\"âœ— Error with batch size {batch_size}: {e}\")\n        batch_size_results[batch_size] = {\n            'success': False,\n            'error': str(e),\n            'generated_text': None\n        }\n    \n    print(\"-\" * 80)\n\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Custom Batch Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Custom configuration - modify these variables for your experiments\nCUSTOM_BATCH_TEXTS = [\n    \"I'm learning to acknowledge my feelings and take things one step at a time.\",\n    \"I choose to focus on solutions rather than dwelling on problems.\",\n    \"I'm practicing self-compassion and recognizing my growth.\",\n    \"I can handle challenges by breaking them into manageable pieces.\",\n    \"I'm building resilience and finding healthy ways to cope.\",\n    \"I trust in my ability to navigate difficult situations.\",\n    \"I'm grateful for the support I have and the progress I've made.\",\n    \"I choose hope and believe that positive change is possible.\"\n]\n\nCUSTOM_CORRUPTED_TEXT = \"I don't know how to deal with this situation and feel\"\nCUSTOM_AGGREGATION = \"mean\"\nCUSTOM_TARGET_WORDS = [\"acknowledge\", \"focus\", \"solutions\", \"growth\", \"hope\"]\nCUSTOM_CAPTURE_LAYER = -1      # Layer to capture activations from\nCUSTOM_PATCH_LAYER = -1        # Layer to patch activations into\nCUSTOM_PLACEHOLDERS = 5\nCUSTOM_MAX_TOKENS = 70\n\nprint(\"CUSTOM BATCH EXPERIMENT WITH MULTI-LAYER FUNCTIONALITY:\")\nprint(\"=\"*100)\nprint(f\"Batch texts ({len(CUSTOM_BATCH_TEXTS)} total):\")\nfor i, text in enumerate(CUSTOM_BATCH_TEXTS[:3]):\n    print(f\"  {i+1}. {text}\")\nif len(CUSTOM_BATCH_TEXTS) > 3:\n    print(f\"  ... and {len(CUSTOM_BATCH_TEXTS) - 3} more\")\n\nprint(f\"\\nCorrupted text: {CUSTOM_CORRUPTED_TEXT}\")\nprint(f\"Aggregation: {CUSTOM_AGGREGATION}\")\nprint(f\"Target words: {CUSTOM_TARGET_WORDS}\")\nprint(f\"Capture layer: {CUSTOM_CAPTURE_LAYER}\")\nprint(f\"Patch layer: {CUSTOM_PATCH_LAYER}\")\nprint(f\"Placeholders: {CUSTOM_PLACEHOLDERS}\")\nprint(f\"Max tokens: {CUSTOM_MAX_TOKENS}\")\n\ntry:\n    predicted_token, generated_text = patcher.batch_patch_and_generate(\n        clean_texts=CUSTOM_BATCH_TEXTS,\n        corrupted_text=CUSTOM_CORRUPTED_TEXT,\n        capture_layer_idx=CUSTOM_CAPTURE_LAYER,  # Updated parameter name\n        patch_layer_idx=CUSTOM_PATCH_LAYER,      # Updated parameter name\n        aggregation=CUSTOM_AGGREGATION,\n        target_words=CUSTOM_TARGET_WORDS,\n        num_placeholder_tokens=CUSTOM_PLACEHOLDERS,\n        max_new_tokens=CUSTOM_MAX_TOKENS\n    )\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"CUSTOM EXPERIMENT RESULT:\")\n    print(\"=\"*50)\n    print(generated_text)\n    print(\"=\"*50)\n    \nexcept Exception as e:\n    print(f\"\\nâœ— Error in custom experiment: {e}\")\n\nprint(\"\\n\" + \"=\"*100)\nprint(\"ADVANCED MULTI-LAYER BATCH EXAMPLES:\")\nprint(\"=\"*100)\n\n# Example 1: Cross-layer batch patching\nprint(\"\\nExample 1: Capture from early layers [0,1,2], patch to late layer [-1]\")\ntry:\n    predicted_token, generated_text = patcher.batch_patch_and_generate(\n        clean_texts=CUSTOM_BATCH_TEXTS[:5],  # Use fewer texts for demo\n        corrupted_text=CUSTOM_CORRUPTED_TEXT,\n        capture_layer_idx=[0, 1, 2],  # Multiple capture layers\n        patch_layer_idx=-1,           # Single patch layer\n        aggregation=\"mean\",\n        target_words=CUSTOM_TARGET_WORDS[:3],\n        num_placeholder_tokens=3,\n        max_new_tokens=50\n    )\n    print(f\"Result: {generated_text}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\n# Example 2: Multi-layer broadcast patching\nprint(\"\\nExample 2: Capture from layer 5, patch to multiple layers [8,9,10]\")\ntry:\n    predicted_token, generated_text = patcher.batch_patch_and_generate(\n        clean_texts=CUSTOM_BATCH_TEXTS[:5],\n        corrupted_text=CUSTOM_CORRUPTED_TEXT,\n        capture_layer_idx=5,           # Single capture layer\n        patch_layer_idx=[8, 9, 10],    # Multiple patch layers\n        aggregation=\"median\",\n        target_words=CUSTOM_TARGET_WORDS[:3],\n        num_placeholder_tokens=3,\n        max_new_tokens=50\n    )\n    print(f\"Result: {generated_text}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\n# Example 3: Range-based layer selection\nprint(\"\\nExample 3: Capture from range(3,6), patch to range(7,10)\")\ntry:\n    predicted_token, generated_text = patcher.batch_patch_and_generate(\n        clean_texts=CUSTOM_BATCH_TEXTS[:5],\n        corrupted_text=CUSTOM_CORRUPTED_TEXT,\n        capture_layer_idx=list(range(3, 6)),  # Layers 3,4,5\n        patch_layer_idx=list(range(7, 10)),   # Layers 7,8,9\n        aggregation=\"max\",\n        target_words=CUSTOM_TARGET_WORDS[:3],\n        num_placeholder_tokens=3,\n        max_new_tokens=50\n    )\n    print(f\"Result: {generated_text}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Baseline Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison experiment\n",
    "comparison_prompt = \"I'm overwhelmed by everything and don't see a way to\"\n",
    "batch_for_comparison = batch_texts[:10]\n",
    "single_text_for_comparison = batch_texts[0]\n",
    "\n",
    "print(\"BASELINE COMPARISON:\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Test prompt: {comparison_prompt}\")\n",
    "print(f\"Batch size: {len(batch_for_comparison)}\")\n",
    "\n",
    "comparison_results = {}\n",
    "\n",
    "# 1. No patching (baseline)\n",
    "print(\"\\n--- 1. NO PATCHING (Baseline) ---\")\n",
    "try:\n",
    "    tokens = patcher.model.to_tokens(comparison_prompt)\n",
    "    generated_tokens = patcher.model.generate(\n",
    "        tokens,\n",
    "        max_new_tokens=60,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "    baseline_text = patcher.model.to_string(generated_tokens[0])\n",
    "    comparison_results['no_patching'] = baseline_text\n",
    "    print(f\"Generated: {baseline_text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    comparison_results['no_patching'] = f\"Error: {e}\"\n",
    "\n",
    "# 2. Single text patching\n",
    "print(\"\\n--- 2. SINGLE TEXT PATCHING ---\")\n",
    "try:\n",
    "    target_words = patcher._extract_key_words(single_text_for_comparison)\n",
    "    predicted_token, single_patch_text = patcher.patch_and_generate(\n",
    "        clean_text=single_text_for_comparison,\n",
    "        corrupted_text=comparison_prompt,\n",
    "        target_words=target_words,\n",
    "        max_new_tokens=60\n",
    "    )\n",
    "    comparison_results['single_patching'] = single_patch_text\n",
    "    print(f\"Generated: {single_patch_text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    comparison_results['single_patching'] = f\"Error: {e}\"\n",
    "\n",
    "# 3. Batch patching\n",
    "print(\"\\n--- 3. BATCH PATCHING ---\")\n",
    "try:\n",
    "    predicted_token, batch_patch_text = patcher.batch_patch_and_generate(\n",
    "        clean_texts=batch_for_comparison,\n",
    "        corrupted_text=comparison_prompt,\n",
    "        aggregation=\"mean\",\n",
    "        max_new_tokens=60\n",
    "    )\n",
    "    comparison_results['batch_patching'] = batch_patch_text\n",
    "    print(f\"Generated: {batch_patch_text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    comparison_results['batch_patching'] = f\"Error: {e}\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPARISON SUMMARY:\")\n",
    "print(\"=\"*100)\n",
    "for method, result in comparison_results.items():\n",
    "    print(f\"\\n{method.upper().replace('_', ' ')}:\")\n",
    "    print(result)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ðŸ› ï¸ BATCH EXPERIMENT UTILITIES\n\n# 1. Quick reset for batch experiments\ndef batch_reset():\n    patcher.reset_hooks()\n    print(\"ðŸ”„ Batch experiment reset - Model ready!\")\n\n# 2. Memory management for large batches\ndef batch_memory_cleanup():\n    import torch\n    import gc\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    gc.collect()\n    patcher.reset_hooks()  # Also reset hooks\n    print(\"ðŸ§¹ Memory cleared and hooks reset for batch experiments\")\n\n# 3. Check batch experiment status\ndef batch_status():\n    print(\"ðŸ“Š BATCH EXPERIMENT STATUS:\")\n    print(f\"  Model: {patcher.model_name}\")\n    print(f\"  Total patterns available: {len(patterns) if 'patterns' in globals() else 'Not loaded'}\")\n    print(f\"  Current batch size: {len(batch_texts) if 'batch_texts' in globals() else 'Not configured'}\")\n    \n# 4. Quick test batch configuration\ndef test_small_batch():\n    \"\"\"Run a quick test with minimal batch to check everything works\"\"\"\n    test_texts = [\n        \"I choose to focus on positive solutions.\",\n        \"I can handle this step by step.\",\n        \"I'm grateful for my progress so far.\"\n    ]\n    \n    try:\n        patcher.reset_hooks()\n        predicted_token, generated_text = patcher.batch_patch_and_generate(\n            clean_texts=test_texts,\n            corrupted_text=\"I feel stuck and don't know\",\n            capture_layer_idx=-1,\n            patch_layer_idx=-1,\n            aggregation=\"mean\",\n            num_placeholder_tokens=3,\n            max_new_tokens=30\n        )\n        print(\"âœ… Batch system working correctly!\")\n        print(f\"Test result: {generated_text}\")\n        return True\n    except Exception as e:\n        print(f\"âŒ Batch system error: {e}\")\n        return False\n\n# Available batch utilities\nprint(\"Available batch utilities:\")\nprint(\"- batch_reset() - Reset hooks for batch experiments\")\nprint(\"- batch_memory_cleanup() - Clear memory + reset hooks\")\nprint(\"- batch_status() - Check experiment configuration\") \nprint(\"- test_small_batch() - Run quick functionality test\")\nprint(\"- patcher.reset_hooks() - Direct reset call\")\n\n# Uncomment to run:\n# batch_reset()\n# batch_status()\n# test_small_batch()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ðŸ› ï¸ Batch Experiment Utilities",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Multi-Layer Batch Patching\n\nThe batch activation patcher now supports the same multi-layer functionality as the single-text patcher:\n\n### New Parameters:\n- **`capture_layer_idx`**: Layer to capture activations from (replaces `layer_idx`)\n- **`patch_layer_idx`**: Layer to patch activations into (defaults to `capture_layer_idx`)\n\n### Current Limitations:\n- Batch mode currently supports single patch layer (first layer if multiple specified)\n- Full multi-layer patch broadcasting planned for future versions\n\n### Batch + Multi-Layer Benefits:\n- **Robust activation extraction**: Aggregate patterns from multiple texts\n- **Cross-layer analysis**: Capture from one layer, patch to another\n- **Reduced noise**: Statistical aggregation reduces individual text variations\n- **Better generalization**: Combined patterns are more representative\n\n### Usage Examples:\n- `capture_layer_idx=0, patch_layer_idx=-1`: Early capture â†’ Late patch\n- `capture_layer_idx=[0,1,2], patch_layer_idx=5`: Multi-capture â†’ Single patch\n- `capture_layer_idx='all', patch_layer_idx=-1`: All layers capture â†’ Late patch",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BATCH ACTIVATION PATCHING EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\nModel Used: {patcher.model_name}\")\n",
    "print(f\"Model Info: {patcher.get_model_info()}\")\n",
    "\n",
    "print(f\"\\nDataset: {len(patterns)} total patterns\")\n",
    "print(f\"Batch Size Tested: {len(batch_texts)} texts\")\n",
    "\n",
    "experiments = [\n",
    "    (\"Aggregation Methods\", globals().get('aggregation_results', {})),\n",
    "    (\"Batch Sizes\", globals().get('batch_size_results', {})),\n",
    "    (\"Baseline Comparison\", globals().get('comparison_results', {}))\n",
    "]\n",
    "\n",
    "for exp_name, results in experiments:\n",
    "    print(f\"\\n{exp_name}:\")\n",
    "    if results:\n",
    "        success_count = sum(1 for r in results.values() if isinstance(r, dict) and r.get('success', False))\n",
    "        total_count = len(results)\n",
    "        print(f\"  - Completed: {success_count}/{total_count} configurations\")\n",
    "        \n",
    "        if success_count > 0:\n",
    "            successful_configs = [k for k, v in results.items() if isinstance(v, dict) and v.get('success', False)]\n",
    "            if not successful_configs:\n",
    "                successful_configs = [k for k in results.keys() if not str(results[k]).startswith('Error')]\n",
    "            print(f\"  - Successful configurations: {successful_configs}\")\n",
    "    else:\n",
    "        print(f\"  - Not run in this session\")\n",
    "\n",
    "print(\"\\nKey Findings:\")\n",
    "print(\"- Batch activation patching allows for more robust activation patterns\")\n",
    "print(\"- Different aggregation methods can produce varied results\")\n",
    "print(\"- Batch size affects the quality and consistency of patching\")\n",
    "print(\"- Comparison with baseline shows the effect of activation patching\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"- Try different models by changing MODEL_NAME and re-running\")\n",
    "print(\"- Experiment with different cognitive pattern types\")\n",
    "print(\"- Test with longer generation sequences\")\n",
    "print(\"- Try different layers for patching\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}