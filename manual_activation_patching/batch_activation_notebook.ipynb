{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Activation Patching Experiments\n",
    "\n",
    "This notebook allows you to experiment with batch activation extraction and aggregation. Instead of using activations from a single text, you can:\n",
    "\n",
    "1. **Extract activations from multiple positive texts**\n",
    "2. **Aggregate them using different methods** (mean, median, max, etc.)\n",
    "3. **Use the aggregated activation for patching**\n",
    "4. **Compare different aggregation strategies**\n",
    "\n",
    "This approach may provide more robust and generalizable activation patterns for patching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Add TransformerLens to path\n",
    "sys.path.append('/home/koalacrown/Desktop/Code/Projects/turnaround/turn_point/third_party/TransformerLens')\n",
    "\n",
    "# Import our activation patcher\n",
    "sys.path.append('/home/koalacrown/Desktop/Code/Projects/turnaround/turn_point/manual_activation_patching')\n",
    "from activation_patcher import ActivationPatcher\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Model and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your model here - change this and re-run to experiment with different models\n",
    "MODEL_NAME = \"gpt2-small\"  # Change to: gpt2-medium, EleutherAI/gpt-neo-125m, etc.\n",
    "\n",
    "# Initialize the activation patcher\n",
    "patcher = ActivationPatcher(MODEL_NAME)\n",
    "\n",
    "# Load the positive patterns dataset\n",
    "dataset_path = \"/home/koalacrown/Desktop/Code/Projects/turnaround/turn_point/data/final/positive_patterns.jsonl\"\n",
    "patterns = patcher.load_dataset(dataset_path)\n",
    "\n",
    "print(f\"Loaded {len(patterns)} positive thought patterns\")\n",
    "print(f\"Model info: {patcher.get_model_info()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Available Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all supported models\n",
    "ActivationPatcher.list_supported_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select patterns by cognitive type or randomly\n",
    "def filter_patterns_by_type(patterns, pattern_type=None, max_count=20):\n",
    "    \"\"\"Filter patterns by cognitive type or return random selection.\"\"\"\n",
    "    if pattern_type:\n",
    "        filtered = [p for p in patterns if pattern_type.lower() in p.get('cognitive_pattern_type', '').lower()]\n",
    "        print(f\"Found {len(filtered)} patterns matching '{pattern_type}'\")\n",
    "    else:\n",
    "        filtered = patterns.copy()\n",
    "        random.shuffle(filtered)\n",
    "        print(f\"Using random selection from {len(filtered)} patterns\")\n",
    "    \n",
    "    return filtered[:max_count]\n",
    "\n",
    "# Available cognitive pattern types in the dataset:\n",
    "pattern_types = set([p.get('cognitive_pattern_type', '') for p in patterns])\n",
    "print(\"Available cognitive pattern types:\")\n",
    "for ptype in sorted(pattern_types):\n",
    "    if ptype:\n",
    "        count = len([p for p in patterns if p.get('cognitive_pattern_type', '') == ptype])\n",
    "        print(f\"  - {ptype} ({count} examples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for batch experiments\n",
    "BATCH_SIZE = 15  # Number of texts to use for batch activation extraction\n",
    "PATTERN_TYPE = None  # Set to specific type like \"rumination\" or None for random\n",
    "\n",
    "# Select patterns for batch\n",
    "selected_patterns = filter_patterns_by_type(patterns, PATTERN_TYPE, BATCH_SIZE)\n",
    "\n",
    "# Extract texts\n",
    "batch_texts = [p['positive_thought_pattern'] for p in selected_patterns]\n",
    "\n",
    "print(f\"\\nSelected {len(batch_texts)} texts for batch processing:\")\n",
    "for i, text in enumerate(batch_texts[:5]):  # Show first 5\n",
    "    print(f\"{i+1}. {text[:100]}...\")\n",
    "if len(batch_texts) > 5:\n",
    "    print(f\"... and {len(batch_texts) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Different Aggregation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different aggregation methods\n",
    "aggregation_methods = [\"mean\", \"median\", \"max\", \"random\"]\n",
    "corrupted_prompt = \"I feel completely overwhelmed and stuck, unable to\"\n",
    "target_words = [\"positive\", \"solutions\", \"growth\", \"hope\", \"progress\"]\n",
    "\n",
    "print(f\"Testing aggregation methods with:\")\n",
    "print(f\"- Batch size: {len(batch_texts)}\")\n",
    "print(f\"- Corrupted prompt: {corrupted_prompt}\")\n",
    "print(f\"- Target words: {target_words}\")\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "aggregation_results = {}\n",
    "\n",
    "for method in aggregation_methods:\n",
    "    print(f\"\\n--- Testing {method.upper()} aggregation ---\")\n",
    "    \n",
    "    try:\n",
    "        predicted_token, generated_text = patcher.batch_patch_and_generate(\n",
    "            clean_texts=batch_texts,\n",
    "            corrupted_text=corrupted_prompt,\n",
    "            layer_idx=-1,\n",
    "            aggregation=method,\n",
    "            target_words=target_words,\n",
    "            num_placeholder_tokens=5,\n",
    "            max_new_tokens=60\n",
    "        )\n",
    "        \n",
    "        aggregation_results[method] = {\n",
    "            'success': True,\n",
    "            'predicted_token': predicted_token,\n",
    "            'generated_text': generated_text\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n✓ SUCCESS with {method} aggregation:\")\n",
    "        print(f\"Generated: {generated_text}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error with {method} aggregation: {e}\")\n",
    "        aggregation_results[method] = {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'generated_text': None\n",
    "        }\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Memory cleanup\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Batch Size Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different batch sizes\n",
    "batch_sizes = [1, 3, 5, 10, 15]\n",
    "corrupted_prompt = \"My thoughts are spiraling and I can't seem to\"\n",
    "aggregation_method = \"mean\"\n",
    "\n",
    "print(f\"Testing batch sizes with {aggregation_method} aggregation\")\n",
    "print(f\"Corrupted prompt: {corrupted_prompt}\")\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "batch_size_results = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    if batch_size > len(batch_texts):\n",
    "        print(f\"Skipping batch size {batch_size} (not enough texts available)\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n--- Testing batch size: {batch_size} ---\")\n",
    "    \n",
    "    subset_texts = batch_texts[:batch_size]\n",
    "    \n",
    "    try:\n",
    "        predicted_token, generated_text = patcher.batch_patch_and_generate(\n",
    "            clean_texts=subset_texts,\n",
    "            corrupted_text=corrupted_prompt,\n",
    "            layer_idx=-1,\n",
    "            aggregation=aggregation_method,\n",
    "            target_words=None,\n",
    "            num_placeholder_tokens=5,\n",
    "            max_new_tokens=55\n",
    "        )\n",
    "        \n",
    "        batch_size_results[batch_size] = {\n",
    "            'success': True,\n",
    "            'generated_text': generated_text,\n",
    "            'predicted_token': predicted_token\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n✓ SUCCESS with batch size {batch_size}:\")\n",
    "        print(f\"Generated: {generated_text}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error with batch size {batch_size}: {e}\")\n",
    "        batch_size_results[batch_size] = {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'generated_text': None\n",
    "        }\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Custom Batch Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom configuration - modify these variables for your experiments\n",
    "CUSTOM_BATCH_TEXTS = [\n",
    "    \"I'm learning to acknowledge my feelings and take things one step at a time.\",\n",
    "    \"I choose to focus on solutions rather than dwelling on problems.\",\n",
    "    \"I'm practicing self-compassion and recognizing my growth.\",\n",
    "    \"I can handle challenges by breaking them into manageable pieces.\",\n",
    "    \"I'm building resilience and finding healthy ways to cope.\",\n",
    "    \"I trust in my ability to navigate difficult situations.\",\n",
    "    \"I'm grateful for the support I have and the progress I've made.\",\n",
    "    \"I choose hope and believe that positive change is possible.\"\n",
    "]\n",
    "\n",
    "CUSTOM_CORRUPTED_TEXT = \"I don't know how to deal with this situation and feel\"\n",
    "CUSTOM_AGGREGATION = \"mean\"\n",
    "CUSTOM_TARGET_WORDS = [\"acknowledge\", \"focus\", \"solutions\", \"growth\", \"hope\"]\n",
    "CUSTOM_LAYER = -1\n",
    "CUSTOM_PLACEHOLDERS = 5\n",
    "CUSTOM_MAX_TOKENS = 70\n",
    "\n",
    "print(\"CUSTOM BATCH EXPERIMENT:\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Batch texts ({len(CUSTOM_BATCH_TEXTS)} total):\")\n",
    "for i, text in enumerate(CUSTOM_BATCH_TEXTS[:3]):\n",
    "    print(f\"  {i+1}. {text}\")\n",
    "if len(CUSTOM_BATCH_TEXTS) > 3:\n",
    "    print(f\"  ... and {len(CUSTOM_BATCH_TEXTS) - 3} more\")\n",
    "\n",
    "print(f\"\\nCorrupted text: {CUSTOM_CORRUPTED_TEXT}\")\n",
    "print(f\"Aggregation: {CUSTOM_AGGREGATION}\")\n",
    "print(f\"Target words: {CUSTOM_TARGET_WORDS}\")\n",
    "print(f\"Layer: {CUSTOM_LAYER}\")\n",
    "print(f\"Placeholders: {CUSTOM_PLACEHOLDERS}\")\n",
    "print(f\"Max tokens: {CUSTOM_MAX_TOKENS}\")\n",
    "\n",
    "try:\n",
    "    predicted_token, generated_text = patcher.batch_patch_and_generate(\n",
    "        clean_texts=CUSTOM_BATCH_TEXTS,\n",
    "        corrupted_text=CUSTOM_CORRUPTED_TEXT,\n",
    "        layer_idx=CUSTOM_LAYER,\n",
    "        aggregation=CUSTOM_AGGREGATION,\n",
    "        target_words=CUSTOM_TARGET_WORDS,\n",
    "        num_placeholder_tokens=CUSTOM_PLACEHOLDERS,\n",
    "        max_new_tokens=CUSTOM_MAX_TOKENS\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CUSTOM EXPERIMENT RESULT:\")\n",
    "    print(\"=\"*50)\n",
    "    print(generated_text)\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Error in custom experiment: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Baseline Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison experiment\n",
    "comparison_prompt = \"I'm overwhelmed by everything and don't see a way to\"\n",
    "batch_for_comparison = batch_texts[:10]\n",
    "single_text_for_comparison = batch_texts[0]\n",
    "\n",
    "print(\"BASELINE COMPARISON:\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Test prompt: {comparison_prompt}\")\n",
    "print(f\"Batch size: {len(batch_for_comparison)}\")\n",
    "\n",
    "comparison_results = {}\n",
    "\n",
    "# 1. No patching (baseline)\n",
    "print(\"\\n--- 1. NO PATCHING (Baseline) ---\")\n",
    "try:\n",
    "    tokens = patcher.model.to_tokens(comparison_prompt)\n",
    "    generated_tokens = patcher.model.generate(\n",
    "        tokens,\n",
    "        max_new_tokens=60,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "    baseline_text = patcher.model.to_string(generated_tokens[0])\n",
    "    comparison_results['no_patching'] = baseline_text\n",
    "    print(f\"Generated: {baseline_text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    comparison_results['no_patching'] = f\"Error: {e}\"\n",
    "\n",
    "# 2. Single text patching\n",
    "print(\"\\n--- 2. SINGLE TEXT PATCHING ---\")\n",
    "try:\n",
    "    target_words = patcher._extract_key_words(single_text_for_comparison)\n",
    "    predicted_token, single_patch_text = patcher.patch_and_generate(\n",
    "        clean_text=single_text_for_comparison,\n",
    "        corrupted_text=comparison_prompt,\n",
    "        target_words=target_words,\n",
    "        max_new_tokens=60\n",
    "    )\n",
    "    comparison_results['single_patching'] = single_patch_text\n",
    "    print(f\"Generated: {single_patch_text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    comparison_results['single_patching'] = f\"Error: {e}\"\n",
    "\n",
    "# 3. Batch patching\n",
    "print(\"\\n--- 3. BATCH PATCHING ---\")\n",
    "try:\n",
    "    predicted_token, batch_patch_text = patcher.batch_patch_and_generate(\n",
    "        clean_texts=batch_for_comparison,\n",
    "        corrupted_text=comparison_prompt,\n",
    "        aggregation=\"mean\",\n",
    "        max_new_tokens=60\n",
    "    )\n",
    "    comparison_results['batch_patching'] = batch_patch_text\n",
    "    print(f\"Generated: {batch_patch_text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    comparison_results['batch_patching'] = f\"Error: {e}\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPARISON SUMMARY:\")\n",
    "print(\"=\"*100)\n",
    "for method, result in comparison_results.items():\n",
    "    print(f\"\\n{method.upper().replace('_', ' ')}:\")\n",
    "    print(result)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BATCH ACTIVATION PATCHING EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\nModel Used: {patcher.model_name}\")\n",
    "print(f\"Model Info: {patcher.get_model_info()}\")\n",
    "\n",
    "print(f\"\\nDataset: {len(patterns)} total patterns\")\n",
    "print(f\"Batch Size Tested: {len(batch_texts)} texts\")\n",
    "\n",
    "experiments = [\n",
    "    (\"Aggregation Methods\", globals().get('aggregation_results', {})),\n",
    "    (\"Batch Sizes\", globals().get('batch_size_results', {})),\n",
    "    (\"Baseline Comparison\", globals().get('comparison_results', {}))\n",
    "]\n",
    "\n",
    "for exp_name, results in experiments:\n",
    "    print(f\"\\n{exp_name}:\")\n",
    "    if results:\n",
    "        success_count = sum(1 for r in results.values() if isinstance(r, dict) and r.get('success', False))\n",
    "        total_count = len(results)\n",
    "        print(f\"  - Completed: {success_count}/{total_count} configurations\")\n",
    "        \n",
    "        if success_count > 0:\n",
    "            successful_configs = [k for k, v in results.items() if isinstance(v, dict) and v.get('success', False)]\n",
    "            if not successful_configs:\n",
    "                successful_configs = [k for k in results.keys() if not str(results[k]).startswith('Error')]\n",
    "            print(f\"  - Successful configurations: {successful_configs}\")\n",
    "    else:\n",
    "        print(f\"  - Not run in this session\")\n",
    "\n",
    "print(\"\\nKey Findings:\")\n",
    "print(\"- Batch activation patching allows for more robust activation patterns\")\n",
    "print(\"- Different aggregation methods can produce varied results\")\n",
    "print(\"- Batch size affects the quality and consistency of patching\")\n",
    "print(\"- Comparison with baseline shows the effect of activation patching\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"- Try different models by changing MODEL_NAME and re-running\")\n",
    "print(\"- Experiment with different cognitive pattern types\")\n",
    "print(\"- Test with longer generation sequences\")\n",
    "print(\"- Try different layers for patching\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}