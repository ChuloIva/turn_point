{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Activation Patching Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports and utilities loaded successfully!\n",
      "Available token strategies: ['keywords', 'mid_tokens', 'last_couple', 'last_token', 'all_tokens']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Add TransformerLens to path\n",
    "sys.path.append('/Users/ivanculo/Desktop/Projects/turn_point/third_party/TransformerLens')\n",
    "\n",
    "# Import our activation patcher with all utilities\n",
    "sys.path.append('/Users/ivanculo/Desktop/Projects/turn_point/manual_activation_patching')\n",
    "from activation_patcher import ActivationPatcher, TokenSelectionStrategy\n",
    "from interpretation_templates import INTERPRETATION_TEMPLATES\n",
    "\n",
    "# Import utility functions from ActivationPatcher class\n",
    "load_cognitive_patterns = ActivationPatcher.load_cognitive_patterns\n",
    "get_pattern_by_index = ActivationPatcher.get_pattern_by_index\n",
    "get_pattern_by_type = ActivationPatcher.get_pattern_by_type\n",
    "get_random_pattern_by_type = ActivationPatcher.get_random_pattern_by_type\n",
    "get_patterns_by_type = ActivationPatcher.get_patterns_by_type\n",
    "list_available_pattern_types = ActivationPatcher.list_available_pattern_types\n",
    "get_pattern_text = ActivationPatcher.get_pattern_text\n",
    "get_template = ActivationPatcher.get_template\n",
    "show_pattern_info = ActivationPatcher.show_pattern_info\n",
    "clear_memory = ActivationPatcher.clear_memory\n",
    "filter_patterns_by_count = ActivationPatcher.filter_patterns_by_count\n",
    "get_filtered_patterns_by_type = ActivationPatcher.get_filtered_patterns_by_type\n",
    "\n",
    "print(\"Imports and utilities loaded successfully!\")\n",
    "print(f\"Available token strategies: {[s.value for s in TokenSelectionStrategy]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Model and Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How activation patching works here (what goes where)\n",
    "\n",
    "- **Source sentence (we capture from)**: `clean_text`\n",
    "  - Set in each experiment (e.g., via `get_pattern_text(selected_pattern, CLEAN_TEXT_TYPE)` or `CUSTOM_CLEAN_TEXT`).\n",
    "  - **Capture layer**: `capture_layer_idx` (resid_post at that layer). `-1` = last layer.\n",
    "  - **Token selection**: `token_selection_strategy` chooses which token positions in `clean_text` provide activations (e.g., `KEYWORDS`, `MID_TOKENS`, `LAST_COUPLE`, `LAST_TOKEN`, `ALL_TOKENS`).\n",
    "\n",
    "- **Target prompt (we patch into)**: depends on mode\n",
    "  - **Zero-placeholder mode** (`zero_placeholder_mode=True`): use either a named `template_name` (with `0` markers) or a `prompt_input` string containing standalone `0` tokens. Those `0` positions are rendered as `<eos>` and become patch points.\n",
    "  - **Placeholder-token mode** (`zero_placeholder_mode=False`): the notebook auto‚Äëprepends `num_placeholder_tokens` copies of `<eos>` to `corrupted_text`; those initial tokens are the patch points.\n",
    "\n",
    "- **Where we patch (layer)**: `patch_layer_idx` (resid_post). Defaults to `capture_layer_idx` if not provided. Can be an int or list of ints.\n",
    "\n",
    "- **Generation**: We run with hooks so the model uses patched activations when computing next token logits and while generating continuation text.\n",
    "\n",
    "- **Mental model**: Capture activations from `clean_text` at selected tokens ‚Üí inject them at `<eos>` positions in the target prompt ‚Üí generate continuation while hooks are active.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: google/gemma-2-2b-it not in supported models list. Attempting to load anyway.\n",
      "Supported models: ['gpt2-small', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'EleutherAI/gpt-j-6b', 'EleutherAI/gpt-neo-125m', 'EleutherAI/gpt-neo-1.3b', 'EleutherAI/gpt-neo-2.7b', 'facebook/opt-125m', 'facebook/opt-1.3b', 'facebook/opt-2.7b', 'facebook/opt-6.7b', 'EleutherAI/pythia-70m', 'EleutherAI/pythia-160m', 'EleutherAI/pythia-410m', 'EleutherAI/pythia-1b', 'EleutherAI/pythia-1.4b', 'EleutherAI/pythia-2.8b', 'google/gemma-2b', 'google/gemma-7b']\n",
      "Using Apple Silicon GPU (MPS)\n",
      "Loading model: google/gemma-2-2b-it on device: mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f186e6bd90c046fb980aa1e950f68abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b-it into HookedTransformer\n",
      "‚úì Model loaded successfully\n",
      "  - Family: unknown\n",
      "  - Size: unknown\n",
      "  - Layers: 26\n",
      "  - Model dimension: 2304\n",
      "  - Vocabulary size: 256000\n",
      "Loaded 520 cognitive patterns (full dataset)\n",
      "Using BOS token: '<bos>'\n",
      "Available cognitive pattern types:\n",
      " 1. Cognitive depletion pattern (40 examples)\n",
      " 2. Intrusive suicidal fixation (40 examples)\n",
      " 3. Negative self-evaluative loop (40 examples)\n",
      " 4. Internal dialectical processing (40 examples)\n",
      " 5. Fragmented perceptual reasoning (40 examples)\n",
      " 6. Hyper-attuned interoception (40 examples)\n",
      " 7. Autobiographical integration (40 examples)\n",
      " 8. Over-elaborative recounting (40 examples)\n",
      " 9. Entrapment cognition (40 examples)\n",
      "10. Existential rumination (40 examples)\n",
      "11. Learned helplessness loop (40 examples)\n",
      "12. Instrumental suicidal reasoning (40 examples)\n",
      "13. Cognitive disorganization (40 examples)\n",
      "Available text types: positive, negative, transition\n",
      "\n",
      "Available interpretation templates: ['cognitive_pattern', 'emotional_state', 'general_concept', 'decision_making']\n",
      "\n",
      "üí° Each experiment can now filter to use 1-40 examples per pattern type at runtime!\n"
     ]
    }
   ],
   "source": [
    "# Choose your model here - change this and re-run to experiment with different models\n",
    "MODEL_NAME = \"google/gemma-2-2b-it\"  # Change to: gpt2-medium, EleutherAI/gpt-neo-125m, etc.\n",
    "\n",
    "# # Set the BOS token for your model (change this based on your model)\n",
    "BOS_TOKEN = \"<bos>\"  # Default for Gemma models\n",
    "\n",
    "# # Initialize the activation patcher\n",
    "patcher = ActivationPatcher(MODEL_NAME)\n",
    "\n",
    "# Load the FULL cognitive patterns dataset (all 40 examples per pattern type)\n",
    "patterns, pattern_types = load_cognitive_patterns()\n",
    "\n",
    "print(f\"Loaded {len(patterns)} cognitive patterns (full dataset)\")\n",
    "# print(f\"Model info: {patcher.get_model_info()}\")\n",
    "print(f\"Using BOS token: '{BOS_TOKEN}'\")\n",
    "\n",
    "# Show available pattern types\n",
    "list_available_pattern_types(pattern_types)\n",
    "print(f\"Available text types: positive, negative, transition\")\n",
    "print(f\"\\nAvailable interpretation templates: {list(INTERPRETATION_TEMPLATES.keys())}\")\n",
    "print(f\"\\nüí° Each experiment can now filter to use 1-40 examples per pattern type at runtime!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° RUNTIME FILTERING READY:\n",
      "‚úÖ Full dataset loaded: 520 patterns (40 examples √ó 13 types)\n",
      "üéõÔ∏è  Available filtering functions:\n",
      "   ‚Ä¢ filter_patterns_by_count(pattern_types, num_examples_per_type)\n",
      "   ‚Ä¢ get_filtered_patterns_by_type(pattern_types, pattern_type, num_examples)\n",
      "\n",
      "üìù Each experiment can now choose 1-40 examples per type at runtime!\n"
     ]
    }
   ],
   "source": [
    "# ===== RUNTIME FILTERING UTILITIES =====\n",
    "# These functions let you filter the full dataset at experiment time\n",
    "# No need to reload data - just specify how many examples you want (1-40 per pattern type)\n",
    "\n",
    "# Import the new filtering functions for easy access\n",
    "filter_patterns_by_count = ActivationPatcher.filter_patterns_by_count\n",
    "get_filtered_patterns_by_type = ActivationPatcher.get_filtered_patterns_by_type\n",
    "\n",
    "def show_filtering_example():\n",
    "    \"\"\"Demonstrate how to use runtime filtering.\"\"\"\n",
    "    print(\"üéØ RUNTIME FILTERING EXAMPLES:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Example 1: Filter entire dataset to use only 5 examples per type\n",
    "    filtered_patterns_5, filtered_types_5 = filter_patterns_by_count(pattern_types, 5)\n",
    "    print(f\"üìä Filtered to 5 examples per type: {len(filtered_patterns_5)} total patterns\")\n",
    "    \n",
    "    # Example 2: Get only 3 examples of a specific pattern type\n",
    "    specific_patterns = get_filtered_patterns_by_type(pattern_types, \"Negative self-evaluative loop\", 3)\n",
    "    print(f\"üìù Got {len(specific_patterns)} examples of 'Negative self-evaluative loop'\")\n",
    "    \n",
    "    print(f\"\\nüí° Use these in your experiments:\")\n",
    "    print(f\"   filtered_patterns, filtered_types = filter_patterns_by_count(pattern_types, NUM_EXAMPLES)\")\n",
    "    print(f\"   specific_patterns = get_filtered_patterns_by_type(pattern_types, PATTERN_TYPE, NUM_EXAMPLES)\")\n",
    "\n",
    "print(f\"üí° RUNTIME FILTERING READY:\")\n",
    "print(f\"‚úÖ Full dataset loaded: {len(patterns)} patterns (40 examples √ó 13 types)\")\n",
    "print(f\"üéõÔ∏è  Available filtering functions:\")\n",
    "print(f\"   ‚Ä¢ filter_patterns_by_count(pattern_types, num_examples_per_type)\")\n",
    "print(f\"   ‚Ä¢ get_filtered_patterns_by_type(pattern_types, pattern_type, num_examples)\")\n",
    "print(f\"\\nüìù Each experiment can now choose 1-40 examples per type at runtime!\")\n",
    "\n",
    "# Uncomment to see filtering examples:\n",
    "# show_filtering_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Basic Activation Patching\n",
    "\n",
    "Let's start with a simple experiment using one positive pattern:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What goes where in Experiment 1\n",
    "\n",
    "- **We capture from**: `clean_text` built from the dataset using `CLEAN_TEXT_TYPE`.\n",
    "- **We patch into**: a prompt constructed from the `template_name` with `0` markers (zero‚Äëplaceholder mode). Each `0` is rendered as `<eos>` and marks a patch point in the prompt.\n",
    "- **Capture layer**: `capture_layer_idx=-1` (last layer, resid_post).\n",
    "- **Patch layer**: `patch_layer_idx=-1` (same last layer).\n",
    "- **Tokens used to extract activations**: `TOKEN_STRATEGY` (default shown is `LAST_COUPLE` averaging the last few tokens).\n",
    "- **Why**: This injects the end‚Äëof‚Äëthought representation from the clean example into the template to steer generation at the `<eos>` slots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 1: Basic Activation Patching with Template (Zero-Placeholder Mode)\n",
    "\n",
    "# ===== MODULAR CONFIGURATION SECTION =====\n",
    "# Easily modify these parameters to experiment with different settings\n",
    "NUM_EXAMPLES_PER_TYPE = 10  # Number of examples to use per pattern type (1-40)\n",
    "PATTERN_TYPE = \"Negative self-evaluative loop\"  # Choose from the list above\n",
    "PATTERN_INDEX_WITHIN_TYPE = 0  # If there are multiple examples of this type, choose which one (0-based)\n",
    "CLEAN_TEXT_TYPE = \"positive\"  # \"positive\", \"negative\", or \"transition\"\n",
    "TEMPLATE_NAME = \"cognitive_pattern\"  # Template to use for corrupted text\n",
    "TOKEN_STRATEGY = TokenSelectionStrategy.LAST_COUPLE  # Change to: MID_TOKENS, LAST_COUPLE, LAST_TOKEN, ALL_TOKENS\n",
    "# ============================================\n",
    "\n",
    "# Apply runtime filtering to use only NUM_EXAMPLES_PER_TYPE examples per pattern type\n",
    "filtered_patterns, filtered_pattern_types = filter_patterns_by_count(pattern_types, NUM_EXAMPLES_PER_TYPE)\n",
    "\n",
    "# Load the selected pattern and template from filtered dataset\n",
    "selected_pattern = get_pattern_by_type(filtered_pattern_types, PATTERN_TYPE, PATTERN_INDEX_WITHIN_TYPE)\n",
    "template = get_template(TEMPLATE_NAME)\n",
    "\n",
    "# Get the clean text (what we want to capture activations from)\n",
    "clean_text = get_pattern_text(selected_pattern, CLEAN_TEXT_TYPE)\n",
    "\n",
    "print(f\"üìã EXPERIMENT 1 SETUP:\")\n",
    "print(f\"üìä Using {NUM_EXAMPLES_PER_TYPE} examples per pattern type (filtered from full dataset)\")\n",
    "print(f\"üî¢ Total patterns available: {len(filtered_patterns)} (was {len(patterns)})\")\n",
    "print(f\"‚úÖ Pattern Type: {PATTERN_TYPE}\")\n",
    "print(f\"üìç Pattern Index within Type: {PATTERN_INDEX_WITHIN_TYPE}\")\n",
    "print(f\"üß† Pattern Name: {selected_pattern['cognitive_pattern_name']}\")\n",
    "print(f\"üìù Clean Text Type: {CLEAN_TEXT_TYPE}\")\n",
    "print(f\"üìÑ Clean Text: {clean_text}\")\n",
    "print(f\"\\nüéØ Using Template: '{TEMPLATE_NAME}'\")\n",
    "print(f\"üîß Token Strategy: {TOKEN_STRATEGY.value}\")\n",
    "\n",
    "# Run activation patching using zero-placeholder mode\n",
    "print(f\"\\n‚ö° Running activation patching (zero-placeholder mode)...\")\n",
    "print(f\"  Clean text: {clean_text[:100]}...\")\n",
    "\n",
    "predicted_token, generated_text = patcher.patch_and_generate(\n",
    "    clean_text=clean_text,\n",
    "    corrupted_text=None,  # ignored in zero-placeholder mode when template_name is provided\n",
    "    capture_layer_idx=-1,  # Last layer for capture\n",
    "    patch_layer_idx=-1,    # Last layer for patch\n",
    "    max_new_tokens=60,\n",
    "    bos_token=BOS_TOKEN,\n",
    "    token_selection_strategy=TOKEN_STRATEGY,\n",
    "    num_strategy_tokens=5,\n",
    "    zero_placeholder_mode=True,\n",
    "    template_name=TEMPLATE_NAME  # use template with '0' markers\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéä EXPERIMENT 1 RESULTS:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {patcher.model_name}\")\n",
    "print(f\"Using {NUM_EXAMPLES_PER_TYPE} examples per pattern type (runtime filtered)\")\n",
    "print(f\"Pattern Type: {PATTERN_TYPE}\")\n",
    "print(f\"Pattern Name: {selected_pattern['cognitive_pattern_name']}\")\n",
    "print(f\"Clean text type: {CLEAN_TEXT_TYPE}\")\n",
    "print(f\"Template: {TEMPLATE_NAME}\")\n",
    "print(f\"Token strategy: {TOKEN_STRATEGY.value}\")\n",
    "print(f\"\\nüìä Generated Text:\")\n",
    "print(f\"{generated_text}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual zero‚Äëplaceholder prompt (what gets patched where)\n",
    "\n",
    "- **We capture from**: the same `clean_text` as Experiment 1.\n",
    "- **We patch into**: `manual_prompt` where each standalone `0` is rendered as `<eos>` and becomes a patch point.\n",
    "- **Positions chosen**: The exact tokenized positions of the `<eos>` markers after rendering are located and used for patching.\n",
    "- **Strategy/layers**: `TokenSelectionStrategy.LAST_COUPLE` from `capture_layer_idx=-1` ‚Üí patched into `patch_layer_idx=-1`.\n",
    "- **Tip**: Add or remove `0` markers to control how many `<eos>` positions get injected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual prompt with '0' markers (zero-placeholder mode)\n",
    "# Example: you can place '0' anywhere you want activations patched\n",
    "manual_prompt = \"0 0 0 I need to shift my perspective and find a constructive way to\"\n",
    "\n",
    "print(f\"Manual prompt: {manual_prompt}\")\n",
    "\n",
    "predicted_token, generated_text = patcher.patch_and_generate(\n",
    "    clean_text=clean_text,\n",
    "    corrupted_text=None,  # ignored in zero-placeholder mode\n",
    "    capture_layer_idx=-1,\n",
    "    patch_layer_idx=-1,\n",
    "    max_new_tokens=60,\n",
    "    bos_token=BOS_TOKEN,\n",
    "    token_selection_strategy=TokenSelectionStrategy.LAST_COUPLE,\n",
    "    num_strategy_tokens=3,\n",
    "    zero_placeholder_mode=True,\n",
    "    prompt_input=manual_prompt\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"MANUAL PROMPT RESULTS:\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Model: {patcher.model_name}\")\n",
    "print(f\"BOS token used: '{BOS_TOKEN}'\")\n",
    "print(f\"Clean text (first 100 chars): {clean_text[:100]}...\")\n",
    "print(f\"\\nGenerated Text:\\n{generated_text}\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üîÑ Reset Model State\n",
    "\n",
    "Before running experiments, it's good practice to reset any lingering hooks from previous runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÑ RESET MODEL HOOKS - Run this cell to reset the model to clean state\n",
    "# This is especially important when switching between different experiments\n",
    "\n",
    "patcher.reset_hooks()\n",
    "\n",
    "print(\"Model is now ready for clean experiments!\")\n",
    "print(\"Run this cell anytime you want to ensure no residual hooks are affecting your results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Different Layers Comparison\n",
    "\n",
    "Let's see how patching at different layers affects the output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What goes where in Experiment 2 (layer sweep)\n",
    "\n",
    "- **We capture from**: `clean_text` of `sample_pattern`.\n",
    "- **We patch into**: `corrupted_text` with `num_placeholder_tokens=5` `<eos>` tokens prepended (non‚Äëzero mode).\n",
    "- **Capture/Patch layers**: same `layer_idx` per loop (tests early‚Üílate layers including `-1`).\n",
    "- **Token selection**: `TokenSelectionStrategy.KEYWORDS` from the clean input.\n",
    "- **Goal**: Compare how different patch layers influence continuation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 2: Different Layers Comparison\n",
    "\n",
    "# ===== MODULAR CONFIGURATION SECTION =====\n",
    "NUM_EXAMPLES = 15  # Number of examples to use from dataset (1-40)\n",
    "LAYERS_TO_TEST = [0, 3, 6, 9, -1]  # Early, middle, late, and final layers\n",
    "TOKEN_STRATEGY = TokenSelectionStrategy.KEYWORDS  # Change to: MID_TOKENS, LAST_COUPLE, LAST_TOKEN, ALL_TOKENS\n",
    "PATTERN_INDEX = 5  # Which pattern from full dataset to use (0-519)\n",
    "CORRUPTED_TEXT = \"I can't stop worrying about everything and feel completely\"\n",
    "MAX_NEW_TOKENS = 50\n",
    "# ============================================\n",
    "\n",
    "# Get limited patterns based on NUM_EXAMPLES\n",
    "limited_patterns = patterns[:NUM_EXAMPLES * 13]  # 13 pattern types * NUM_EXAMPLES each\n",
    "sample_pattern = patterns[PATTERN_INDEX] if PATTERN_INDEX < len(patterns) else patterns[5]\n",
    "\n",
    "clean_text = sample_pattern['positive_thought_pattern']\n",
    "\n",
    "print(f\"üìã EXPERIMENT 2 SETUP - LAYER COMPARISON:\")\n",
    "print(f\"üìä Using {NUM_EXAMPLES} examples per pattern type from dataset\")\n",
    "print(f\"üß† Pattern: {sample_pattern['cognitive_pattern_name']}\")\n",
    "print(f\"üîß Token strategy: {TOKEN_STRATEGY.value}\")\n",
    "print(f\"üìù Corrupted prompt: {CORRUPTED_TEXT}\")\n",
    "print(f\"üèóÔ∏è Testing layers: {LAYERS_TO_TEST}\")\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "layer_results = {}\n",
    "\n",
    "for layer_idx in LAYERS_TO_TEST:\n",
    "    print(f\"\\n--- LAYER {layer_idx} ---\")\n",
    "    try:\n",
    "        predicted_token, generated_text = patcher.patch_and_generate(\n",
    "            clean_text=clean_text,\n",
    "            corrupted_text=CORRUPTED_TEXT,\n",
    "            num_placeholder_tokens=5,\n",
    "            capture_layer_idx=layer_idx,  # Updated parameter name\n",
    "            patch_layer_idx=layer_idx,    # Patch to same layer\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            token_selection_strategy=TOKEN_STRATEGY,\n",
    "            num_strategy_tokens=3\n",
    "        )\n",
    "        layer_results[layer_idx] = generated_text\n",
    "        print(f\"Generated: {generated_text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error at layer {layer_idx}: {e}\")\n",
    "        layer_results[layer_idx] = f\"Error: {e}\"\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 3: Multiple Patterns Comparison\n",
    "\n",
    "# ===== MODULAR CONFIGURATION SECTION =====\n",
    "NUM_EXAMPLES_PER_TYPE = 8  # Number of examples to use per pattern type (1-40)\n",
    "NUM_PATTERNS_TO_TEST = 5  # How many different patterns to test\n",
    "TOKEN_STRATEGY = TokenSelectionStrategy.KEYWORDS  # Change to: MID_TOKENS, LAST_COUPLE, LAST_TOKEN, ALL_TOKENS\n",
    "CORRUPTED_TEXT = \"I feel trapped and don't see a way forward because\"\n",
    "MAX_NEW_TOKENS = 65\n",
    "CAPTURE_LAYER = -1\n",
    "PATCH_LAYER = -1\n",
    "NUM_PLACEHOLDERS = 5\n",
    "# ============================================\n",
    "\n",
    "# Apply runtime filtering to get limited dataset\n",
    "filtered_patterns, filtered_pattern_types = filter_patterns_by_count(pattern_types, NUM_EXAMPLES_PER_TYPE)\n",
    "\n",
    "# Get test patterns from filtered dataset\n",
    "test_patterns = filtered_patterns[:NUM_PATTERNS_TO_TEST]\n",
    "\n",
    "print(f\"üìã EXPERIMENT 3 SETUP - MULTIPLE PATTERNS COMPARISON:\")\n",
    "print(f\"üìä Using {NUM_EXAMPLES_PER_TYPE} examples per pattern type (runtime filtered)\")\n",
    "print(f\"üî¢ Total filtered patterns available: {len(filtered_patterns)} (was {len(patterns)})\")\n",
    "print(f\"üß™ Testing {NUM_PATTERNS_TO_TEST} different patterns\")\n",
    "print(f\"üìù Corrupted prompt: '{CORRUPTED_TEXT}'\")\n",
    "print(f\"üîß Token strategy: {TOKEN_STRATEGY.value}\")\n",
    "print(f\"üèóÔ∏è Layers: capture={CAPTURE_LAYER}, patch={PATCH_LAYER}\")\n",
    "print(f\"üìç Placeholders: {NUM_PLACEHOLDERS}, Max tokens: {MAX_NEW_TOKENS}\")\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "pattern_results = []\n",
    "\n",
    "for i, pattern in enumerate(test_patterns):\n",
    "    print(f\"\\n--- PATTERN {i+1}/{NUM_PATTERNS_TO_TEST}: {pattern['cognitive_pattern_name']} ---\")\n",
    "    \n",
    "    clean_text = pattern['positive_thought_pattern']\n",
    "    \n",
    "    print(f\"Clean text (first 100 chars): {clean_text[:100]}...\")\n",
    "    \n",
    "    try:\n",
    "        predicted_token, generated_text = patcher.patch_and_generate(\n",
    "            clean_text=clean_text,\n",
    "            corrupted_text=CORRUPTED_TEXT,\n",
    "            num_placeholder_tokens=NUM_PLACEHOLDERS,\n",
    "            capture_layer_idx=CAPTURE_LAYER,  # Updated parameter name\n",
    "            patch_layer_idx=PATCH_LAYER,      # Updated parameter name\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            token_selection_strategy=TOKEN_STRATEGY,\n",
    "            num_strategy_tokens=3\n",
    "        )\n",
    "        \n",
    "        pattern_results.append({\n",
    "            'pattern_name': pattern['cognitive_pattern_name'],\n",
    "            'generated_text': generated_text\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n‚ú® GENERATED TEXT:\")\n",
    "        print(generated_text)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with pattern {i+1}: {e}\")\n",
    "        pattern_results.append({\n",
    "            'pattern_name': pattern['cognitive_pattern_name'],\n",
    "            'generated_text': f\"Error: {e}\"\n",
    "        })\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "\n",
    "print(f\"\\nüéä EXPERIMENT 3 COMPLETED!\")\n",
    "print(f\"‚úÖ Successfully tested {len([r for r in pattern_results if not r['generated_text'].startswith('Error')])} out of {NUM_PATTERNS_TO_TEST} patterns\")\n",
    "print(f\"üìä Using {NUM_EXAMPLES_PER_TYPE} examples per pattern type (runtime filtered)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 3: Multiple Patterns Comparison\n",
    "\n",
    "# ===== MODULAR CONFIGURATION SECTION =====\n",
    "NUM_EXAMPLES = 8  # Number of examples to test from dataset (1-40)\n",
    "TOKEN_STRATEGY = TokenSelectionStrategy.KEYWORDS  # Change to: MID_TOKENS, LAST_COUPLE, LAST_TOKEN, ALL_TOKENS\n",
    "CORRUPTED_TEXT = \"I feel trapped and don't see a way forward because\"\n",
    "MAX_NEW_TOKENS = 65\n",
    "CAPTURE_LAYER = -1\n",
    "PATCH_LAYER = -1\n",
    "NUM_PLACEHOLDERS = 5\n",
    "# ============================================\n",
    "\n",
    "# Get limited patterns for testing\n",
    "test_patterns = patterns[:NUM_EXAMPLES]  # Use first NUM_EXAMPLES patterns\n",
    "\n",
    "print(f\"üìã EXPERIMENT 3 SETUP - MULTIPLE PATTERNS COMPARISON:\")\n",
    "print(f\"üìä Testing with {NUM_EXAMPLES} patterns from dataset\")\n",
    "print(f\"üìù Corrupted prompt: '{CORRUPTED_TEXT}'\")\n",
    "print(f\"üîß Token strategy: {TOKEN_STRATEGY.value}\")\n",
    "print(f\"üèóÔ∏è Layers: capture={CAPTURE_LAYER}, patch={PATCH_LAYER}\")\n",
    "print(f\"üìç Placeholders: {NUM_PLACEHOLDERS}, Max tokens: {MAX_NEW_TOKENS}\")\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "pattern_results = []\n",
    "\n",
    "for i, pattern in enumerate(test_patterns):\n",
    "    print(f\"\\n--- PATTERN {i+1}/{NUM_EXAMPLES}: {pattern['cognitive_pattern_name']} ---\")\n",
    "    \n",
    "    clean_text = pattern['positive_thought_pattern']\n",
    "    \n",
    "    print(f\"Clean text (first 100 chars): {clean_text[:100]}...\")\n",
    "    \n",
    "    try:\n",
    "        predicted_token, generated_text = patcher.patch_and_generate(\n",
    "            clean_text=clean_text,\n",
    "            corrupted_text=CORRUPTED_TEXT,\n",
    "            num_placeholder_tokens=NUM_PLACEHOLDERS,\n",
    "            capture_layer_idx=CAPTURE_LAYER,  # Updated parameter name\n",
    "            patch_layer_idx=PATCH_LAYER,      # Updated parameter name\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            token_selection_strategy=TOKEN_STRATEGY,\n",
    "            num_strategy_tokens=3\n",
    "        )\n",
    "        \n",
    "        pattern_results.append({\n",
    "            'pattern_name': pattern['cognitive_pattern_name'],\n",
    "            'generated_text': generated_text\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n‚ú® GENERATED TEXT:\")\n",
    "        print(generated_text)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with pattern {i+1}: {e}\")\n",
    "        pattern_results.append({\n",
    "            'pattern_name': pattern['cognitive_pattern_name'],\n",
    "            'generated_text': f\"Error: {e}\"\n",
    "        })\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "\n",
    "print(f\"\\nüéä EXPERIMENT 3 COMPLETED!\")\n",
    "print(f\"‚úÖ Successfully tested {len([r for r in pattern_results if not r['generated_text'].startswith('Error')])} out of {NUM_EXAMPLES} patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Different Number of Patch Positions\n",
    "\n",
    "Let's experiment with varying the number of placeholder tokens we patch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 4: Different Number of Patch Positions\n",
    "\n",
    "# ===== MODULAR CONFIGURATION SECTION =====\n",
    "NUM_EXAMPLES = 12  # Number of examples to choose from dataset (1-40)\n",
    "PLACEHOLDER_COUNTS = [1, 3, 5, 7]  # Different numbers of placeholder tokens to test\n",
    "TOKEN_STRATEGY = TokenSelectionStrategy.KEYWORDS  # Change to: MID_TOKENS, LAST_COUPLE, LAST_TOKEN, ALL_TOKENS\n",
    "PATTERN_INDEX = 10  # Which pattern to use from the limited set\n",
    "CORRUPTED_TEXT = \"My mind keeps racing with negative thoughts and I feel\"\n",
    "MAX_NEW_TOKENS = 55\n",
    "CAPTURE_LAYER = -1\n",
    "PATCH_LAYER = -1\n",
    "# ============================================\n",
    "\n",
    "# Get limited patterns and select one for testing\n",
    "limited_patterns = patterns[:NUM_EXAMPLES]\n",
    "sample_pattern = limited_patterns[PATTERN_INDEX] if PATTERN_INDEX < len(limited_patterns) else limited_patterns[0]\n",
    "\n",
    "clean_text = sample_pattern['positive_thought_pattern']\n",
    "\n",
    "print(f\"üìã EXPERIMENT 4 SETUP - PLACEHOLDER COUNT TESTING:\")\n",
    "print(f\"üìä Selected from {NUM_EXAMPLES} examples in dataset\")\n",
    "print(f\"üß† Pattern: {sample_pattern['cognitive_pattern_name']}\")\n",
    "print(f\"üîß Token strategy: {TOKEN_STRATEGY.value}\")\n",
    "print(f\"üìù Corrupted prompt: {CORRUPTED_TEXT}\")\n",
    "print(f\"üìç Testing placeholder counts: {PLACEHOLDER_COUNTS}\")\n",
    "print(f\"üèóÔ∏è Layers: capture={CAPTURE_LAYER}, patch={PATCH_LAYER}\")\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "placeholder_results = {}\n",
    "\n",
    "for num_placeholders in PLACEHOLDER_COUNTS:\n",
    "    print(f\"\\n--- {num_placeholders} PLACEHOLDER TOKENS ---\")\n",
    "    try:\n",
    "        predicted_token, generated_text = patcher.patch_and_generate(\n",
    "            clean_text=clean_text,\n",
    "            corrupted_text=CORRUPTED_TEXT,\n",
    "            num_placeholder_tokens=num_placeholders,\n",
    "            capture_layer_idx=CAPTURE_LAYER,  # Updated parameter name\n",
    "            patch_layer_idx=PATCH_LAYER,      # Updated parameter name\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            token_selection_strategy=TOKEN_STRATEGY,\n",
    "            num_strategy_tokens=3\n",
    "        )\n",
    "        placeholder_results[num_placeholders] = generated_text\n",
    "        print(f\"‚ú® Generated: {generated_text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with {num_placeholders} placeholders: {e}\")\n",
    "        placeholder_results[num_placeholders] = f\"Error: {e}\"\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nüéä EXPERIMENT 4 COMPLETED!\")\n",
    "print(f\"‚úÖ Tested {len(PLACEHOLDER_COUNTS)} different placeholder configurations\")\n",
    "print(f\"üìä Using pattern from limited dataset of {NUM_EXAMPLES} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 6: Custom Pattern Testing - Interactive Sandbox\n",
    "\n",
    "# ===== MODULAR CONFIGURATION SECTION =====\n",
    "# You can change these to experiment with different combinations\n",
    "NUM_EXAMPLES = 20  # Number of examples to choose from in dataset (1-40)\n",
    "\n",
    "CUSTOM_CLEAN_TEXT = \"I'm taking a moment to acknowledge my feelings and remind myself that challenges are temporary. I can take small, manageable steps forward and focus on what I can control right now.\"\n",
    "CUSTOM_CORRUPTED_TEXT = \"I don't know what to do anymore and feel completely lost\"\n",
    "CUSTOM_TOKEN_STRATEGY = TokenSelectionStrategy.KEYWORDS  # Change to: MID_TOKENS, LAST_COUPLE, LAST_TOKEN, ALL_TOKENS\n",
    "CUSTOM_NUM_PLACEHOLDERS = 5\n",
    "CUSTOM_CAPTURE_LAYER = -1  # Layer to capture activations from\n",
    "CUSTOM_PATCH_LAYER = 3     # Layer to patch activations into\n",
    "CUSTOM_MAX_TOKENS = 70\n",
    "\n",
    "# Strategy comparison settings\n",
    "COMPARISON_NUM_PLACEHOLDERS = 3\n",
    "COMPARISON_MAX_TOKENS = 50\n",
    "COMPARISON_STRATEGIES = [TokenSelectionStrategy.KEYWORDS, TokenSelectionStrategy.MID_TOKENS, \n",
    "                        TokenSelectionStrategy.LAST_COUPLE, TokenSelectionStrategy.LAST_TOKEN, \n",
    "                        TokenSelectionStrategy.ALL_TOKENS]\n",
    "# ============================================\n",
    "\n",
    "print(\"üìã EXPERIMENT 6 SETUP - CUSTOM PATTERN TESTING:\")\n",
    "print(\"=\"*100)\n",
    "print(f\"üìä Working with {NUM_EXAMPLES} examples from dataset\")\n",
    "print(f\"üìù Clean text: {CUSTOM_CLEAN_TEXT}\")\n",
    "print(f\"üìù Corrupted text: {CUSTOM_CORRUPTED_TEXT}\")\n",
    "print(f\"üîß Token strategy: {CUSTOM_TOKEN_STRATEGY.value}\")\n",
    "print(f\"üèóÔ∏è Settings: {CUSTOM_NUM_PLACEHOLDERS} placeholders, capture layer {CUSTOM_CAPTURE_LAYER}, patch layer {CUSTOM_PATCH_LAYER}, {CUSTOM_MAX_TOKENS} max tokens\")\n",
    "\n",
    "try:\n",
    "    predicted_token, generated_text = patcher.patch_and_generate(\n",
    "        clean_text=CUSTOM_CLEAN_TEXT,\n",
    "        corrupted_text=CUSTOM_CORRUPTED_TEXT,\n",
    "        num_placeholder_tokens=CUSTOM_NUM_PLACEHOLDERS,\n",
    "        capture_layer_idx=CUSTOM_CAPTURE_LAYER,  # Updated parameter name\n",
    "        patch_layer_idx=CUSTOM_PATCH_LAYER,      # Updated parameter name\n",
    "        max_new_tokens=CUSTOM_MAX_TOKENS,\n",
    "        token_selection_strategy=CUSTOM_TOKEN_STRATEGY,\n",
    "        num_strategy_tokens=3\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚ú® GENERATED TEXT:\")\n",
    "    print(generated_text)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in custom experiment: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üîß TOKEN STRATEGY COMPARISON:\")\n",
    "print(\"=\"*100)\n",
    "print(f\"üìä Comparing strategies with {NUM_EXAMPLES} examples available\")\n",
    "\n",
    "# Compare all token strategies\n",
    "for strategy in COMPARISON_STRATEGIES:\n",
    "    print(f\"\\n--- STRATEGY: {strategy.value.upper()} ---\")\n",
    "    try:\n",
    "        predicted_token, generated_text = patcher.patch_and_generate(\n",
    "            clean_text=CUSTOM_CLEAN_TEXT,\n",
    "            corrupted_text=CUSTOM_CORRUPTED_TEXT,\n",
    "            num_placeholder_tokens=COMPARISON_NUM_PLACEHOLDERS,\n",
    "            capture_layer_idx=-1,\n",
    "            patch_layer_idx=-1,\n",
    "            max_new_tokens=COMPARISON_MAX_TOKENS,\n",
    "            token_selection_strategy=strategy,\n",
    "            num_strategy_tokens=3\n",
    "        )\n",
    "        print(f\"‚ú® Generated: {generated_text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(f\"\\nüéä EXPERIMENT 6 COMPLETED!\")\n",
    "print(f\"‚úÖ Tested custom settings and {len(COMPARISON_STRATEGIES)} token strategies\")\n",
    "print(f\"üìä Dataset scope: {NUM_EXAMPLES} examples per cognitive pattern type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Troubleshooting & Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è TROUBLESHOOTING UTILITIES\n",
    "\n",
    "# 1. Reset hooks if experiments behave unexpectedly\n",
    "def quick_reset():\n",
    "    patcher.reset_hooks()\n",
    "    print(\"üîÑ Hooks reset - Model is clean!\")\n",
    "\n",
    "# 2. Check model state - now uses the instance method\n",
    "def check_model_info():\n",
    "    patcher.check_model_info()\n",
    "\n",
    "# 3. Clear memory if needed - now uses the static method\n",
    "def clear_memory_util():\n",
    "    ActivationPatcher.clear_memory()\n",
    "\n",
    "# Quick access functions\n",
    "print(\"Available utilities:\")\n",
    "print(\"- quick_reset() - Reset model hooks\")\n",
    "print(\"- check_model_info() - Display model details\") \n",
    "print(\"- clear_memory_util() - Clear GPU/system memory\")\n",
    "print(\"- patcher.reset_hooks() - Direct reset call\")\n",
    "\n",
    "# Uncomment any line below to run:\n",
    "# quick_reset()\n",
    "# check_model_info()\n",
    "# clear_memory_util()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
