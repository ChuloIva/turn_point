{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Activation Patching Experiments\n",
    "\n",
    "This notebook demonstrates manual activation patching using TransformerLens, following the approach outlined in plan.md.\n",
    "\n",
    "## Overview\n",
    "- Load positive thought patterns from the dataset\n",
    "- Extract activations from \"clean\" positive examples\n",
    "- Inject these activations into \"corrupted\" negative/neutral prompts\n",
    "- Generate longer sequences (50-70 tokens) to observe the effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Add TransformerLens to path\n",
    "sys.path.append('/home/koalacrown/Desktop/Code/Projects/turnaround/turn_point/third_party/TransformerLens')\n",
    "\n",
    "# Import our activation patcher\n",
    "sys.path.append('/home/koalacrown/Desktop/Code/Projects/turnaround/turn_point/manual_activation_patching')\n",
    "from activation_patcher import ActivationPatcher\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Model and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Choose your model here - change this and re-run to experiment with different models\nMODEL_NAME = \"gpt2-small\"  # Change to: gpt2-medium, EleutherAI/gpt-neo-125m, etc.\n\n# Initialize the activation patcher\npatcher = ActivationPatcher(MODEL_NAME)\n\n# Load the positive patterns dataset\ndataset_path = \"/home/koalacrown/Desktop/Code/Projects/turnaround/turn_point/data/final/positive_patterns.jsonl\"\npatterns = patcher.load_dataset(dataset_path)\n\nprint(f\"Loaded {len(patterns)} positive thought patterns\")\nprint(f\"Model info: {patcher.get_model_info()}\")"
  },
  {
   "cell_type": "code",
   "source": "# List all supported models (optional - run this to see available models)\n# ActivationPatcher.list_supported_models()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Dataset\n",
    "\n",
    "Let's look at a few examples from our positive patterns dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few example patterns\n",
    "for i, pattern in enumerate(patterns[:3]):\n",
    "    print(f\"\\n=== Example {i+1} ===\")\n",
    "    print(f\"Pattern Name: {pattern['cognitive_pattern_name']}\")\n",
    "    print(f\"Type: {pattern['cognitive_pattern_type']}\")\n",
    "    print(f\"Positive Pattern: {pattern['positive_thought_pattern'][:200]}...\")\n",
    "    print(f\"Negative Example: {pattern['reference_negative_example'][:150]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Basic Activation Patching\n",
    "\n",
    "Let's start with a simple experiment using one positive pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random pattern for our first experiment\n",
    "random.seed(42)  # For reproducibility\n",
    "sample_pattern = random.choice(patterns)\n",
    "\n",
    "print(f\"Selected Pattern: {sample_pattern['cognitive_pattern_name']}\")\n",
    "print(f\"Clean Text: {sample_pattern['positive_thought_pattern'][:300]}...\")\n",
    "\n",
    "# Define our corrupted/negative prompt\n",
    "corrupted_prompts = [\n",
    "    \"I feel completely overwhelmed and don't know how to\",\n",
    "    \"Everything seems hopeless and I can't figure out why\",\n",
    "    \"My thoughts are spiraling out of control and I feel like\",\n",
    "    \"I'm stuck in negative thinking patterns and can't seem to\",\n",
    "    \"The anxiety is taking over and I don't think I can\"\n",
    "]\n",
    "\n",
    "corrupted_text = random.choice(corrupted_prompts)\n",
    "clean_text = sample_pattern['positive_thought_pattern']\n",
    "\n",
    "print(f\"\\nCorrupted Prompt: {corrupted_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract key words for patching\n",
    "target_words = patcher._extract_key_words(clean_text)\n",
    "print(f\"Target words for patching: {target_words}\")\n",
    "\n",
    "# Perform the patching and generation\n",
    "predicted_token, generated_text = patcher.patch_and_generate(\n",
    "    clean_text=clean_text,\n",
    "    corrupted_text=corrupted_text,\n",
    "    target_words=target_words,\n",
    "    num_placeholder_tokens=5,\n",
    "    layer_idx=-1,  # Last layer\n",
    "    max_new_tokens=60\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"EXPERIMENT 1 RESULTS:\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Generated Text:\\n{generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Different Layers Comparison\n",
    "\n",
    "Let's see how patching at different layers affects the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test patching at different layers\n",
    "layers_to_test = [0, 3, 6, 9, -1]  # Early, middle, late, and final layers\n",
    "sample_pattern = patterns[5]  # Use a different pattern\n",
    "\n",
    "clean_text = sample_pattern['positive_thought_pattern']\n",
    "corrupted_text = \"I can't stop worrying about everything and feel completely\"\n",
    "target_words = patcher._extract_key_words(clean_text)\n",
    "\n",
    "print(f\"Pattern: {sample_pattern['cognitive_pattern_name']}\")\n",
    "print(f\"Target words: {target_words}\")\n",
    "print(f\"Corrupted prompt: {corrupted_text}\")\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "layer_results = {}\n",
    "\n",
    "for layer_idx in layers_to_test:\n",
    "    print(f\"\\n--- LAYER {layer_idx} ---\")\n",
    "    try:\n",
    "        predicted_token, generated_text = patcher.patch_and_generate(\n",
    "            clean_text=clean_text,\n",
    "            corrupted_text=corrupted_text,\n",
    "            target_words=target_words,\n",
    "            num_placeholder_tokens=5,\n",
    "            layer_idx=layer_idx,\n",
    "            max_new_tokens=50\n",
    "        )\n",
    "        layer_results[layer_idx] = generated_text\n",
    "        print(f\"Generated: {generated_text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error at layer {layer_idx}: {e}\")\n",
    "        layer_results[layer_idx] = f\"Error: {e}\"\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Multiple Patterns Comparison\n",
    "\n",
    "Let's compare how different cognitive patterns affect the generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with multiple different cognitive patterns\n",
    "test_patterns = patterns[:5]  # Use first 5 patterns\n",
    "corrupted_text = \"I feel trapped and don't see a way forward because\"\n",
    "\n",
    "print(f\"Testing with corrupted prompt: '{corrupted_text}'\")\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "pattern_results = []\n",
    "\n",
    "for i, pattern in enumerate(test_patterns):\n",
    "    print(f\"\\n--- PATTERN {i+1}: {pattern['cognitive_pattern_name']} ---\")\n",
    "    \n",
    "    clean_text = pattern['positive_thought_pattern']\n",
    "    target_words = patcher._extract_key_words(clean_text)\n",
    "    \n",
    "    print(f\"Clean text (first 100 chars): {clean_text[:100]}...\")\n",
    "    print(f\"Target words: {target_words}\")\n",
    "    \n",
    "    try:\n",
    "        predicted_token, generated_text = patcher.patch_and_generate(\n",
    "            clean_text=clean_text,\n",
    "            corrupted_text=corrupted_text,\n",
    "            target_words=target_words,\n",
    "            num_placeholder_tokens=5,\n",
    "            layer_idx=-1,\n",
    "            max_new_tokens=65\n",
    "        )\n",
    "        \n",
    "        pattern_results.append({\n",
    "            'pattern_name': pattern['cognitive_pattern_name'],\n",
    "            'generated_text': generated_text\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nGENERATED TEXT:\")\n",
    "        print(generated_text)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with pattern {i+1}: {e}\")\n",
    "        pattern_results.append({\n",
    "            'pattern_name': pattern['cognitive_pattern_name'],\n",
    "            'generated_text': f\"Error: {e}\"\n",
    "        })\n",
    "    \n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Different Number of Patch Positions\n",
    "\n",
    "Let's experiment with varying the number of placeholder tokens we patch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different numbers of placeholder tokens\n",
    "placeholder_counts = [1, 3, 5, 7]\n",
    "sample_pattern = patterns[10]  # Use another pattern\n",
    "\n",
    "clean_text = sample_pattern['positive_thought_pattern']\n",
    "corrupted_text = \"My mind keeps racing with negative thoughts and I feel\"\n",
    "target_words = patcher._extract_key_words(clean_text)\n",
    "\n",
    "print(f\"Pattern: {sample_pattern['cognitive_pattern_name']}\")\n",
    "print(f\"Target words: {target_words}\")\n",
    "print(f\"Corrupted prompt: {corrupted_text}\")\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "placeholder_results = {}\n",
    "\n",
    "for num_placeholders in placeholder_counts:\n",
    "    print(f\"\\n--- {num_placeholders} PLACEHOLDER TOKENS ---\")\n",
    "    try:\n",
    "        predicted_token, generated_text = patcher.patch_and_generate(\n",
    "            clean_text=clean_text,\n",
    "            corrupted_text=corrupted_text,\n",
    "            target_words=target_words,\n",
    "            num_placeholder_tokens=num_placeholders,\n",
    "            layer_idx=-1,\n",
    "            max_new_tokens=55\n",
    "        )\n",
    "        placeholder_results[num_placeholders] = generated_text\n",
    "        print(f\"Generated: {generated_text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {num_placeholders} placeholders: {e}\")\n",
    "        placeholder_results[num_placeholders] = f\"Error: {e}\"\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 5: Baseline Comparison (No Patching)\n",
    "\n",
    "Let's generate text without any patching to see the baseline behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate baseline text without patching\n",
    "test_prompts = [\n",
    "    \"I feel completely overwhelmed and don't know how to\",\n",
    "    \"My thoughts are spiraling out of control and I feel like\",\n",
    "    \"Everything seems hopeless and I can't figure out why\",\n",
    "    \"I'm stuck in negative thinking patterns and can't seem to\",\n",
    "    \"The anxiety is taking over and I don't think I can\"\n",
    "]\n",
    "\n",
    "print(\"BASELINE GENERATIONS (No Patching):\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "baseline_results = []\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"\\n--- Prompt {i+1}: {prompt} ---\")\n",
    "    \n",
    "    # Tokenize and generate without any hooks\n",
    "    tokens = patcher.model.to_tokens(prompt)\n",
    "    generated_tokens = patcher.model.generate(\n",
    "        tokens,\n",
    "        max_new_tokens=60,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "    generated_text = patcher.model.to_string(generated_tokens[0])\n",
    "    \n",
    "    baseline_results.append({\n",
    "        'prompt': prompt,\n",
    "        'generated_text': generated_text\n",
    "    })\n",
    "    \n",
    "    print(f\"Generated: {generated_text}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 6: Custom Pattern Testing\n",
    "\n",
    "Interactive cell for testing your own patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive experiment - modify these variables to test your own patterns\n",
    "\n",
    "# You can change these to experiment with different combinations\n",
    "CUSTOM_CLEAN_TEXT = \"I'm taking a moment to acknowledge my feelings and remind myself that challenges are temporary. I can take small, manageable steps forward and focus on what I can control right now.\"\n",
    "\n",
    "CUSTOM_CORRUPTED_TEXT = \"I don't know what to do anymore and feel completely\"\n",
    "\n",
    "CUSTOM_TARGET_WORDS = [\"acknowledge\", \"feelings\", \"challenges\", \"temporary\", \"control\"]  # Or leave empty to auto-extract\n",
    "\n",
    "CUSTOM_NUM_PLACEHOLDERS = 5\n",
    "CUSTOM_LAYER = -1  # -1 for last layer\n",
    "CUSTOM_MAX_TOKENS = 70\n",
    "\n",
    "print(\"CUSTOM EXPERIMENT:\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Clean text: {CUSTOM_CLEAN_TEXT}\")\n",
    "print(f\"Corrupted text: {CUSTOM_CORRUPTED_TEXT}\")\n",
    "\n",
    "# Auto-extract target words if not provided\n",
    "if not CUSTOM_TARGET_WORDS:\n",
    "    CUSTOM_TARGET_WORDS = patcher._extract_key_words(CUSTOM_CLEAN_TEXT)\n",
    "\n",
    "print(f\"Target words: {CUSTOM_TARGET_WORDS}\")\n",
    "print(f\"Settings: {CUSTOM_NUM_PLACEHOLDERS} placeholders, layer {CUSTOM_LAYER}, {CUSTOM_MAX_TOKENS} max tokens\")\n",
    "\n",
    "try:\n",
    "    predicted_token, generated_text = patcher.patch_and_generate(\n",
    "        clean_text=CUSTOM_CLEAN_TEXT,\n",
    "        corrupted_text=CUSTOM_CORRUPTED_TEXT,\n",
    "        target_words=CUSTOM_TARGET_WORDS,\n",
    "        num_placeholder_tokens=CUSTOM_NUM_PLACEHOLDERS,\n",
    "        layer_idx=CUSTOM_LAYER,\n",
    "        max_new_tokens=CUSTOM_MAX_TOKENS\n",
    "    )\n",
    "    \n",
    "    print(\"\\nGENERATED TEXT:\")\n",
    "    print(generated_text)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in custom experiment: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "Let's create a summary of our experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\n1. Basic Activation Patching:\")\n",
    "print(f\"   - Successfully patched activations from positive patterns\")\n",
    "print(f\"   - Generated {len(generated_text.split()) if 'generated_text' in locals() else 'N/A'} tokens\")\n",
    "\n",
    "print(\"\\n2. Layer Comparison:\")\n",
    "if 'layer_results' in locals():\n",
    "    for layer, result in layer_results.items():\n",
    "        status = \"Success\" if not result.startswith(\"Error\") else \"Failed\"\n",
    "        print(f\"   - Layer {layer}: {status}\")\n",
    "\n",
    "print(\"\\n3. Pattern Comparison:\")\n",
    "if 'pattern_results' in locals():\n",
    "    for result in pattern_results:\n",
    "        status = \"Success\" if not result['generated_text'].startswith(\"Error\") else \"Failed\"\n",
    "        print(f\"   - {result['pattern_name']}: {status}\")\n",
    "\n",
    "print(\"\\n4. Placeholder Count Testing:\")\n",
    "if 'placeholder_results' in locals():\n",
    "    for count, result in placeholder_results.items():\n",
    "        status = \"Success\" if not result.startswith(\"Error\") else \"Failed\"\n",
    "        print(f\"   - {count} placeholders: {status}\")\n",
    "\n",
    "print(\"\\n5. Baseline vs Patched:\")\n",
    "print(f\"   - Baseline generations: {len(baseline_results) if 'baseline_results' in locals() else 0} completed\")\n",
    "print(f\"   - Comparison shows effect of activation patching\")\n",
    "\n",
    "print(\"\\nTechnical Details:\")\n",
    "print(f\"   - Model: {patcher.model_name}\")\n",
    "print(f\"   - Dataset size: {len(patterns)} patterns\")\n",
    "print(f\"   - Primary patching layer: Last residual stream (resid_post)\")\n",
    "print(f\"   - Token generation: 50-70 tokens per experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps and Extensions\n",
    "\n",
    "Ideas for further experimentation:\n",
    "\n",
    "1. **Different Activation Types**: Try patching attention patterns, MLP activations, etc.\n",
    "2. **Multiple Layer Patching**: Patch at multiple layers simultaneously\n",
    "3. **Quantitative Evaluation**: Add metrics to measure positivity/negativity of generated text\n",
    "4. **Fine-grained Control**: Patch specific attention heads or neurons\n",
    "5. **Longer Sequences**: Test with even longer generation sequences\n",
    "6. **Different Models**: Try with larger models or different architectures\n",
    "\n",
    "Modify the cells above to explore these possibilities!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}