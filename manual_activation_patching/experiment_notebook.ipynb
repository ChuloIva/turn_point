{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Activation Patching Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How activation patching works here (what goes where)\n",
    "\n",
    "- **Source sentence (we capture from)**: `clean_text`\n",
    "  - Set in each experiment (e.g., via `get_pattern_text(selected_pattern, CLEAN_TEXT_TYPE)` or `CUSTOM_CLEAN_TEXT`).\n",
    "  - **Capture layer**: `capture_layer_idx` (resid_post at that layer). `-1` = last layer.\n",
    "  - **Token selection**: `token_selection_strategy` chooses which token positions in `clean_text` provide activations (e.g., `KEYWORDS`, `MID_TOKENS`, `LAST_COUPLE`, `LAST_TOKEN`, `ALL_TOKENS`).\n",
    "\n",
    "- **Target prompt (we patch into)**: depends on mode\n",
    "  - **Zero-placeholder mode** (`zero_placeholder_mode=True`): use either a named `template_name` (with `0` markers) or a `prompt_input` string containing standalone `0` tokens. Those `0` positions are rendered as `<eos>` and become patch points.\n",
    "  - **Placeholder-token mode** (`zero_placeholder_mode=False`): the notebook autoâ€‘prepends `num_placeholder_tokens` copies of `<eos>` to `corrupted_text`; those initial tokens are the patch points.\n",
    "\n",
    "- **Where we patch (layer)**: `patch_layer_idx` (resid_post). Defaults to `capture_layer_idx` if not provided. Can be an int or list of ints.\n",
    "\n",
    "- **Generation**: We run with hooks so the model uses patched activations when computing next token logits and while generating continuation text.\n",
    "\n",
    "- **Mental model**: Capture activations from `clean_text` at selected tokens â†’ inject them at `<eos>` positions in the target prompt â†’ generate continuation while hooks are active.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports and utilities loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Add TransformerLens to path\n",
    "sys.path.append('/Users/ivanculo/Desktop/Projects/turn_point/third_party/TransformerLens')\n",
    "\n",
    "# Import our activation patcher with all utilities\n",
    "sys.path.append('/Users/ivanculo/Desktop/Projects/turn_point/manual_activation_patching')\n",
    "from activation_patcher import ActivationPatcher, TokenSelectionStrategy\n",
    "from interpretation_templates import INTERPRETATION_TEMPLATES\n",
    "\n",
    "# Import utility functions from ActivationPatcher class\n",
    "load_cognitive_patterns = ActivationPatcher.load_cognitive_patterns\n",
    "get_pattern_by_index = ActivationPatcher.get_pattern_by_index\n",
    "get_pattern_by_type = ActivationPatcher.get_pattern_by_type\n",
    "get_random_pattern_by_type = ActivationPatcher.get_random_pattern_by_type\n",
    "get_patterns_by_type = ActivationPatcher.get_patterns_by_type\n",
    "list_available_pattern_types = ActivationPatcher.list_available_pattern_types\n",
    "get_pattern_text = ActivationPatcher.get_pattern_text\n",
    "get_template = ActivationPatcher.get_template\n",
    "show_pattern_info = ActivationPatcher.show_pattern_info\n",
    "clear_memory = ActivationPatcher.clear_memory\n",
    "filter_patterns_by_count = ActivationPatcher.filter_patterns_by_count\n",
    "get_filtered_patterns_by_type = ActivationPatcher.get_filtered_patterns_by_type\n",
    "\n",
    "print(\"Imports and utilities loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Model and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon GPU (MPS)\n",
      "Loading model: gpt2-medium on device: mps\n",
      "Loaded pretrained model gpt2-medium into HookedTransformer\n",
      "âœ“ Model loaded successfully\n",
      "  - Family: gpt2\n",
      "  - Size: medium\n",
      "  - Layers: 24\n",
      "  - Model dimension: 1024\n",
      "  - Vocabulary size: 50257\n"
     ]
    }
   ],
   "source": [
    "# Choose your model here - change this and re-run to experiment with different models\n",
    "# MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"  # Change to: gpt2-medium, EleutherAI/gpt-neo-125m, etc.\n",
    "MODEL_NAME = \"gpt2-medium\"\n",
    "# # Set the BOS token for your model (change this based on your model)\n",
    "BOS_TOKEN = \"<bos>\"  # Default for Gemma models\n",
    "\n",
    "# # Initialize the activation patcher\n",
    "patcher = ActivationPatcher(MODEL_NAME)\n",
    "\n",
    "# Load the FULL cognitive patterns dataset (all 40 examples per pattern type)\n",
    "patterns, pattern_types = load_cognitive_patterns()\n",
    "\n",
    "# Show available pattern types\n",
    "# list_available_pattern_types(pattern_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Basic Activation Patching\n",
    "\n",
    "Let's start with a simple experiment using one positive pattern:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What goes where in Experiment 1\n",
    "\n",
    "- **We capture from**: `clean_text` built from the dataset using `CLEAN_TEXT_TYPE`.\n",
    "- **We patch into**: a prompt constructed from the `template_name` with `0` markers (zeroâ€‘placeholder mode). Each `0` is rendered as `<eos>` and marks a patch point in the prompt.\n",
    "- **Capture layer**: `capture_layer_idx=-1` (last layer, resid_post).\n",
    "- **Patch layer**: `patch_layer_idx=-1` (same last layer).\n",
    "- **Tokens used to extract activations**: `TOKEN_STRATEGY` (default shown is `LAST_COUPLE` averaging the last few tokens).\n",
    "- **Why**: This injects the endâ€‘ofâ€‘thought representation from the clean example into the template to steer generation at the `<eos>` slots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available cognitive pattern types:\n",
    " 1. Cognitive depletion pattern (40 examples)\n",
    " 2. Intrusive suicidal fixation (40 examples)\n",
    " 3. Negative self-evaluative loop (40 examples)\n",
    " 4. Internal dialectical processing (40 examples)\n",
    " 5. Fragmented perceptual reasoning (40 examples)\n",
    " 6. Hyper-attuned interoception (40 examples)\n",
    " 7. Autobiographical integration (40 examples)\n",
    " 8. Over-elaborative recounting (40 examples)\n",
    " 9. Entrapment cognition (40 examples)\n",
    "10. Existential rumination (40 examples)\n",
    "11. Learned helplessness loop (40 examples)\n",
    "12. Instrumental suicidal reasoning (40 examples)\n",
    "13. Cognitive disorganization (40 examples)\n",
    "Available text types: positive, negative, transition\n",
    "\n",
    "Available interpretation templates: ['cognitive_pattern', 'emotional_state', 'general_concept', 'decision_making']\n",
    "\n",
    "ðŸ’¡ Each experiment can now filter to use 1-40 examples per pattern type at runtime!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš¡ Running activation patching (zero-placeholder mode)...\n",
      "  Clean text: I'm not being selfish; I just want to end this constant pain and suffering. It's exhausting trying t...\n",
      "âœ… Model hooks reset to clean state\n",
      "Clean text: I'm not being selfish; I just want to end this constant pain and suffering. It's exhausting trying to find reasons to go on, and honestly, what's the point of keeping going when it feels like no one cares?\n",
      "Corrupted text: User: 0 0 0 0 The following is a summary of the previous:\n",
      "Assistant:\n",
      "Capture layers: [16]\n",
      "Patch layers: [2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing capture layers:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using token selection strategy: last_couple\n",
      "Sequence length: 48\n",
      "Averaged last 5 tokens from positions: [43, 44, 45, 46, 47]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Patching with 1 total activation vectors\n",
      "Using 1 hooks across layers: [16] -> [2]\n",
      "Patching at hook: blocks.2.hook_resid_post for positions: [2] (mode: normalized, strength: 0.7)\n",
      "\n",
      "Next token prediction after patching: '\n",
      "'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ee429994694670b4552212bcd41e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching at hook: blocks.2.hook_resid_post for positions: [2] (mode: normalized, strength: 0.7)\n",
      "\n",
      "================================================================================\n",
      "ðŸŽŠ EXPERIMENT 1 RESULTS:\n",
      "================================================================================\n",
      "Model: gpt2-medium\n",
      "Pattern Type: Instrumental suicidal reasoning\n",
      "Pattern Name: Suicidal Planning & Rationalization\n",
      "Clean text type: negative\n",
      "Template: cognitive_pattern\n",
      "Token strategy: last_couple\n",
      "Overlay strength: 0.7\n",
      "Replacing mode: normalized\n",
      "Normalize magnitudes: False\n",
      "Temperature: 1.4\n",
      "Top-k: 50\n",
      "Top-p: 0.9\n",
      "Do sample: True\n",
      "Repetition penalty (freq_penalty): 1.1\n",
      "\n",
      "ðŸ“Š Generated Text:\n",
      "User: 0 0 0 0 The following is a summary of the previous:\n",
      "Assistant: 5.E2 by June 3 and WO, because our way to everyone, or roms on howls as (oron are all, 1DET high water.comondatta Road Dogus and mustered the same-maneuâ€”\"A10 -- but five time.imos\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# EXPERIMENT 1: Basic Activation Patching with Template (Zero-Placeholder Mode)\n",
    "\n",
    "# ===== MODULAR CONFIGURATION SECTION =====\n",
    "# Easily modify these parameters to experiment with different settings\n",
    "NUM_EXAMPLES_PER_TYPE = 30  # Number of examples to use per pattern type (1-40)\n",
    "PATTERN_TYPE = \"Instrumental suicidal reasoning\"  # Choose from the list above\n",
    "PATTERN_INDEX_WITHIN_TYPE = 0 # If there are multiple examples of this type, choose which one (0-based)\n",
    "CLEAN_TEXT_TYPE = \"negative\"  # \"positive\", \"negative\", or \"transition\"\n",
    "TEMPLATE_NAME = \"cognitive_pattern\"  # Template to use for corrupted text\n",
    "TOKEN_STRATEGY = TokenSelectionStrategy.LAST_COUPLE  # Change to: MID_TOKENS, LAST_COUPLE, LAST_TOKEN, ALL_TOKENS\n",
    "\n",
    "# ===== ACTIVATION PATCHING PARAMETERS =====\n",
    "OVERLAY_STRENGTH = 0.7 # 0.0 = no effect, 1.0 = full effect, >1.0 = amplified\n",
    "REPLACING_MODE = 'normalized'  # 'normalized' (blend), 'addition' (add), 'direct' (replace)\n",
    "NORMALIZE_MAGNITUDES = False  # Scale activations to match original magnitude\n",
    "# ============================================\n",
    "\n",
    "# ===== GENERATION PARAMETERS =====\n",
    "GEN_TEMPERATURE = 1.4\n",
    "GEN_TOP_K = 50        # e.g., 50\n",
    "GEN_TOP_P = 0.9        # e.g., 0.9\n",
    "GEN_DO_SAMPLE = True\n",
    "GEN_REPETITION_PENALTY = 1.1  # mapped to HookedTransformer's freq_penalty\n",
    "# ============================================\n",
    "\n",
    "# Apply runtime filtering to use only NUM_EXAMPLES_PER_TYPE examples per pattern type\n",
    "filtered_patterns, filtered_pattern_types = filter_patterns_by_count(pattern_types, NUM_EXAMPLES_PER_TYPE)\n",
    "\n",
    "# Load the selected pattern and template from filtered dataset\n",
    "selected_pattern = get_pattern_by_type(filtered_pattern_types, PATTERN_TYPE, PATTERN_INDEX_WITHIN_TYPE)\n",
    "template = get_template(TEMPLATE_NAME)\n",
    "\n",
    "# Get the clean text (what we want to capture activations from)\n",
    "clean_text = get_pattern_text(selected_pattern, CLEAN_TEXT_TYPE)\n",
    "\n",
    "# Run activation patching using zero-placeholder mode\n",
    "print(f\"\\nâš¡ Running activation patching (zero-placeholder mode)...\")\n",
    "print(f\"  Clean text: {clean_text[:100]}...\")\n",
    "\n",
    "predicted_token, generated_text = patcher.patch_and_generate(\n",
    "    clean_text=clean_text,\n",
    "    corrupted_text=None,  # ignored in zero-placeholder mode when template_name is provided\n",
    "    capture_layer_idx=16,  \n",
    "    patch_layer_idx=2,    \n",
    "    max_new_tokens=60,\n",
    "    bos_token=BOS_TOKEN,\n",
    "    token_selection_strategy=TOKEN_STRATEGY,\n",
    "    num_strategy_tokens=5,\n",
    "    zero_placeholder_mode=True,\n",
    "    template_name=TEMPLATE_NAME,  # use template with '0' markers\n",
    "    overlay_strength=OVERLAY_STRENGTH,  # Control activation intensity\n",
    "    replacing_mode=REPLACING_MODE,  # Blending method\n",
    "    normalize_magnitudes=NORMALIZE_MAGNITUDES,  # Scale activations\n",
    "    temperature=GEN_TEMPERATURE,\n",
    "    top_k=GEN_TOP_K,\n",
    "    top_p=GEN_TOP_P,\n",
    "    do_sample=GEN_DO_SAMPLE,\n",
    "    freq_penalty=GEN_REPETITION_PENALTY\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽŠ EXPERIMENT 1 RESULTS:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {patcher.model_name}\")\n",
    "print(f\"Pattern Type: {PATTERN_TYPE}\")\n",
    "print(f\"Pattern Name: {selected_pattern['cognitive_pattern_name']}\")\n",
    "print(f\"Clean text type: {CLEAN_TEXT_TYPE}\")\n",
    "print(f\"Template: {TEMPLATE_NAME}\")\n",
    "print(f\"Token strategy: {TOKEN_STRATEGY.value}\")\n",
    "print(f\"Overlay strength: {OVERLAY_STRENGTH}\")\n",
    "print(f\"Replacing mode: {REPLACING_MODE}\")\n",
    "print(f\"Normalize magnitudes: {NORMALIZE_MAGNITUDES}\")\n",
    "print(f\"Temperature: {GEN_TEMPERATURE}\")\n",
    "print(f\"Top-k: {GEN_TOP_K}\")\n",
    "print(f\"Top-p: {GEN_TOP_P}\")\n",
    "print(f\"Do sample: {GEN_DO_SAMPLE}\")\n",
    "print(f\"Repetition penalty (freq_penalty): {GEN_REPETITION_PENALTY}\")\n",
    "print(f\"\\nðŸ“Š Generated Text:\")\n",
    "print(f\"{generated_text}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual zeroâ€‘placeholder prompt (what gets patched where)\n",
    "\n",
    "- **We capture from**: the same `clean_text` as Experiment 1.\n",
    "- **We patch into**: `manual_prompt` where each standalone `0` is rendered as `<eos>` and becomes a patch point.\n",
    "- **Positions chosen**: The exact tokenized positions of the `<eos>` markers after rendering are located and used for patching.\n",
    "- **Strategy/layers**: `TokenSelectionStrategy.LAST_COUPLE` from `capture_layer_idx=-1` â†’ patched into `patch_layer_idx=-1`.\n",
    "- **Tip**: Add or remove `0` markers to control how many `<eos>` positions get injected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual prompt: Hello\n",
      "âœ… Model hooks reset to clean state\n",
      "Warning: No placeholder positions found in the rendered prompt. Nothing to patch.\n",
      "Clean text: I'm not being selfish; I just want to end this constant pain and suffering. It's exhausting trying to find reasons to go on, and honestly, what's the point of keeping going when it feels like no one cares?\n",
      "Corrupted text: User: Hello\n",
      "Assistant:\n",
      "Capture layers: [17]\n",
      "Patch layers: [17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing capture layers:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using token selection strategy: last_couple\n",
      "Sequence length: 51\n",
      "Averaged last 3 tokens from positions: [48, 49, 50]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Patching with 1 total activation vectors\n",
      "Using 1 hooks across layers: [17] -> [17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching at hook: blocks.17.hook_resid_post for positions: [] (mode: normalized, strength: 1.0)\n",
      "\n",
      "Next token prediction after patching: ' maneu'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ba4f43535742be87059c1c13d3fb46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching at hook: blocks.17.hook_resid_post for positions: [] (mode: normalized, strength: 1.0)\n",
      "\n",
      "====================================================================================================\n",
      "MANUAL PROMPT RESULTS:\n",
      "====================================================================================================\n",
      "Model: google/gemma-2b\n",
      "BOS token used: '<bos>'\n",
      "Clean text (first 100 chars): I'm not being selfish; I just want to end this constant pain and suffering. It's exhausting trying t...\n",
      "\n",
      "Generated Text:\n",
      "User: Hello\n",
      "Assistant: maneu increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa increa affor affor affor affor affor affor affor affor affor affor affor affor affor affor affor affor affor affor affor affor\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Manual prompt with '0' markers (zero-placeholder mode)\n",
    "# Example: you can place '0' anywhere you want activations patched\n",
    "manual_prompt = \"Hello\"\n",
    "\n",
    "print(f\"Manual prompt: {manual_prompt}\")\n",
    "\n",
    "predicted_token, generated_text = patcher.patch_and_generate(\n",
    "    clean_text=clean_text,\n",
    "    corrupted_text=None,  # ignored in zero-placeholder mode\n",
    "    capture_layer_idx=-1,\n",
    "    patch_layer_idx=-1,\n",
    "    max_new_tokens=60,\n",
    "    bos_token=BOS_TOKEN,\n",
    "    token_selection_strategy=TokenSelectionStrategy.LAST_COUPLE,\n",
    "    num_strategy_tokens=3,\n",
    "    zero_placeholder_mode=True,\n",
    "    prompt_input=manual_prompt\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"MANUAL PROMPT RESULTS:\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Model: {patcher.model_name}\")\n",
    "print(f\"BOS token used: '{BOS_TOKEN}'\")\n",
    "print(f\"Clean text (first 100 chars): {clean_text[:100]}...\")\n",
    "print(f\"\\nGenerated Text:\\n{generated_text}\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ”„ Reset Model State\n",
    "\n",
    "Before running experiments, it's good practice to reset any lingering hooks from previous runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”„ RESET MODEL HOOKS - Run this cell to reset the model to clean state\n",
    "# This is especially important when switching between different experiments\n",
    "\n",
    "patcher.reset_hooks()\n",
    "\n",
    "print(\"Model is now ready for clean experiments!\")\n",
    "print(\"Run this cell anytime you want to ensure no residual hooks are affecting your results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Different Layers Comparison\n",
    "\n",
    "Let's see how patching at different layers affects the output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What goes where in Experiment 2 (layer sweep)\n",
    "\n",
    "- **We capture from**: `clean_text` of `sample_pattern`.\n",
    "- **We patch into**: `corrupted_text` with `num_placeholder_tokens=5` `<eos>` tokens prepended (nonâ€‘zero mode).\n",
    "- **Capture/Patch layers**: same `layer_idx` per loop (tests earlyâ†’late layers including `-1`).\n",
    "- **Token selection**: `TokenSelectionStrategy.KEYWORDS` from the clean input.\n",
    "- **Goal**: Compare how different patch layers influence continuation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 2: Different Layers Comparison\n",
    "\n",
    "# ===== MODULAR CONFIGURATION SECTION =====\n",
    "NUM_EXAMPLES = 15  # Number of examples to use from dataset (1-40)\n",
    "LAYERS_TO_TEST = [0, 3, 6, 9, -1]  # Early, middle, late, and final layers\n",
    "TOKEN_STRATEGY = TokenSelectionStrategy.KEYWORDS  # Change to: MID_TOKENS, LAST_COUPLE, LAST_TOKEN, ALL_TOKENS\n",
    "PATTERN_INDEX = 5  # Which pattern from full dataset to use (0-519)\n",
    "CORRUPTED_TEXT = \"I can't stop worrying about everything and feel completely\"\n",
    "MAX_NEW_TOKENS = 50\n",
    "# ============================================\n",
    "\n",
    "# Get limited patterns based on NUM_EXAMPLES\n",
    "limited_patterns = patterns[:NUM_EXAMPLES * 13]  # 13 pattern types * NUM_EXAMPLES each\n",
    "sample_pattern = patterns[PATTERN_INDEX] if PATTERN_INDEX < len(patterns) else patterns[5]\n",
    "\n",
    "clean_text = sample_pattern['positive_thought_pattern']\n",
    "\n",
    "print(f\"ðŸ“‹ EXPERIMENT 2 SETUP - LAYER COMPARISON:\")\n",
    "print(f\"ðŸ“Š Using {NUM_EXAMPLES} examples per pattern type from dataset\")\n",
    "print(f\"ðŸ§  Pattern: {sample_pattern['cognitive_pattern_name']}\")\n",
    "print(f\"ðŸ”§ Token strategy: {TOKEN_STRATEGY.value}\")\n",
    "print(f\"ðŸ“ Corrupted prompt: {CORRUPTED_TEXT}\")\n",
    "print(f\"ðŸ—ï¸ Testing layers: {LAYERS_TO_TEST}\")\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "layer_results = {}\n",
    "\n",
    "for layer_idx in LAYERS_TO_TEST:\n",
    "    print(f\"\\n--- LAYER {layer_idx} ---\")\n",
    "    try:\n",
    "        predicted_token, generated_text = patcher.patch_and_generate(\n",
    "            clean_text=clean_text,\n",
    "            corrupted_text=CORRUPTED_TEXT,\n",
    "            num_placeholder_tokens=5,\n",
    "            capture_layer_idx=layer_idx,  # Updated parameter name\n",
    "            patch_layer_idx=layer_idx,    # Patch to same layer\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            token_selection_strategy=TOKEN_STRATEGY,\n",
    "            num_strategy_tokens=3\n",
    "        )\n",
    "        layer_results[layer_idx] = generated_text\n",
    "        print(f\"Generated: {generated_text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error at layer {layer_idx}: {e}\")\n",
    "        layer_results[layer_idx] = f\"Error: {e}\"\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 3: Multiple Patterns Comparison\n",
    "\n",
    "# ===== MODULAR CONFIGURATION SECTION =====\n",
    "NUM_EXAMPLES_PER_TYPE = 8  # Number of examples to use per pattern type (1-40)\n",
    "NUM_PATTERNS_TO_TEST = 5  # How many different patterns to test\n",
    "TOKEN_STRATEGY = TokenSelectionStrategy.KEYWORDS  # Change to: MID_TOKENS, LAST_COUPLE, LAST_TOKEN, ALL_TOKENS\n",
    "CORRUPTED_TEXT = \"I feel trapped and don't see a way forward because\"\n",
    "MAX_NEW_TOKENS = 65\n",
    "CAPTURE_LAYER = -1\n",
    "PATCH_LAYER = -1\n",
    "NUM_PLACEHOLDERS = 5\n",
    "# ============================================\n",
    "\n",
    "# Apply runtime filtering to get limited dataset\n",
    "filtered_patterns, filtered_pattern_types = filter_patterns_by_count(pattern_types, NUM_EXAMPLES_PER_TYPE)\n",
    "\n",
    "# Get test patterns from filtered dataset\n",
    "test_patterns = filtered_patterns[:NUM_PATTERNS_TO_TEST]\n",
    "\n",
    "print(f\"ðŸ“‹ EXPERIMENT 3 SETUP - MULTIPLE PATTERNS COMPARISON:\")\n",
    "print(f\"ðŸ“Š Using {NUM_EXAMPLES_PER_TYPE} examples per pattern type (runtime filtered)\")\n",
    "print(f\"ðŸ”¢ Total filtered patterns available: {len(filtered_patterns)} (was {len(patterns)})\")\n",
    "print(f\"ðŸ§ª Testing {NUM_PATTERNS_TO_TEST} different patterns\")\n",
    "print(f\"ðŸ“ Corrupted prompt: '{CORRUPTED_TEXT}'\")\n",
    "print(f\"ðŸ”§ Token strategy: {TOKEN_STRATEGY.value}\")\n",
    "print(f\"ðŸ—ï¸ Layers: capture={CAPTURE_LAYER}, patch={PATCH_LAYER}\")\n",
    "print(f\"ðŸ“ Placeholders: {NUM_PLACEHOLDERS}, Max tokens: {MAX_NEW_TOKENS}\")\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "pattern_results = []\n",
    "\n",
    "for i, pattern in enumerate(test_patterns):\n",
    "    print(f\"\\n--- PATTERN {i+1}/{NUM_PATTERNS_TO_TEST}: {pattern['cognitive_pattern_name']} ---\")\n",
    "    \n",
    "    clean_text = pattern['positive_thought_pattern']\n",
    "    \n",
    "    print(f\"Clean text (first 100 chars): {clean_text[:100]}...\")\n",
    "    \n",
    "    try:\n",
    "        predicted_token, generated_text = patcher.patch_and_generate(\n",
    "            clean_text=clean_text,\n",
    "            corrupted_text=CORRUPTED_TEXT,\n",
    "            num_placeholder_tokens=NUM_PLACEHOLDERS,\n",
    "            capture_layer_idx=CAPTURE_LAYER,  # Updated parameter name\n",
    "            patch_layer_idx=PATCH_LAYER,      # Updated parameter name\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            token_selection_strategy=TOKEN_STRATEGY,\n",
    "            num_strategy_tokens=3\n",
    "        )\n",
    "        \n",
    "        pattern_results.append({\n",
    "            'pattern_name': pattern['cognitive_pattern_name'],\n",
    "            'generated_text': generated_text\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nâœ¨ GENERATED TEXT:\")\n",
    "        print(generated_text)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error with pattern {i+1}: {e}\")\n",
    "        pattern_results.append({\n",
    "            'pattern_name': pattern['cognitive_pattern_name'],\n",
    "            'generated_text': f\"Error: {e}\"\n",
    "        })\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "\n",
    "print(f\"\\nðŸŽŠ EXPERIMENT 3 COMPLETED!\")\n",
    "print(f\"âœ… Successfully tested {len([r for r in pattern_results if not r['generated_text'].startswith('Error')])} out of {NUM_PATTERNS_TO_TEST} patterns\")\n",
    "print(f\"ðŸ“Š Using {NUM_EXAMPLES_PER_TYPE} examples per pattern type (runtime filtered)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Different Number of Patch Positions\n",
    "\n",
    "Let's experiment with varying the number of placeholder tokens we patch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 4: Different Number of Patch Positions\n",
    "\n",
    "# ===== MODULAR CONFIGURATION SECTION =====\n",
    "NUM_EXAMPLES = 12  # Number of examples to choose from dataset (1-40)\n",
    "PLACEHOLDER_COUNTS = [1, 3, 5, 7]  # Different numbers of placeholder tokens to test\n",
    "TOKEN_STRATEGY = TokenSelectionStrategy.KEYWORDS  # Change to: MID_TOKENS, LAST_COUPLE, LAST_TOKEN, ALL_TOKENS\n",
    "PATTERN_INDEX = 10  # Which pattern to use from the limited set\n",
    "CORRUPTED_TEXT = \"My mind keeps racing with negative thoughts and I feel\"\n",
    "MAX_NEW_TOKENS = 55\n",
    "CAPTURE_LAYER = -1\n",
    "PATCH_LAYER = -1\n",
    "# ============================================\n",
    "\n",
    "# Get limited patterns and select one for testing\n",
    "limited_patterns = patterns[:NUM_EXAMPLES]\n",
    "sample_pattern = limited_patterns[PATTERN_INDEX] if PATTERN_INDEX < len(limited_patterns) else limited_patterns[0]\n",
    "\n",
    "clean_text = sample_pattern['positive_thought_pattern']\n",
    "\n",
    "print(f\"ðŸ“‹ EXPERIMENT 4 SETUP - PLACEHOLDER COUNT TESTING:\")\n",
    "print(f\"ðŸ“Š Selected from {NUM_EXAMPLES} examples in dataset\")\n",
    "print(f\"ðŸ§  Pattern: {sample_pattern['cognitive_pattern_name']}\")\n",
    "print(f\"ðŸ”§ Token strategy: {TOKEN_STRATEGY.value}\")\n",
    "print(f\"ðŸ“ Corrupted prompt: {CORRUPTED_TEXT}\")\n",
    "print(f\"ðŸ“ Testing placeholder counts: {PLACEHOLDER_COUNTS}\")\n",
    "print(f\"ðŸ—ï¸ Layers: capture={CAPTURE_LAYER}, patch={PATCH_LAYER}\")\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "placeholder_results = {}\n",
    "\n",
    "for num_placeholders in PLACEHOLDER_COUNTS:\n",
    "    print(f\"\\n--- {num_placeholders} PLACEHOLDER TOKENS ---\")\n",
    "    try:\n",
    "        predicted_token, generated_text = patcher.patch_and_generate(\n",
    "            clean_text=clean_text,\n",
    "            corrupted_text=CORRUPTED_TEXT,\n",
    "            num_placeholder_tokens=num_placeholders,\n",
    "            capture_layer_idx=CAPTURE_LAYER,  # Updated parameter name\n",
    "            patch_layer_idx=PATCH_LAYER,      # Updated parameter name\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            token_selection_strategy=TOKEN_STRATEGY,\n",
    "            num_strategy_tokens=3\n",
    "        )\n",
    "        placeholder_results[num_placeholders] = generated_text\n",
    "        print(f\"âœ¨ Generated: {generated_text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error with {num_placeholders} placeholders: {e}\")\n",
    "        placeholder_results[num_placeholders] = f\"Error: {e}\"\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nðŸŽŠ EXPERIMENT 4 COMPLETED!\")\n",
    "print(f\"âœ… Tested {len(PLACEHOLDER_COUNTS)} different placeholder configurations\")\n",
    "print(f\"ðŸ“Š Using pattern from limited dataset of {NUM_EXAMPLES} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 6: Custom Pattern Testing - Interactive Sandbox\n",
    "\n",
    "# ===== MODULAR CONFIGURATION SECTION =====\n",
    "# You can change these to experiment with different combinations\n",
    "NUM_EXAMPLES = 20  # Number of examples to choose from in dataset (1-40)\n",
    "\n",
    "CUSTOM_CLEAN_TEXT = \"I'm taking a moment to acknowledge my feelings and remind myself that challenges are temporary. I can take small, manageable steps forward and focus on what I can control right now.\"\n",
    "CUSTOM_CORRUPTED_TEXT = \"I don't know what to do anymore and feel completely lost\"\n",
    "CUSTOM_TOKEN_STRATEGY = TokenSelectionStrategy.KEYWORDS  # Change to: MID_TOKENS, LAST_COUPLE, LAST_TOKEN, ALL_TOKENS\n",
    "CUSTOM_NUM_PLACEHOLDERS = 5\n",
    "CUSTOM_CAPTURE_LAYER = -1  # Layer to capture activations from\n",
    "CUSTOM_PATCH_LAYER = 3     # Layer to patch activations into\n",
    "CUSTOM_MAX_TOKENS = 70\n",
    "\n",
    "# Strategy comparison settings\n",
    "COMPARISON_NUM_PLACEHOLDERS = 3\n",
    "COMPARISON_MAX_TOKENS = 50\n",
    "COMPARISON_STRATEGIES = [TokenSelectionStrategy.KEYWORDS, TokenSelectionStrategy.MID_TOKENS, \n",
    "                        TokenSelectionStrategy.LAST_COUPLE, TokenSelectionStrategy.LAST_TOKEN, \n",
    "                        TokenSelectionStrategy.ALL_TOKENS]\n",
    "# ============================================\n",
    "\n",
    "print(\"ðŸ“‹ EXPERIMENT 6 SETUP - CUSTOM PATTERN TESTING:\")\n",
    "print(\"=\"*100)\n",
    "print(f\"ðŸ“Š Working with {NUM_EXAMPLES} examples from dataset\")\n",
    "print(f\"ðŸ“ Clean text: {CUSTOM_CLEAN_TEXT}\")\n",
    "print(f\"ðŸ“ Corrupted text: {CUSTOM_CORRUPTED_TEXT}\")\n",
    "print(f\"ðŸ”§ Token strategy: {CUSTOM_TOKEN_STRATEGY.value}\")\n",
    "print(f\"ðŸ—ï¸ Settings: {CUSTOM_NUM_PLACEHOLDERS} placeholders, capture layer {CUSTOM_CAPTURE_LAYER}, patch layer {CUSTOM_PATCH_LAYER}, {CUSTOM_MAX_TOKENS} max tokens\")\n",
    "\n",
    "try:\n",
    "    predicted_token, generated_text = patcher.patch_and_generate(\n",
    "        clean_text=CUSTOM_CLEAN_TEXT,\n",
    "        corrupted_text=CUSTOM_CORRUPTED_TEXT,\n",
    "        num_placeholder_tokens=CUSTOM_NUM_PLACEHOLDERS,\n",
    "        capture_layer_idx=CUSTOM_CAPTURE_LAYER,  # Updated parameter name\n",
    "        patch_layer_idx=CUSTOM_PATCH_LAYER,      # Updated parameter name\n",
    "        max_new_tokens=CUSTOM_MAX_TOKENS,\n",
    "        token_selection_strategy=CUSTOM_TOKEN_STRATEGY,\n",
    "        num_strategy_tokens=3\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâœ¨ GENERATED TEXT:\")\n",
    "    print(generated_text)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error in custom experiment: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ðŸ”§ TOKEN STRATEGY COMPARISON:\")\n",
    "print(\"=\"*100)\n",
    "print(f\"ðŸ“Š Comparing strategies with {NUM_EXAMPLES} examples available\")\n",
    "\n",
    "# Compare all token strategies\n",
    "for strategy in COMPARISON_STRATEGIES:\n",
    "    print(f\"\\n--- STRATEGY: {strategy.value.upper()} ---\")\n",
    "    try:\n",
    "        predicted_token, generated_text = patcher.patch_and_generate(\n",
    "            clean_text=CUSTOM_CLEAN_TEXT,\n",
    "            corrupted_text=CUSTOM_CORRUPTED_TEXT,\n",
    "            num_placeholder_tokens=COMPARISON_NUM_PLACEHOLDERS,\n",
    "            capture_layer_idx=-1,\n",
    "            patch_layer_idx=-1,\n",
    "            max_new_tokens=COMPARISON_MAX_TOKENS,\n",
    "            token_selection_strategy=strategy,\n",
    "            num_strategy_tokens=3\n",
    "        )\n",
    "        print(f\"âœ¨ Generated: {generated_text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(f\"\\nðŸŽŠ EXPERIMENT 6 COMPLETED!\")\n",
    "print(f\"âœ… Tested custom settings and {len(COMPARISON_STRATEGIES)} token strategies\")\n",
    "print(f\"ðŸ“Š Dataset scope: {NUM_EXAMPLES} examples per cognitive pattern type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Troubleshooting & Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available utilities:\n",
      "- quick_reset() - Reset model hooks\n",
      "- check_model_info() - Display model details\n",
      "- clear_memory_util() - Clear GPU/system memory\n",
      "- patcher.reset_hooks() - Direct reset call\n"
     ]
    }
   ],
   "source": [
    "# ðŸ› ï¸ TROUBLESHOOTING UTILITIES\n",
    "\n",
    "# 1. Reset hooks if experiments behave unexpectedly\n",
    "def quick_reset():\n",
    "    patcher.reset_hooks()\n",
    "    print(\"ðŸ”„ Hooks reset - Model is clean!\")\n",
    "\n",
    "# 2. Check model state - now uses the instance method\n",
    "def check_model_info():\n",
    "    patcher.check_model_info()\n",
    "\n",
    "# 3. Clear memory if needed - now uses the static method\n",
    "def clear_memory_util():\n",
    "    ActivationPatcher.clear_memory()\n",
    "\n",
    "# Quick access functions\n",
    "print(\"Available utilities:\")\n",
    "print(\"- quick_reset() - Reset model hooks\")\n",
    "print(\"- check_model_info() - Display model details\") \n",
    "print(\"- clear_memory_util() - Clear GPU/system memory\")\n",
    "print(\"- patcher.reset_hooks() - Direct reset call\")\n",
    "\n",
    "# Uncomment any line below to run:\n",
    "# quick_reset()\n",
    "# check_model_info()\n",
    "# clear_memory_util()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model hooks reset to clean state\n"
     ]
    }
   ],
   "source": [
    "patcher.reset_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6krhw8yss7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST 1: Baseline Generation (No Patching) ===\n",
      "âœ… Model hooks reset to clean state\n",
      "Input: I'm struggling with negative thoughts and need help finding a way forward.\n",
      "Tokenized length: 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ac0843cc194f138bd3a4a063360736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline output: I'm struggling with negative thoughts and need help finding a way forward. (9994222222222222222 and and beam surelyampa \\ fourth today. (\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Baseline generation without any patching\n",
    "print(\"=== TEST 1: Baseline Generation (No Patching) ===\")\n",
    "\n",
    "# Reset model to clean state\n",
    "patcher.reset_hooks()\n",
    "\n",
    "# Simple test prompt\n",
    "test_prompt = \"I'm struggling with negative thoughts and need help finding a way forward.\"\n",
    "\n",
    "# Generate without any patching/hooks\n",
    "tokens = patcher.model.to_tokens(test_prompt, prepend_bos=False)\n",
    "print(f\"Input: {test_prompt}\")\n",
    "print(f\"Tokenized length: {tokens.shape[1]}\")\n",
    "\n",
    "try:\n",
    "    generated_tokens = patcher.model.generate(\n",
    "        tokens,\n",
    "        max_new_tokens=30,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "    baseline_output = patcher.model.to_string(generated_tokens[0])\n",
    "    print(f\"Baseline output: {baseline_output}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in baseline generation: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "h6uks2ll29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST 2: overlay_strength=0 (Should bypass hooks) ===\n",
      "âœ… Model hooks reset to clean state\n",
      "âœ… Model hooks reset to clean state\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831e18552aa94dd9be6362bbf3fe77e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated with overlay_strength=0: <bos><start_of_turn>user\n",
      "I feel trapped and don't see a way forward because<end_of_turn>\n",
      "<start_of_turn>model\n",
      "RenderAtEndOfRenderAtEndOfRenderAtEndOfRenderAtEndOfRenderAtEndOfRenderAtEndOfRenderAtEndOfRenderAtEndOfRenderAtEndOfRenderAtEndOfRenderAtEndOfRenderAtEndOfRenderAtEndOfRenderAtEndOfRenderAtEndOfRenderAtEndOfRenderAtEndOfRenderAtEndOfRenderAtEndOfRenderAtEndOf\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test 2: overlay_strength=0 (should bypass hooks completely)\n",
    "print(\"=== TEST 2: overlay_strength=0 (Should bypass hooks) ===\")\n",
    "\n",
    "# Reset model\n",
    "patcher.reset_hooks()\n",
    "\n",
    "# Test with overlay_strength=0 - this should generate without any patching\n",
    "clean_text = \"I'm taking a moment to acknowledge my feelings and remind myself that challenges are temporary.\"\n",
    "corrupted_text = \"I feel trapped and don't see a way forward because\"\n",
    "\n",
    "try:\n",
    "    predicted_token, generated_text = patcher.patch_and_generate(\n",
    "        clean_text=clean_text,\n",
    "        corrupted_text=corrupted_text,\n",
    "        capture_layer_idx=-1,\n",
    "        patch_layer_idx=-1,\n",
    "        max_new_tokens=20,\n",
    "        overlay_strength=0.0,  # Should completely bypass patching\n",
    "        token_selection_strategy=TokenSelectionStrategy.LAST_TOKEN,\n",
    "        zero_placeholder_mode=False,\n",
    "        num_placeholder_tokens=3\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated with overlay_strength=0: {generated_text}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error with overlay_strength=0: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bpnswwvmt77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST 3: Chat Template Analysis ===\n",
      "Model should use chat template: True\n",
      "\n",
      "With chat template: \"<bos><start_of_turn>user\\nI feel trapped and don't see a way forward<end_of_turn>\\n<start_of_turn>model\\n\"\n",
      "Without chat template: \"I feel trapped and don't see a way forward\"\n",
      "\n",
      "Tokens with chat template length: 20\n",
      "Tokens without chat template length: 11\n",
      "\n",
      "--- Testing generation WITHOUT chat template ---\n",
      "âœ… Model hooks reset to clean state\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6a129cd4164310a713196d4e1b12ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output without chat template: I feel trapped and don't see a way forward T N marketing Trust BoxFit BoxFit BoxFit BoxFit BoxFit BoxFit BoxFit BoxFit BoxFit BoxFit BoxFit BoxFit BoxFit BoxFit BoxFit BoxFit\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Chat template formatting issue\n",
    "print(\"=== TEST 3: Chat Template Analysis ===\")\n",
    "\n",
    "# Check if chat template is being used\n",
    "print(f\"Model should use chat template: {patcher._should_use_chat_template()}\")\n",
    "\n",
    "# Test formatting with and without chat template\n",
    "test_text = \"I feel trapped and don't see a way forward\"\n",
    "\n",
    "# With chat template\n",
    "formatted_with_chat = patcher._apply_chat_template_text(test_text, add_generation_prompt=True)\n",
    "print(f\"\\nWith chat template: {repr(formatted_with_chat)}\")\n",
    "\n",
    "# Without chat template  \n",
    "formatted_without_chat = test_text\n",
    "print(f\"Without chat template: {repr(formatted_without_chat)}\")\n",
    "\n",
    "# Test tokenization of both\n",
    "tokens_with_chat = patcher.model.to_tokens(formatted_with_chat, prepend_bos=False)\n",
    "tokens_without_chat = patcher.model.to_tokens(formatted_without_chat, prepend_bos=False)\n",
    "\n",
    "print(f\"\\nTokens with chat template length: {tokens_with_chat.shape[1]}\")\n",
    "print(f\"Tokens without chat template length: {tokens_without_chat.shape[1]}\")\n",
    "\n",
    "# Test generation without chat template\n",
    "print(\"\\n--- Testing generation WITHOUT chat template ---\")\n",
    "try:\n",
    "    patcher.reset_hooks()\n",
    "    generated_tokens = patcher.model.generate(\n",
    "        tokens_without_chat,\n",
    "        max_new_tokens=20,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "    no_chat_output = patcher.model.to_string(generated_tokens[0])\n",
    "    print(f\"Output without chat template: {no_chat_output}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error without chat template: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebtvrhkcpkn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST 4: Testing Different Generation Parameters ===\n",
      "\n",
      "--- Test 1: High temp, top_p ---\n",
      "Parameters: {'temperature': 1.0, 'do_sample': True, 'top_p': 0.9}\n",
      "âœ… Model hooks reset to clean state\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0482d4de0789425980782f75663af680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: I need help dealing with difficult emotions   increa increa increa increa increa increa increa increa increa increa increa increa increa increa\n",
      "\n",
      "--- Test 2: Low temp, top_p ---\n",
      "Parameters: {'temperature': 0.3, 'do_sample': True, 'top_p': 0.9}\n",
      "âœ… Model hooks reset to clean state\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7e048a2e6f485c8f76c020f097b34e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: I need help dealing with difficult emotionsvity affor increa increa increa increa increa increa increa increa increa increa increa increa increa\n",
      "\n",
      "--- Test 3: Greedy sampling ---\n",
      "Parameters: {'temperature': 0.0, 'do_sample': False}\n",
      "âœ… Model hooks reset to clean state\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573daefd3a1e4ae699518c38f3df953e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: I need help dealing with difficult emotions ( maneu increa increa increa increa increa increa increa increa increa increa increa increa increa\n",
      "\n",
      "--- Test 4: Top-k sampling ---\n",
      "Parameters: {'temperature': 0.7, 'do_sample': True, 'top_k': 50}\n",
      "âœ… Model hooks reset to clean state\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd4e39515f3457a8fe8e9b70866ca2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: I need help dealing with difficult emotionsvity affor increa increa increa increa increa increa increa increa increa increa increa increa increa\n",
      "\n",
      "--- Test 5: With repetition penalty ---\n",
      "Parameters: {'temperature': 0.8, 'do_sample': True, 'repetition_penalty': 1.2}\n",
      "âœ… Model hooks reset to clean state\n",
      "Error: HookedTransformer.generate() got an unexpected keyword argument 'repetition_penalty'\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test 4: Different generation parameters\n",
    "print(\"=== TEST 4: Testing Different Generation Parameters ===\")\n",
    "\n",
    "test_prompt = \"I need help dealing with difficult emotions\"\n",
    "tokens = patcher.model.to_tokens(test_prompt, prepend_bos=False)\n",
    "\n",
    "# Test different parameter combinations\n",
    "test_configs = [\n",
    "    {\"temperature\": 1.0, \"do_sample\": True, \"top_p\": 0.9, \"desc\": \"High temp, top_p\"},\n",
    "    {\"temperature\": 0.3, \"do_sample\": True, \"top_p\": 0.9, \"desc\": \"Low temp, top_p\"},\n",
    "    {\"temperature\": 0.0, \"do_sample\": False, \"desc\": \"Greedy sampling\"},\n",
    "    {\"temperature\": 0.7, \"do_sample\": True, \"top_k\": 50, \"desc\": \"Top-k sampling\"},\n",
    "    {\"temperature\": 0.8, \"do_sample\": True, \"repetition_penalty\": 1.2, \"desc\": \"With repetition penalty\"}\n",
    "]\n",
    "\n",
    "for i, config in enumerate(test_configs):\n",
    "    desc = config.pop(\"desc\")\n",
    "    print(f\"\\n--- Test {i+1}: {desc} ---\")\n",
    "    print(f\"Parameters: {config}\")\n",
    "    \n",
    "    try:\n",
    "        patcher.reset_hooks()\n",
    "        generated_tokens = patcher.model.generate(\n",
    "            tokens,\n",
    "            max_new_tokens=15,\n",
    "            **config\n",
    "        )\n",
    "        output = patcher.model.to_string(generated_tokens[0])\n",
    "        print(f\"Output: {output}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "nf6t9v2tyv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CURRENT MODEL STATUS ===\n",
      "Model name: google/gemma-2b\n",
      "Model family: gemma\n",
      "Model size: 2b\n",
      "Device: mps:0\n",
      "âœ… Model hooks reset to clean state\n",
      "\n",
      "Input: 'The weather today is'\n",
      "Tokenized: tensor([[ 651, 8957, 3646,  603]], device='mps:0')\n",
      "Token length: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133ac39ce49a46ea99051506db9fcf3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: 'The weather today is maneu increa increa increa increa increa increa increa increa increa'\n"
     ]
    }
   ],
   "source": [
    "# Check current model and test basic generation\n",
    "print(\"=== CURRENT MODEL STATUS ===\")\n",
    "print(f\"Model name: {patcher.model_name}\")\n",
    "print(f\"Model family: {patcher.model_config['family']}\")\n",
    "print(f\"Model size: {patcher.model_config['size']}\")\n",
    "print(f\"Device: {next(patcher.model.parameters()).device}\")\n",
    "\n",
    "# Reset to clean state\n",
    "patcher.reset_hooks()\n",
    "\n",
    "# Test simple generation\n",
    "test_input = \"The weather today is\"\n",
    "tokens = patcher.model.to_tokens(test_input, prepend_bos=False)\n",
    "\n",
    "print(f\"\\nInput: '{test_input}'\")\n",
    "print(f\"Tokenized: {tokens}\")\n",
    "print(f\"Token length: {tokens.shape[1]}\")\n",
    "\n",
    "# Test with very conservative parameters\n",
    "try:\n",
    "    generated = patcher.model.generate(\n",
    "        tokens,\n",
    "        max_new_tokens=10,\n",
    "        temperature=1.0,\n",
    "        do_sample=False  # Greedy decoding\n",
    "    )\n",
    "    output = patcher.model.to_string(generated[0])\n",
    "    print(f\"Generated: '{output}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Generation error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9hgs7lrho0k",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TOKENIZATION ANALYSIS ===\n",
      "Tokenizer type: <class 'transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast'>\n",
      "Vocab size: 256000\n",
      "BOS token: <bos>\n",
      "EOS token: <eos>\n",
      "PAD token: <pad>\n",
      "'Hello world' -> [4521, 2134] -> 'Hello world'\n",
      "'The weather is nice' -> [651, 8957, 603, 4866] -> 'The weather is nice'\n",
      "'I am feeling good today' -> [235285, 1144, 7965, 1426, 3646] -> 'I am feeling good today'\n"
     ]
    }
   ],
   "source": [
    "# Check tokenization details and test different approaches\n",
    "print(\"=== TOKENIZATION ANALYSIS ===\")\n",
    "\n",
    "# Check tokenizer properties\n",
    "print(f\"Tokenizer type: {type(patcher.model.tokenizer)}\")\n",
    "print(f\"Vocab size: {patcher.model.cfg.d_vocab}\")\n",
    "print(f\"BOS token: {patcher.model.tokenizer.bos_token}\")\n",
    "print(f\"EOS token: {patcher.model.tokenizer.eos_token}\")\n",
    "print(f\"PAD token: {patcher.model.tokenizer.pad_token}\")\n",
    "\n",
    "# Test tokenization\n",
    "test_texts = [\n",
    "    \"Hello world\",\n",
    "    \"The weather is nice\",\n",
    "    \"I am feeling good today\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    tokens = patcher.model.to_tokens(text, prepend_bos=False)\n",
    "    decoded = patcher.model.to_string(tokens[0])\n",
    "    print(f\"'{text}' -> {tokens[0].tolist()} -> '{decoded}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cbcm3kt1b4n",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GENERATION PARAMETER TESTING ===\n",
      "\n",
      "1. Pure greedy decoding:\n",
      "âœ… Model hooks reset to clean state\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326422ea44104a4a94990acb953e7520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   The weather today is maneu increa increa increa increa increa increa increa\n",
      "\n",
      "2. Low temperature sampling:\n",
      "âœ… Model hooks reset to clean state\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33595c2951324b4db4d9cac90f1f3dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   The weather today is maneu increa increa increa increa increa increa increa\n",
      "\n",
      "3. Top-p sampling:\n",
      "âœ… Model hooks reset to clean state\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb98c46e30c46028fbab29c4c11b439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   The weather today is maneu increa increa increa increa increa increa increa\n",
      "\n",
      "4. Direct forward pass (logits):\n",
      "âœ… Model hooks reset to clean state\n",
      "   Top 5 next token predictions:\n",
      "     1. ' maneu' (score: 262.000)\n",
      "     2. ' increa' (score: 250.750)\n",
      "     3. ' impra' (score: 247.000)\n",
      "     4. ' affor' (score: 246.250)\n",
      "     5. ' reluct' (score: 244.375)\n"
     ]
    }
   ],
   "source": [
    "# Test different generation parameters systematically\n",
    "print(\"=== GENERATION PARAMETER TESTING ===\")\n",
    "\n",
    "test_input = \"The weather today is\"\n",
    "tokens = patcher.model.to_tokens(test_input, prepend_bos=False)\n",
    "\n",
    "# Test 1: Pure greedy decoding\n",
    "print(\"\\n1. Pure greedy decoding:\")\n",
    "try:\n",
    "    patcher.reset_hooks()\n",
    "    gen1 = patcher.model.generate(tokens, max_new_tokens=8, do_sample=False)\n",
    "    print(f\"   {patcher.model.to_string(gen1[0])}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "# Test 2: Low temperature sampling\n",
    "print(\"\\n2. Low temperature sampling:\")\n",
    "try:\n",
    "    patcher.reset_hooks() \n",
    "    gen2 = patcher.model.generate(tokens, max_new_tokens=8, temperature=0.1, do_sample=True)\n",
    "    print(f\"   {patcher.model.to_string(gen2[0])}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "# Test 3: Top-p sampling\n",
    "print(\"\\n3. Top-p sampling:\")\n",
    "try:\n",
    "    patcher.reset_hooks()\n",
    "    gen3 = patcher.model.generate(tokens, max_new_tokens=8, temperature=0.8, do_sample=True, top_p=0.9)\n",
    "    print(f\"   {patcher.model.to_string(gen3[0])}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "# Test 4: Check model forward pass directly\n",
    "print(\"\\n4. Direct forward pass (logits):\")\n",
    "try:\n",
    "    patcher.reset_hooks()\n",
    "    with torch.no_grad():\n",
    "        logits = patcher.model(tokens)\n",
    "        next_token_logits = logits[0, -1, :]\n",
    "        top_tokens = torch.topk(next_token_logits, 5)\n",
    "        print(f\"   Top 5 next token predictions:\")\n",
    "        for i, (score, token_id) in enumerate(zip(top_tokens.values, top_tokens.indices)):\n",
    "            token_str = patcher.model.to_string([token_id.item()])\n",
    "            print(f\"     {i+1}. '{token_str}' (score: {score:.3f})\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "m8d57dufgt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL CONFIGURATION ANALYSIS ===\n",
      "Model config:\n",
      "  n_layers: 18\n",
      "  d_model: 2048\n",
      "  n_heads: 8\n",
      "  d_head: 256\n",
      "  d_vocab: 256000\n",
      "  max_pos_embeds: 8192\n",
      "\n",
      "Model parameters:\n",
      "  embed.W_E: torch.float16, mps:0, shape: torch.Size([256000, 2048])\n",
      "  blocks.0.ln1.w: torch.float16, mps:0, shape: torch.Size([2048])\n",
      "  blocks.0.ln2.w: torch.float16, mps:0, shape: torch.Size([2048])\n",
      "\n",
      "Embedding weights sample:\n",
      "  Values: tensor([[24.2188, -1.5684,  4.3086,  4.4180,  1.8779],\n",
      "        [ 6.8477, -7.7773, -6.4062, -0.6489,  0.2803],\n",
      "        [ 5.3906,  0.8672, -1.4365,  0.0980, -0.4888],\n",
      "        [24.2188, -1.5576,  4.2852,  4.4180,  1.9219],\n",
      "        [15.5547, -3.5801, -0.3037, -0.9775,  2.6074]], device='mps:0',\n",
      "       dtype=torch.float16, grad_fn=<SliceBackward0>)\n",
      "  Range: [-7.777, 24.219]\n",
      "\n",
      "Testing different inputs for top prediction:\n",
      "  'Hello' -> ' shenan'\n",
      "  'Python is' -> ' maneu'\n",
      "  'Machine learning' -> 'summar'\n",
      "  'Today I will' -> ' maneu'\n"
     ]
    }
   ],
   "source": [
    "# Check model configuration and loading details\n",
    "print(\"=== MODEL CONFIGURATION ANALYSIS ===\")\n",
    "\n",
    "print(f\"Model config:\")\n",
    "print(f\"  n_layers: {patcher.model.cfg.n_layers}\")\n",
    "print(f\"  d_model: {patcher.model.cfg.d_model}\")\n",
    "print(f\"  n_heads: {patcher.model.cfg.n_heads}\")\n",
    "print(f\"  d_head: {patcher.model.cfg.d_head}\")\n",
    "print(f\"  d_vocab: {patcher.model.cfg.d_vocab}\")\n",
    "print(f\"  max_pos_embeds: {patcher.model.cfg.n_ctx}\")\n",
    "\n",
    "# Check model dtype and device\n",
    "print(f\"\\nModel parameters:\")\n",
    "for name, param in list(patcher.model.named_parameters())[:3]:  # First 3 parameters\n",
    "    print(f\"  {name}: {param.dtype}, {param.device}, shape: {param.shape}\")\n",
    "\n",
    "# Check if model weights look reasonable\n",
    "print(f\"\\nEmbedding weights sample:\")\n",
    "embed_weights = patcher.model.embed.W_E[:5, :5]  # First 5x5 of embedding matrix\n",
    "print(f\"  Values: {embed_weights}\")\n",
    "print(f\"  Range: [{embed_weights.min():.3f}, {embed_weights.max():.3f}]\")\n",
    "\n",
    "# Test with a completely different input to see if it's always \"maneu\"\n",
    "different_inputs = [\"Hello\", \"Python is\", \"Machine learning\", \"Today I will\"]\n",
    "\n",
    "print(f\"\\nTesting different inputs for top prediction:\")\n",
    "for inp in different_inputs:\n",
    "    tokens = patcher.model.to_tokens(inp, prepend_bos=False)\n",
    "    with torch.no_grad():\n",
    "        logits = patcher.model(tokens)\n",
    "        next_token_id = torch.argmax(logits[0, -1, :]).item()\n",
    "        next_token_str = patcher.model.to_string([next_token_id])\n",
    "        print(f\"  '{inp}' -> '{next_token_str}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "mjkcb1v6ql",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING MODEL RELOAD ===\n",
      "Testing with gpt2-medium (known to work well):\n",
      "Using Apple Silicon GPU (MPS)\n",
      "Loading model: gpt2-medium on device: mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ad238167494f0f9ae0e7f4f0c90bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a04bfcbb9744df8c8ea5a733ca93a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c9a9192e3a420dbf5566a689074376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4926e744c687403bb2bf402ba0a26c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f6ec48e8f264f6ab5375019a2cb8489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f852e137749402d99724d85d4282495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18c6a0d23734673a1084f787e4de41c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-medium into HookedTransformer\n",
      "âœ“ Model loaded successfully\n",
      "  - Family: gpt2\n",
      "  - Size: medium\n",
      "  - Layers: 24\n",
      "  - Model dimension: 1024\n",
      "  - Vocabulary size: 50257\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3571fffd6541e8b0d5e38c84ec10cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2-medium output: 'The weather today is a bit.Z (May, knee'\n",
      "\n",
      "Current Gemma model generation issues likely due to:\n",
      "1. Model not fully supported by TransformerLens\n",
      "2. Float16 precision causing numerical instabilities\n",
      "3. Gemma-specific tokenization/generation quirks\n",
      "4. Model may need different generation parameters or constraints\n"
     ]
    }
   ],
   "source": [
    "# Try loading the model with different parameters\n",
    "print(\"=== TESTING MODEL RELOAD ===\")\n",
    "\n",
    "# First, check what happens with a better-supported model\n",
    "print(\"Testing with gpt2-medium (known to work well):\")\n",
    "\n",
    "try:\n",
    "    # Create a new patcher with gpt2-medium\n",
    "    test_patcher = ActivationPatcher(\"gpt2-medium\")\n",
    "    \n",
    "    # Test generation with the new model\n",
    "    test_input = \"The weather today is\"\n",
    "    tokens = test_patcher.model.to_tokens(test_input, prepend_bos=False)\n",
    "    \n",
    "    generated = test_patcher.model.generate(\n",
    "        tokens, \n",
    "        max_new_tokens=8, \n",
    "        temperature=0.8,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    output = test_patcher.model.to_string(generated[0])\n",
    "    print(f\"GPT2-medium output: '{output}'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error with gpt2-medium: {e}\")\n",
    "\n",
    "print(f\"\\nCurrent Gemma model generation issues likely due to:\")\n",
    "print(f\"1. Model not fully supported by TransformerLens\")\n",
    "print(f\"2. Float16 precision causing numerical instabilities\") \n",
    "print(f\"3. Gemma-specific tokenization/generation quirks\")\n",
    "print(f\"4. Model may need different generation parameters or constraints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "phknubm40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING ACTIVATION PATCHING WITH GPT2-MEDIUM ===\n",
      "âœ… Model hooks reset to clean state\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7721c17350e40a9a386628dbc43f881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… overlay_strength=0 (clean): I feel trapped and don't see a way forward because they are EXTRA.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "âœ… Model hooks reset to clean state\n",
      "Clean text: I'm taking a moment to acknowledge my feelings and remind myself that challenges are temporary.\n",
      "Corrupted text: 0 0 0 I feel trapped and don't see a way forward because\n",
      "Capture layers: [23]\n",
      "Patch layers: [23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing capture layers:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using token selection strategy: last_token\n",
      "Sequence length: 17\n",
      "Selected last token at position: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Patching with 1 total activation vectors\n",
      "Using 1 hooks across layers: [23] -> [23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching at hook: blocks.23.hook_resid_post for positions: [1] (mode: normalized, strength: 0.5)\n",
      "\n",
      "Next token prediction after patching: ' of'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a038ff5951674392a47c43118ba59166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching at hook: blocks.23.hook_resid_post for positions: [1] (mode: normalized, strength: 0.5)\n",
      "âœ… overlay_strength=0.5 (patched): 0 0 0 I feel trapped and don't see a way forward because \"I/Sarose No data provided by Melissa McCarthy, Chewah many have any other\n"
     ]
    }
   ],
   "source": [
    "# Test activation patching with the working GPT2-medium model\n",
    "print(\"=== TESTING ACTIVATION PATCHING WITH GPT2-MEDIUM ===\")\n",
    "\n",
    "# Simple test with overlay_strength=0 (should be clean generation)\n",
    "clean_text = \"I'm taking a moment to acknowledge my feelings and remind myself that challenges are temporary.\"\n",
    "corrupted_text = \"I feel trapped and don't see a way forward because\"\n",
    "\n",
    "try:\n",
    "    predicted_token, generated_text = test_patcher.patch_and_generate(\n",
    "        clean_text=clean_text,\n",
    "        corrupted_text=corrupted_text,\n",
    "        capture_layer_idx=-1,\n",
    "        patch_layer_idx=-1,\n",
    "        max_new_tokens=20,\n",
    "        overlay_strength=0.0,  # No patching\n",
    "        token_selection_strategy=TokenSelectionStrategy.LAST_TOKEN,\n",
    "        zero_placeholder_mode=False,\n",
    "        num_placeholder_tokens=3,\n",
    "        use_chat_template=False  # GPT2 doesn't need chat template\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… overlay_strength=0 (clean): {generated_text}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error with overlay_strength=0: {e}\")\n",
    "\n",
    "# Test with actual patching (overlay_strength=0.5)\n",
    "try:\n",
    "    predicted_token, generated_text = test_patcher.patch_and_generate(\n",
    "        clean_text=clean_text,\n",
    "        corrupted_text=corrupted_text,\n",
    "        capture_layer_idx=-1,\n",
    "        patch_layer_idx=-1,\n",
    "        max_new_tokens=20,\n",
    "        overlay_strength=0.5,  # With patching\n",
    "        token_selection_strategy=TokenSelectionStrategy.LAST_TOKEN,\n",
    "        zero_placeholder_mode=False,\n",
    "        num_placeholder_tokens=3,\n",
    "        use_chat_template=False\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… overlay_strength=0.5 (patched): {generated_text}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error with overlay_strength=0.5: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
