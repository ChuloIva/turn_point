{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Activation Patching Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nimport torch\nimport json\nimport random\nfrom IPython.display import display, HTML\n\n# Add TransformerLens to path\nsys.path.append('/Users/ivanculo/Desktop/Projects/turn_point/third_party/TransformerLens')\n\n# Import our activation patcher\nsys.path.append('/Users/ivanculo/Desktop/Projects/turn_point/manual_activation_patching')\nfrom activation_patcher import ActivationPatcher\nfrom interpretation_templates import INTERPRETATION_TEMPLATES\n\n# Data loading utilities\ndef load_cognitive_patterns(dataset_path=\"/Users/ivanculo/Desktop/Projects/turn_point/data/final/positive_patterns.jsonl\"):\n    \"\"\"Load the cognitive patterns dataset with all text variants (positive, negative, transition).\"\"\"\n    patterns = []\n    pattern_types = {}\n    \n    with open(dataset_path, 'r') as f:\n        for line in f:\n            pattern = json.loads(line.strip())\n            patterns.append(pattern)\n            \n            # Group by cognitive pattern type\n            pattern_type = pattern['cognitive_pattern_type']\n            if pattern_type not in pattern_types:\n                pattern_types[pattern_type] = []\n            pattern_types[pattern_type].append(pattern)\n    \n    return patterns, pattern_types\n\ndef get_pattern_by_index(patterns, index):\n    \"\"\"Get a pattern by index with bounds checking.\"\"\"\n    if 0 <= index < len(patterns):\n        return patterns[index]\n    else:\n        raise IndexError(f\"Index {index} out of range. Dataset has {len(patterns)} patterns.\")\n\ndef get_pattern_by_type(pattern_types, pattern_type):\n    \"\"\"Get patterns by cognitive pattern type.\"\"\"\n    if pattern_type in pattern_types:\n        return pattern_types[pattern_type]\n    else:\n        available_types = list(pattern_types.keys())\n        raise KeyError(f\"Pattern type '{pattern_type}' not found. Available types: {available_types}\")\n\ndef get_pattern_text(pattern, text_type=\"positive\"):\n    \"\"\"\n    Get specific text variant from a pattern.\n    \n    Args:\n        pattern: The pattern dictionary\n        text_type: \"positive\", \"negative\", or \"transition\"\n    \n    Returns:\n        The requested text string\n    \"\"\"\n    text_map = {\n        \"positive\": \"positive_thought_pattern\",\n        \"negative\": \"reference_negative_example\", \n        \"transition\": \"reference_transformed_example\"\n    }\n    \n    if text_type not in text_map:\n        raise ValueError(f\"text_type must be one of: {list(text_map.keys())}\")\n    \n    field_name = text_map[text_type]\n    if field_name not in pattern:\n        raise KeyError(f\"Pattern missing field: {field_name}\")\n    \n    return pattern[field_name]\n\ndef get_template(template_name):\n    \"\"\"Get an interpretation template by name.\"\"\"\n    if template_name in INTERPRETATION_TEMPLATES:\n        return INTERPRETATION_TEMPLATES[template_name]\n    else:\n        available_templates = list(INTERPRETATION_TEMPLATES.keys())\n        raise KeyError(f\"Template '{template_name}' not found. Available templates: {available_templates}\")\n\ndef show_pattern_info(pattern):\n    \"\"\"Display detailed information about a pattern.\"\"\"\n    print(f\"üß† Pattern: {pattern['cognitive_pattern_name']}\")\n    print(f\"üîÑ Type: {pattern['cognitive_pattern_type']}\")\n    print(f\"üìù Description: {pattern['pattern_description']}\")\n    print(f\"‚ùì Source Question: {pattern['source_question']}\")\n    print(f\"\\n‚úÖ Positive Text: {pattern['positive_thought_pattern']}\")\n    print(f\"\\n‚ùå Negative Text: {pattern['reference_negative_example']}\")\n    print(f\"\\nüîÑ Transition Text: {pattern['reference_transformed_example']}\")\n\nprint(\"Imports and utilities loaded successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Model and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Choose your model here - change this and re-run to experiment with different models\nMODEL_NAME = \"google/gemma-2-2b-it\"  # Change to: gpt2-medium, EleutherAI/gpt-neo-125m, etc.\n\n# Set the BOS token for your model (change this based on your model)\nBOS_TOKEN = \"<bos>\"  # Default for Gemma models\n\n# Initialize the activation patcher\npatcher = ActivationPatcher(MODEL_NAME)\n\n# Load the cognitive patterns dataset with new utilities\npatterns, pattern_types = load_cognitive_patterns()\n\nprint(f\"Loaded {len(patterns)} cognitive patterns\")\nprint(f\"Available pattern types: {len(pattern_types)}\")\nfor pattern_type, examples in pattern_types.items():\n    print(f\"  - {pattern_type}: {len(examples)} examples\")\n\nprint(f\"\\nModel info: {patcher.get_model_info()}\")\nprint(f\"Using BOS token: '{BOS_TOKEN}'\")\nprint(f\"\\nAvailable interpretation templates: {list(INTERPRETATION_TEMPLATES.keys())}\")\nprint(f\"\\nAvailable text types: positive, negative, transition\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available pattern types:\n",
    "  1. Instrumental suicidal reasoning\n",
    "  2. Intrusive suicidal fixation\n",
    "  3. Fragmented perceptual reasoning\n",
    "  4. Over-elaborative recounting\n",
    "  5. Autobiographical integration\n",
    "  6. Cognitive disorganization\n",
    "  7. Negative self-evaluative loop\n",
    "  8. Existential rumination\n",
    "  9. Entrapment cognition\n",
    "  10. Cognitive depletion pattern\n",
    "  11. Hyper-attuned interoception\n",
    "  12. Learned helplessness loop\n",
    "  13. Internal dialectical processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Basic Activation Patching\n",
    "\n",
    "Let's start with a simple experiment using one positive pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# EXPERIMENT 1: Basic Activation Patching with Template\n\n# Configuration - Easy to modify these parameters\nPATTERN_INDEX = 2  # Change this to use different patterns (0-49)\nCLEAN_TEXT_TYPE = \"positive\"  # \"positive\", \"negative\", or \"transition\"\nTEMPLATE_NAME = \"cognitive_pattern\"  # Template to use for corrupted text\n\n# Load the selected pattern and template\nselected_pattern = get_pattern_by_index(patterns, PATTERN_INDEX)\ntemplate = get_template(TEMPLATE_NAME)\n\n# Get the clean text (what we want to capture activations from)\nclean_text = get_pattern_text(selected_pattern, CLEAN_TEXT_TYPE)\n\nprint(f\"üìã EXPERIMENT 1 SETUP:\")\nprint(f\"‚úÖ Pattern {PATTERN_INDEX}: {selected_pattern['cognitive_pattern_name']}\")\nprint(f\"üß† Pattern Type: {selected_pattern['cognitive_pattern_type']}\")\nprint(f\"üìù Clean Text Type: {CLEAN_TEXT_TYPE}\")\nprint(f\"üìÑ Clean Text: {clean_text}\")\nprint(f\"\\nüéØ Using Template: '{TEMPLATE_NAME}'\")\n\n# Extract the template components  \ntemplate_bos = template[0]  # \"<bos>\"\nzero_positions = template[1:6]  # The 5 zeros (0, 0, 0, 0, 0) \ncontinuation_text = template[6]  # The text part\n\nprint(f\"üìÑ Template: {template}\")\nprint(f\"\\nüîß Template Structure:\")\nprint(f\"  BOS placeholder: {template_bos}\")\nprint(f\"  Patch positions: {zero_positions} (count: {len(zero_positions)})\")\nprint(f\"  Continuation: {continuation_text}\")\n\n# The corrupted text is the continuation part (BOS will be added automatically)\ncorrupted_text = continuation_text\n\n# Extract key words for patching\ntarget_words = patcher._extract_key_words(clean_text)\n\nprint(f\"\\n‚ö° Running activation patching...\")\nprint(f\"  Clean text: {clean_text[:100]}...\")\nprint(f\"  Corrupted text: {corrupted_text}\")\nprint(f\"  Target words: {target_words}\")\nprint(f\"  Number of patch positions: {len(zero_positions)}\")\n\n# Perform the patching and generation\npredicted_token, generated_text = patcher.patch_and_generate(\n    clean_text=clean_text,\n    corrupted_text=corrupted_text,\n    target_words=target_words,\n    num_placeholder_tokens=len(zero_positions),\n    capture_layer_idx=-1,  # Last layer for capture\n    patch_layer_idx=-1,    # Last layer for patch\n    max_new_tokens=60,\n    bos_token=BOS_TOKEN\n)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üéä EXPERIMENT 1 RESULTS:\")\nprint(\"=\"*80)\nprint(f\"Model: {patcher.model_name}\")\nprint(f\"Pattern: {selected_pattern['cognitive_pattern_name']}\")\nprint(f\"Clean text type: {CLEAN_TEXT_TYPE}\")\nprint(f\"Template: {TEMPLATE_NAME}\")\nprint(f\"\\nüìä Generated Text:\")\nprint(f\"{generated_text}\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract key words for patching\n",
    "target_words = patcher._extract_key_words(clean_text)\n",
    "print(f\"Target words for patching: {target_words}\")\n",
    "\n",
    "# Define the template structure for corrupted text input\n",
    "# This template provides the structure: BOS token + placeholder tokens + continuation text\n",
    "template_key = \"basic_activation_template\"\n",
    "template = (\n",
    "    \"<bos>\",  # BOS token (will be replaced by actual BOS_TOKEN)\n",
    "    0, 0, 0, 0, 0,  # 5 placeholder tokens (represented as zeros)\n",
    "    \"I need to shift my perspective and find a constructive way to\"  # Continuation text\n",
    ")\n",
    "\n",
    "# Create the corrupted prompt from the template\n",
    "# The template structure is: (\"<bos>\", 0, 0, 0, 0, 0, \"\\n\\nThis neural activation represents...\")\n",
    "template_bos_token = template[0]  # \"<bos>\" from the template\n",
    "zero_tokens = template[1:6]  # The 5 zeros\n",
    "continuation_text = template[6]  # The text part\n",
    "\n",
    "print(f\"Template structure:\")\n",
    "print(f\"  Template BOS token: {template_bos_token}\")\n",
    "print(f\"  Zero tokens: {zero_tokens} (count: {len(zero_tokens)})\")\n",
    "print(f\"  Continuation: {continuation_text}\")\n",
    "\n",
    "# Use the continuation text as the corrupted_text\n",
    "# The BOS_TOKEN variable will be automatically prepended\n",
    "corrupted_text = continuation_text\n",
    "\n",
    "print(f\"\\nUsing BOS token: '{BOS_TOKEN}'\")\n",
    "print(f\"Corrupted text for patching: '{corrupted_text}'\")\n",
    "\n",
    "# Perform the patching and generation with NEW MULTI-LAYER FUNCTIONALITY\n",
    "predicted_token, generated_text = patcher.patch_and_generate(\n",
    "    clean_text=clean_text,\n",
    "    corrupted_text=corrupted_text,\n",
    "    target_words=target_words,\n",
    "    num_placeholder_tokens=5,  # This matches the 5 zero tokens in the template\n",
    "    capture_layer_idx=-1,  # Last layer for capture\n",
    "    patch_layer_idx=-1,    # Last layer for patch (same as capture)\n",
    "    max_new_tokens=60,\n",
    "    bos_token=BOS_TOKEN  # Use the manually set BOS token\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"EXPERIMENT 1 RESULTS:\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Model: {patcher.model_name}\")\n",
    "print(f\"BOS token used: '{BOS_TOKEN}'\")\n",
    "print(f\"Template used: {template_key}\")\n",
    "print(f\"Clean pattern: {sample_pattern['cognitive_pattern_name']}\")\n",
    "print(f\"Clean text (first 100 chars): {clean_text[:100]}...\")\n",
    "print(f\"Target words extracted: {target_words}\")\n",
    "print(f\"\\nGenerated Text:\\n{generated_text}\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üîÑ Reset Model State\n",
    "\n",
    "Before running experiments, it's good practice to reset any lingering hooks from previous runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÑ RESET MODEL HOOKS - Run this cell to reset the model to clean state\n",
    "# This is especially important when switching between different experiments\n",
    "\n",
    "patcher.reset_hooks()\n",
    "\n",
    "print(\"Model is now ready for clean experiments!\")\n",
    "print(\"Run this cell anytime you want to ensure no residual hooks are affecting your results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Different Layers Comparison\n",
    "\n",
    "Let's see how patching at different layers affects the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test patching at different layers with NEW MULTI-LAYER FUNCTIONALITY\n",
    "layers_to_test = [0, 3, 6, 9, -1]  # Early, middle, late, and final layers\n",
    "sample_pattern = patterns[5]  # Use a different pattern\n",
    "\n",
    "clean_text = sample_pattern['positive_thought_pattern']\n",
    "corrupted_text = \"I can't stop worrying about everything and feel completely\"\n",
    "target_words = patcher._extract_key_words(clean_text)\n",
    "\n",
    "print(f\"Pattern: {sample_pattern['cognitive_pattern_name']}\")\n",
    "print(f\"Target words: {target_words}\")\n",
    "print(f\"Corrupted prompt: {corrupted_text}\")\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "layer_results = {}\n",
    "\n",
    "for layer_idx in layers_to_test:\n",
    "    print(f\"\\n--- LAYER {layer_idx} ---\")\n",
    "    try:\n",
    "        predicted_token, generated_text = patcher.patch_and_generate(\n",
    "            clean_text=clean_text,\n",
    "            corrupted_text=corrupted_text,\n",
    "            target_words=target_words,\n",
    "            num_placeholder_tokens=5,\n",
    "            capture_layer_idx=layer_idx,  # Updated parameter name\n",
    "            patch_layer_idx=layer_idx,    # Patch to same layer\n",
    "            max_new_tokens=50\n",
    "        )\n",
    "        layer_results[layer_idx] = generated_text\n",
    "        print(f\"Generated: {generated_text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error at layer {layer_idx}: {e}\")\n",
    "        layer_results[layer_idx] = f\"Error: {e}\"\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Multiple Patterns Comparison\n",
    "\n",
    "Let's compare how different cognitive patterns affect the generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with multiple different cognitive patterns\n",
    "test_patterns = patterns[:5]  # Use first 5 patterns\n",
    "corrupted_text = \"I feel trapped and don't see a way forward because\"\n",
    "\n",
    "print(f\"Testing with corrupted prompt: '{corrupted_text}'\")\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "pattern_results = []\n",
    "\n",
    "for i, pattern in enumerate(test_patterns):\n",
    "    print(f\"\\n--- PATTERN {i+1}: {pattern['cognitive_pattern_name']} ---\")\n",
    "    \n",
    "    clean_text = pattern['positive_thought_pattern']\n",
    "    target_words = patcher._extract_key_words(clean_text)\n",
    "    \n",
    "    print(f\"Clean text (first 100 chars): {clean_text[:100]}...\")\n",
    "    print(f\"Target words: {target_words}\")\n",
    "    \n",
    "    try:\n",
    "        predicted_token, generated_text = patcher.patch_and_generate(\n",
    "            clean_text=clean_text,\n",
    "            corrupted_text=corrupted_text,\n",
    "            target_words=target_words,\n",
    "            num_placeholder_tokens=5,\n",
    "            capture_layer_idx=-1,  # Updated parameter name\n",
    "            patch_layer_idx=-1,    # Updated parameter name\n",
    "            max_new_tokens=65\n",
    "        )\n",
    "        \n",
    "        pattern_results.append({\n",
    "            'pattern_name': pattern['cognitive_pattern_name'],\n",
    "            'generated_text': generated_text\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nGENERATED TEXT:\")\n",
    "        print(generated_text)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with pattern {i+1}: {e}\")\n",
    "        pattern_results.append({\n",
    "            'pattern_name': pattern['cognitive_pattern_name'],\n",
    "            'generated_text': f\"Error: {e}\"\n",
    "        })\n",
    "    \n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Different Number of Patch Positions\n",
    "\n",
    "Let's experiment with varying the number of placeholder tokens we patch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different numbers of placeholder tokens\n",
    "placeholder_counts = [1, 3, 5, 7]\n",
    "sample_pattern = patterns[10]  # Use another pattern\n",
    "\n",
    "clean_text = sample_pattern['positive_thought_pattern']\n",
    "corrupted_text = \"My mind keeps racing with negative thoughts and I feel\"\n",
    "target_words = patcher._extract_key_words(clean_text)\n",
    "\n",
    "print(f\"Pattern: {sample_pattern['cognitive_pattern_name']}\")\n",
    "print(f\"Target words: {target_words}\")\n",
    "print(f\"Corrupted prompt: {corrupted_text}\")\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "placeholder_results = {}\n",
    "\n",
    "for num_placeholders in placeholder_counts:\n",
    "    print(f\"\\n--- {num_placeholders} PLACEHOLDER TOKENS ---\")\n",
    "    try:\n",
    "        predicted_token, generated_text = patcher.patch_and_generate(\n",
    "            clean_text=clean_text,\n",
    "            corrupted_text=corrupted_text,\n",
    "            target_words=target_words,\n",
    "            num_placeholder_tokens=num_placeholders,\n",
    "            capture_layer_idx=-1,  # Updated parameter name\n",
    "            patch_layer_idx=-1,    # Updated parameter name\n",
    "            max_new_tokens=55\n",
    "        )\n",
    "        placeholder_results[num_placeholders] = generated_text\n",
    "        print(f\"Generated: {generated_text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {num_placeholders} placeholders: {e}\")\n",
    "        placeholder_results[num_placeholders] = f\"Error: {e}\"\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 5: Baseline Comparison (No Patching)\n",
    "\n",
    "Let's generate text without any patching to see the baseline behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate baseline text without patching\n",
    "test_prompts = [\n",
    "    \"I feel completely overwhelmed and don't know how to\",\n",
    "    \"My thoughts are spiraling out of control and I feel like\",\n",
    "    \"Everything seems hopeless and I can't figure out why\",\n",
    "    \"I'm stuck in negative thinking patterns and can't seem to\",\n",
    "    \"The anxiety is taking over and I don't think I can\"\n",
    "]\n",
    "\n",
    "print(\"BASELINE GENERATIONS (No Patching):\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "baseline_results = []\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"\\n--- Prompt {i+1}: {prompt} ---\")\n",
    "    \n",
    "    # Tokenize and generate without any hooks\n",
    "    tokens = patcher.model.to_tokens(prompt)\n",
    "    generated_tokens = patcher.model.generate(\n",
    "        tokens,\n",
    "        max_new_tokens=60,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "    generated_text = patcher.model.to_string(generated_tokens[0])\n",
    "    \n",
    "    baseline_results.append({\n",
    "        'prompt': prompt,\n",
    "        'generated_text': generated_text\n",
    "    })\n",
    "    \n",
    "    print(f\"Generated: {generated_text}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 6: Custom Pattern Testing\n",
    "\n",
    "Interactive cell for testing your own patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive experiment - modify these variables to test your own patterns\n",
    "\n",
    "# You can change these to experiment with different combinations\n",
    "CUSTOM_CLEAN_TEXT = \"I'm taking a moment to acknowledge my feelings and remind myself that challenges are temporary. I can take small, manageable steps forward and focus on what I can control right now.\"\n",
    "\n",
    "CUSTOM_CORRUPTED_TEXT = \"I don't know what to do anymore and feel completely lost\"\n",
    "\n",
    "CUSTOM_TARGET_WORDS = [\"acknowledge\", \"feelings\", \"challenges\", \"temporary\", \"control\"]  # Or leave empty to auto-extract\n",
    "\n",
    "CUSTOM_NUM_PLACEHOLDERS = 5\n",
    "CUSTOM_CAPTURE_LAYER = -1  # Layer to capture activations from\n",
    "CUSTOM_PATCH_LAYER = 3    # Layer to patch activations into\n",
    "CUSTOM_MAX_TOKENS = 70\n",
    "\n",
    "print(\"CUSTOM EXPERIMENT WITH MULTI-LAYER FUNCTIONALITY:\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Clean text: {CUSTOM_CLEAN_TEXT}\")\n",
    "print(f\"Corrupted text: {CUSTOM_CORRUPTED_TEXT}\")\n",
    "\n",
    "# Auto-extract target words if not provided\n",
    "if not CUSTOM_TARGET_WORDS:\n",
    "    CUSTOM_TARGET_WORDS = patcher._extract_key_words(CUSTOM_CLEAN_TEXT)\n",
    "\n",
    "print(f\"Target words: {CUSTOM_TARGET_WORDS}\")\n",
    "print(f\"Settings: {CUSTOM_NUM_PLACEHOLDERS} placeholders, capture layer {CUSTOM_CAPTURE_LAYER}, patch layer {CUSTOM_PATCH_LAYER}, {CUSTOM_MAX_TOKENS} max tokens\")\n",
    "\n",
    "try:\n",
    "    predicted_token, generated_text = patcher.patch_and_generate(\n",
    "        clean_text=CUSTOM_CLEAN_TEXT,\n",
    "        corrupted_text=CUSTOM_CORRUPTED_TEXT,\n",
    "        target_words=CUSTOM_TARGET_WORDS,\n",
    "        num_placeholder_tokens=CUSTOM_NUM_PLACEHOLDERS,\n",
    "        capture_layer_idx=CUSTOM_CAPTURE_LAYER,  # Updated parameter name\n",
    "        patch_layer_idx=CUSTOM_PATCH_LAYER,      # Updated parameter name\n",
    "        max_new_tokens=CUSTOM_MAX_TOKENS\n",
    "    )\n",
    "    \n",
    "    print(\"\\nGENERATED TEXT:\")\n",
    "    print(generated_text)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in custom experiment: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ADVANCED MULTI-LAYER EXAMPLES:\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Example 1: Capture from multiple layers, patch to single layer\n",
    "print(\"\\nExample 1: Capture from early layers [0, 1, 2], patch to last layer\")\n",
    "try:\n",
    "    predicted_token, generated_text = patcher.patch_and_generate(\n",
    "        clean_text=CUSTOM_CLEAN_TEXT,\n",
    "        corrupted_text=CUSTOM_CORRUPTED_TEXT,\n",
    "        target_words=CUSTOM_TARGET_WORDS[:3],  # Use fewer words for this test\n",
    "        num_placeholder_tokens=3,\n",
    "        capture_layer_idx=[0, 1, 2],  # Multiple capture layers\n",
    "        patch_layer_idx=-1,           # Single patch layer\n",
    "        max_new_tokens=50\n",
    "    )\n",
    "    print(f\"Result: {generated_text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Example 2: Capture from one layer, patch to multiple layers\n",
    "print(\"\\nExample 2: Capture from last layer, patch to multiple middle layers [5, 7, 9]\")\n",
    "try:\n",
    "    predicted_token, generated_text = patcher.patch_and_generate(\n",
    "        clean_text=CUSTOM_CLEAN_TEXT,\n",
    "        corrupted_text=CUSTOM_CORRUPTED_TEXT,\n",
    "        target_words=CUSTOM_TARGET_WORDS[:3],\n",
    "        num_placeholder_tokens=3,\n",
    "        capture_layer_idx=-1,         # Single capture layer\n",
    "        patch_layer_idx=[5, 7, 9],    # Multiple patch layers\n",
    "        max_new_tokens=50\n",
    "    )\n",
    "    print(f\"Result: {generated_text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Example 3: Use range for layers\n",
    "print(\"\\nExample 3: Capture from range(3, 6), patch to range(8, 11)\")\n",
    "try:\n",
    "    predicted_token, generated_text = patcher.patch_and_generate(\n",
    "        clean_text=CUSTOM_CLEAN_TEXT,\n",
    "        corrupted_text=CUSTOM_CORRUPTED_TEXT,\n",
    "        target_words=CUSTOM_TARGET_WORDS[:3],\n",
    "        num_placeholder_tokens=3,\n",
    "        capture_layer_idx=list(range(3, 6)),  # Layers 3, 4, 5\n",
    "        patch_layer_idx=list(range(8, 11)),   # Layers 8, 9, 10\n",
    "        max_new_tokens=50\n",
    "    )\n",
    "    print(f\"Result: {generated_text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Example 4: All layers (use with caution - computationally intensive)\n",
    "print(\"\\nExample 4: Capture from 'all' layers, patch to last 3 layers\")\n",
    "try:\n",
    "    predicted_token, generated_text = patcher.patch_and_generate(\n",
    "        clean_text=CUSTOM_CLEAN_TEXT,\n",
    "        corrupted_text=CUSTOM_CORRUPTED_TEXT,\n",
    "        target_words=CUSTOM_TARGET_WORDS[:2],  # Use even fewer words\n",
    "        num_placeholder_tokens=2,\n",
    "        capture_layer_idx='all',      # All layers for capture\n",
    "        patch_layer_idx=[-3, -2, -1], # Last 3 layers for patch\n",
    "        max_new_tokens=40\n",
    "    )\n",
    "    print(f\"Result: {generated_text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Troubleshooting & Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è TROUBLESHOOTING UTILITIES\n",
    "\n",
    "# 1. Reset hooks if experiments behave unexpectedly\n",
    "def quick_reset():\n",
    "    patcher.reset_hooks()\n",
    "    print(\"üîÑ Hooks reset - Model is clean!\")\n",
    "\n",
    "\n",
    "\n",
    "# 2. Check model state\n",
    "def check_model_info():\n",
    "    print(\"üìä MODEL STATUS:\")\n",
    "    print(f\"  Model: {patcher.model_name}\")\n",
    "    print(f\"  Device: {patcher.model.cfg.device}\")\n",
    "    print(f\"  Layers: {patcher.model.cfg.n_layers}\")\n",
    "    print(f\"  D_model: {patcher.model.cfg.d_model}\")\n",
    "    print(f\"  Vocab size: {patcher.model.cfg.d_vocab}\")\n",
    "    \n",
    "# 3. Clear memory if needed\n",
    "def clear_memory():\n",
    "    import torch\n",
    "    import gc\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"üßπ Memory cleared!\")\n",
    "\n",
    "# Quick access functions\n",
    "print(\"Available utilities:\")\n",
    "print(\"- quick_reset() - Reset model hooks\")\n",
    "print(\"- check_model_info() - Display model details\") \n",
    "print(\"- clear_memory() - Clear GPU/system memory\")\n",
    "print(\"- patcher.reset_hooks() - Direct reset call\")\n",
    "\n",
    "# Uncomment any line below to run:\n",
    "# quick_reset()\n",
    "# check_model_info()\n",
    "# clear_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}