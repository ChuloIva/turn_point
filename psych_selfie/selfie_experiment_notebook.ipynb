{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SelfIE Psychology Experiments\n",
    "\n",
    "This notebook implements cognitive pattern analysis using the SelfIE (Self-Interpretation of Embeddings) technique, providing a similar workflow to the manual activation patching experiments but using the model's own ability to interpret its internal representations.\n",
    "\n",
    "## How SelfIE Works Here\n",
    "\n",
    "- **Source text**: We extract internal representations from cognitive pattern text\n",
    "- **Interpretation**: The model describes what these representations mean in natural language\n",
    "- **Analysis**: We compare interpretations between different types of cognitive patterns\n",
    "\n",
    "Unlike activation patching which modifies model behavior, SelfIE reveals what the model \"thinks\" its internal representations mean, providing interpretable insights into cognitive processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "\n",
    "# Add our SelfIE wrapper to path\n",
    "# sys.path.append('/Users/ivanculo/Desktop/Projects/turn_point/psych_selfie')\n",
    "# sys.path.append('/Users/ivanculo/Desktop/Projects/turn_point/manual_activation_patching')\n",
    "\n",
    "sys.path.append('/home/koalacrown/Desktop/Code/Projects/turnaround/turn_point/psych_selfie')\n",
    "sys.path.append('/home/koalacrown/Desktop/Code/Projects/turnaround/turn_point/manual_activation_patching')\n",
    "\n",
    "# Import SelfIE wrapper and utilities\n",
    "from selfie_patcher import SelfIEPatcher, TokenSelectionStrategy, AggregationStrategy\n",
    "from utils import process_layers_to_interpret\n",
    "\n",
    "# Import utility functions from the original activation patcher for dataset loading\n",
    "try:\n",
    "    from activation_patcher import ActivationPatcher\n",
    "    load_cognitive_patterns = ActivationPatcher.load_cognitive_patterns\n",
    "    get_pattern_by_index = ActivationPatcher.get_pattern_by_index\n",
    "    get_pattern_by_type = ActivationPatcher.get_pattern_by_type\n",
    "    get_pattern_text = ActivationPatcher.get_pattern_text\n",
    "    filter_patterns_by_count = ActivationPatcher.filter_patterns_by_count\n",
    "    get_filtered_patterns_by_type = ActivationPatcher.get_filtered_patterns_by_type\n",
    "    list_available_pattern_types = ActivationPatcher.list_available_pattern_types\n",
    "    show_pattern_info = ActivationPatcher.show_pattern_info\n",
    "    get_random_pattern_by_type = ActivationPatcher.get_random_pattern_by_type\n",
    "    get_patterns_by_type = ActivationPatcher.get_patterns_by_type\n",
    "    print(\"‚úì Successfully imported cognitive pattern utilities\")\n",
    "except ImportError as e:\n",
    "    warnings.warn(f\"Could not import pattern utilities: {e}\")\n",
    "    print(\"‚ö†Ô∏è  Will use manual pattern definitions\")\n",
    "\n",
    "print(\"SelfIE Psychology Experiment Setup Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize Model and Load Dataset\n",
    "# Model Selection - Choose a LLaMA-compatible model for SelfIE\n",
    "# Note: SelfIE currently works best with LLaMA models\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"  # Change to available LLaMA model\n",
    "# Alternative models to try:\n",
    "# MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# MODEL_NAME = \"huggyllama/llama-7b\"\n",
    "\n",
    "# Initialize the SelfIE patcher\n",
    "print(f\"Initializing SelfIE with model: {MODEL_NAME}\")\n",
    "print(\"Note: This requires transformers==4.34.0 (see requirements.txt)\")\n",
    "\n",
    "try:\n",
    "    selfie_patcher = SelfIEPatcher(MODEL_NAME)\n",
    "    print(\"‚úì SelfIE patcher initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing SelfIE: {e}\")\n",
    "    print(\"\\nüîß Troubleshooting tips:\")\n",
    "    print(\"1. Ensure you have transformers==4.34.0 installed\")\n",
    "    print(\"2. Make sure the model is available and you have access\")\n",
    "    print(\"3. Try a different LLaMA model\")\n",
    "    print(\"4. Check your GPU memory and CUDA setup\")\n",
    "\n",
    "# Load the cognitive patterns dataset\n",
    "print(\"\\nüìä Loading cognitive patterns dataset...\")\n",
    "try:\n",
    "    patterns, pattern_types = load_cognitive_patterns()\n",
    "    print(f\"‚úì Loaded {len(patterns)} cognitive patterns\")\n",
    "    print(f\"‚úì Found {len(pattern_types)} different pattern types\")\n",
    "    print(\"\\nüìã Available pattern types:\")\n",
    "    list_available_pattern_types(pattern_types)\n",
    "    print(f\"\\nüí° Each pattern has three text variants: positive, negative, transition\")\n",
    "    print(\"Available text types: positive, negative, transition\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading dataset: {e}\")\n",
    "    print(\"\\n‚ö†Ô∏è  Using manual pattern definitions for this demo\")\n",
    "    # Define some sample patterns manually\n",
    "    patterns = [\n",
    "        {\n",
    "            'cognitive_pattern_name': 'Negative Self-Evaluation',\n",
    "            'cognitive_pattern_type': 'Negative self-evaluative loop',\n",
    "            'positive_thought_pattern': \"I'm learning and growing from my mistakes, and that's part of being human.\",\n",
    "            'reference_negative_example': \"I always mess everything up and never do anything right.\",\n",
    "            'reference_transformed_example': \"I made a mistake, but I can learn from this experience.\"\n",
    "        },\n",
    "        {\n",
    "            'cognitive_pattern_name': 'Anxiety Response',\n",
    "            'cognitive_pattern_type': 'Anxiety pattern',\n",
    "            'positive_thought_pattern': \"I can handle challenging situations by taking things one step at a time.\",\n",
    "            'reference_negative_example': \"Everything is going to go wrong and I won't be able to cope.\",\n",
    "            'reference_transformed_example': \"I'm feeling anxious, but I can use coping strategies to manage this.\"\n",
    "        }\n",
    "    ]\n",
    "    pattern_types = {\n",
    "        'Negative self-evaluative loop': [patterns[0]], \n",
    "        'Anxiety pattern': [patterns[1]]\n",
    "    }\n",
    "    print(f\"‚úì Using {len(patterns)} manual patterns for demonstration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ TEST: Verify Data Loading and Access\n",
    "\n",
    "print(\"üß™ TESTING DATA LOADING AND ACCESS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Test basic data access\n",
    "    print(f\"üìä Total patterns loaded: {len(patterns)}\")\n",
    "    print(f\"üìã Pattern types available: {len(pattern_types)}\")\n",
    "    \n",
    "    # Test filtering functionality\n",
    "    test_filtered_patterns, test_filtered_types = filter_patterns_by_count(pattern_types, 5)\n",
    "    print(f\"‚úÖ Filtering test: {len(test_filtered_patterns)} patterns when limited to 5 per type\")\n",
    "    \n",
    "    # Test pattern retrieval\n",
    "    first_pattern_type = list(pattern_types.keys())[0]\n",
    "    test_pattern = get_pattern_by_type(pattern_types, first_pattern_type, 0)\n",
    "    print(f\"‚úÖ Pattern retrieval test: Got pattern '{test_pattern['cognitive_pattern_name']}'\")\n",
    "    \n",
    "    # Test text extraction\n",
    "    positive_text = get_pattern_text(test_pattern, \"positive\")\n",
    "    negative_text = get_pattern_text(test_pattern, \"negative\")\n",
    "    transition_text = get_pattern_text(test_pattern, \"transition\")\n",
    "    \n",
    "    print(f\"‚úÖ Text extraction test:\")\n",
    "    print(f\"   Positive: {positive_text[:50]}...\")\n",
    "    print(f\"   Negative: {negative_text[:50]}...\")\n",
    "    print(f\"   Transition: {transition_text[:50]}...\")\n",
    "    \n",
    "    # Show sample pattern info\n",
    "    print(f\"\\nüìã SAMPLE PATTERN DETAILS:\")\n",
    "    show_pattern_info(test_pattern)\n",
    "    \n",
    "    print(f\"\\nüéâ All data loading tests PASSED!\")\n",
    "    print(f\"‚úÖ Ready to run SelfIE experiments with cognitive patterns dataset\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Data loading test FAILED: {e}\")\n",
    "    print(\"Check that the dataset path and utility functions are correct\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Basic SelfIE Interpretation\n",
    "\n",
    "Let's start with a simple experiment to interpret internal representations of cognitive patterns using SelfIE.\n",
    "\n",
    "### What this experiment does:\n",
    "- **Input**: We provide a cognitive pattern text (positive or negative thought)\n",
    "- **Extraction**: SelfIE extracts internal representations from specific layers and token positions\n",
    "- **Interpretation**: The model describes what these representations mean in natural language\n",
    "- **Analysis**: We examine the interpretations to understand how the model processes different cognitive patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 1: Basic SelfIE Interpretation\n",
    "\n",
    "# ===== CONFIGURATION SECTION =====\n",
    "NUM_EXAMPLES_PER_TYPE = 1  # Number of examples to use per pattern type (1-40)\n",
    "PATTERN_TYPE = \"Existential rumination\"  # Choose from the list above\n",
    "PATTERN_INDEX_WITHIN_TYPE = 0  # If there are multiple examples of this type, choose which one (0-based)\n",
    "TEXT_TYPE = \"negative\"  # \"positive\", \"negative\", or \"transition\"\n",
    "INTERPRETATION_TEMPLATE = \"cognitive_pattern\"  # Template for interpretation\n",
    "LAYERS_TO_INTERPRET = \"all\"  # Enhanced options: \"all\", [-1, -2, -3], (0, 5), \"0:5\", \"0:10:2\"\n",
    "TOKEN_STRATEGY = TokenSelectionStrategy.LAST_TOKEN  # How to select tokens\n",
    "MAX_INTERPRETATION_TOKENS = 40  # Max tokens for interpretation\n",
    "BATCH_SIZE = 1\n",
    "INJECTION_LAYER = 3  # Layer where extracted activations are injected (0-based from start)\n",
    "AGGREGATION_STRATEGY = AggregationStrategy.PRINCIPAL_COMPONENT  # How to average activations when aggregating\n",
    "# ====================================\n",
    "\n",
    "# Apply runtime filtering to use only NUM_EXAMPLES_PER_TYPE examples per pattern type\n",
    "filtered_patterns, filtered_pattern_types = filter_patterns_by_count(pattern_types, NUM_EXAMPLES_PER_TYPE)\n",
    "\n",
    "# Get the pattern to analyze using the filtered dataset\n",
    "try:\n",
    "    selected_pattern = get_pattern_by_type(filtered_pattern_types, PATTERN_TYPE, PATTERN_INDEX_WITHIN_TYPE)\n",
    "    input_text = get_pattern_text(selected_pattern, TEXT_TYPE)\n",
    "    pattern_name = selected_pattern['cognitive_pattern_name']\n",
    "    \n",
    "    # Process the layer specification to get actual layer indices\n",
    "    layers_to_use = process_layers_to_interpret(LAYERS_TO_INTERPRET)\n",
    "    \n",
    "    # print(f\"\\nüß† EXPERIMENT 1: SelfIE Interpretation\")\n",
    "    # print(f\"üìä Using {NUM_EXAMPLES_PER_TYPE} examples per pattern type (runtime filtered)\")\n",
    "    # print(f\"üîç Pattern Type: {PATTERN_TYPE}\")\n",
    "    # print(f\"üß† Pattern: {pattern_name}\")\n",
    "    # print(f\"üìù Text type: {TEXT_TYPE}\")\n",
    "    # print(f\"üìñ Input text: {input_text}\")\n",
    "    # print(f\"üèóÔ∏è Layers to interpret: {LAYERS_TO_INTERPRET} ‚Üí {layers_to_use}\")\n",
    "    # print(f\"üíâ Injection layer: {INJECTION_LAYER}\")\n",
    "    # print(f\"üîß Token strategy: {TOKEN_STRATEGY.value}\")\n",
    "    # print(f\"üßÆ Aggregation strategy (used when BATCH_SIZE > 1): {AGGREGATION_STRATEGY.value}\")\n",
    "    # print(\"\\\\n\" + \"=\"*80)\n",
    "\n",
    "    # Show pattern details for context\n",
    "    print(\"\\\\nüîç PATTERN DETAILS:\")\n",
    "    show_pattern_info(selected_pattern)\n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading pattern: {e}\")\n",
    "    print(\"Available pattern types:\")\n",
    "\n",
    "\n",
    "try:\n",
    "    if BATCH_SIZE <= 1:\n",
    "        # Perform standard SelfIE interpretation on a single input\n",
    "        interpretation_results = selfie_patcher.interpret_text(\n",
    "            text=input_text,\n",
    "            layers_to_interpret=layers_to_use,\n",
    "            interpretation_template=INTERPRETATION_TEMPLATE,\n",
    "            max_new_tokens=MAX_INTERPRETATION_TOKENS,\n",
    "            batch_size=1,\n",
    "            k=INJECTION_LAYER,\n",
    "            # token_positions=TOKEN_STRATEGY\n",
    "        )\n",
    "    else:\n",
    "        # Aggregate activations across multiple examples of the same pattern type\n",
    "        print(f\"\\nüîó Aggregating across {BATCH_SIZE} examples using {AGGREGATION_STRATEGY.value}...\")\n",
    "        # Collect up to BATCH_SIZE examples from the selected pattern type\n",
    "        available = filtered_pattern_types.get(PATTERN_TYPE, [])\n",
    "        if not available:\n",
    "            raise ValueError(f\"No patterns available for type: {PATTERN_TYPE}\")\n",
    "        patterns_for_aggregation = available[:BATCH_SIZE]\n",
    "        interpretation_results = selfie_patcher.batch_interpret_patterns(\n",
    "            patterns=patterns_for_aggregation,\n",
    "            text_type=TEXT_TYPE,\n",
    "            aggregation_strategy=AGGREGATION_STRATEGY,\n",
    "            layers_to_interpret=layers_to_use,\n",
    "            interpretation_template=INTERPRETATION_TEMPLATE,\n",
    "            max_new_tokens=MAX_INTERPRETATION_TOKENS,\n",
    "            batch_size=1,\n",
    "            k=INJECTION_LAYER,\n",
    "        )\n",
    "    \n",
    "    print(\"\\nüéä EXPERIMENT 1 RESULTS:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Display results in a readable format\n",
    "    for idx, row in interpretation_results.iterrows():\n",
    "        if 'token_decoded' in row:\n",
    "            print(f\"\\nüìç Layer {row['layer']}, Token {row['token']} ('{row['token_decoded']}'):\")\n",
    "        else:\n",
    "            print(f\"\\nüìç Layer {row['layer']}, Token {row['token']}\")\n",
    "        print(f\"   Interpretation: {row['interpretation'].strip()}\")\n",
    "    \n",
    "    print(\"\\nüìä Full Results DataFrame:\")\n",
    "    display_columns = [c for c in ['layer', 'token', 'token_decoded', 'aggregation_strategy', 'num_patterns', 'interpretation'] if c in interpretation_results.columns]\n",
    "    display(interpretation_results[display_columns])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in SelfIE interpretation: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Comparing Positive vs Negative Patterns\n",
    "\n",
    "Let's compare how the model interprets positive versus negative cognitive patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 2: Positive vs Negative Pattern Comparison\n",
    "\n",
    "# ===== CONFIGURATION SECTION =====\n",
    "NUM_EXAMPLES_PER_TYPE = 15  # Number of examples to use per pattern type (1-40)\n",
    "COMPARISON_PATTERN_TYPE = \"Intrusive suicidal fixation\"  # Choose from available types\n",
    "COMPARISON_PATTERN_INDEX = 0  # Which example within the type\n",
    "COMPARISON_LAYER = [-1]  # Enhanced options: \"all\", [-1, -2, -3], (0, 5), \"0:5\", \"0:10:2\"\n",
    "COMPARISON_STRATEGY = TokenSelectionStrategy.LAST_TOKEN\n",
    "COMPARISON_TEMPLATE = \"psychological_state\"\n",
    "COMPARISON_MAX_TOKENS = 30\n",
    "COMPARISON_INJECTION_LAYER = 1  # Layer where extracted activations are injected (0-based from start)\n",
    "# ====================================\n",
    "\n",
    "# Apply runtime filtering\n",
    "filtered_patterns, filtered_pattern_types = filter_patterns_by_count(pattern_types, NUM_EXAMPLES_PER_TYPE)\n",
    "\n",
    "try:\n",
    "    selected_pattern = get_pattern_by_type(filtered_pattern_types, COMPARISON_PATTERN_TYPE, COMPARISON_PATTERN_INDEX)\n",
    "    \n",
    "    positive_text = get_pattern_text(selected_pattern, \"positive\")\n",
    "    negative_text = get_pattern_text(selected_pattern, \"negative\")\n",
    "    pattern_name = selected_pattern['cognitive_pattern_name']\n",
    "    \n",
    "    # Process the layer specification\n",
    "    layers_to_use = process_layers_to_interpret(COMPARISON_LAYER)\n",
    "    \n",
    "    print(f\"\\\\nüîÑ EXPERIMENT 2: Positive vs Negative Comparison\")\n",
    "    print(f\"üìä Using {NUM_EXAMPLES_PER_TYPE} examples per pattern type (runtime filtered)\")\n",
    "    print(f\"üîç Pattern Type: {COMPARISON_PATTERN_TYPE}\")\n",
    "    print(f\"üß† Pattern: {pattern_name}\")\n",
    "    print(f\"üèóÔ∏è Layer: {COMPARISON_LAYER} ‚Üí {layers_to_use}\")\n",
    "    print(f\"üíâ Injection layer: {COMPARISON_INJECTION_LAYER}\")\n",
    "    print(f\"üîß Strategy: {COMPARISON_STRATEGY.value}\")\n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Show pattern details\n",
    "    print(\"\\\\nüîç PATTERN DETAILS:\")\n",
    "    show_pattern_info(selected_pattern)\n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # Interpret positive pattern\n",
    "        print(\"\\\\n‚úÖ Analyzing POSITIVE pattern...\")\n",
    "        print(f\"Text: {positive_text}\")\n",
    "        \n",
    "        positive_results = selfie_patcher.interpret_text(\n",
    "            text=positive_text,\n",
    "            layers_to_interpret=layers_to_use,\n",
    "            interpretation_template=COMPARISON_TEMPLATE,\n",
    "            max_new_tokens=COMPARISON_MAX_TOKENS,\n",
    "            batch_size=1,\n",
    "            k=COMPARISON_INJECTION_LAYER\n",
    "        )\n",
    "        \n",
    "        # Interpret negative pattern\n",
    "        print(\"\\\\n‚ùå Analyzing NEGATIVE pattern...\")\n",
    "        print(f\"Text: {negative_text}\")\n",
    "        \n",
    "        negative_results = selfie_patcher.interpret_text(\n",
    "            text=negative_text,\n",
    "            layers_to_interpret=layers_to_use,\n",
    "            interpretation_template=COMPARISON_TEMPLATE,\n",
    "            max_new_tokens=COMPARISON_MAX_TOKENS,\n",
    "            batch_size=1,\n",
    "            k=COMPARISON_INJECTION_LAYER\n",
    "        )\n",
    "        \n",
    "        print(\"\\\\nüéä EXPERIMENT 2 RESULTS:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        print(\"\\\\n‚úÖ POSITIVE INTERPRETATION:\")\n",
    "        for idx, row in positive_results.iterrows():\n",
    "            print(f\"   {row['interpretation'].strip()}\")\n",
    "        \n",
    "        print(\"\\\\n‚ùå NEGATIVE INTERPRETATION:\")\n",
    "        for idx, row in negative_results.iterrows():\n",
    "            print(f\"   {row['interpretation'].strip()}\")\n",
    "        \n",
    "        # Store results for further analysis\n",
    "        comparison_results = {\n",
    "            'positive': positive_results,\n",
    "            'negative': negative_results,\n",
    "            'pattern_name': pattern_name,\n",
    "            'pattern_type': COMPARISON_PATTERN_TYPE\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in comparison experiment: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading pattern: {e}\")\n",
    "    print(\"Available pattern types:\")\n",
    "    for pattern_type in filtered_pattern_types.keys():\n",
    "        print(f\"  - {pattern_type}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Multi-Layer Analysis\n",
    "\n",
    "Analyze how interpretations change across different layers of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 3: Multi-Layer Analysis\n",
    "\n",
    "# ===== CONFIGURATION SECTION =====\n",
    "NUM_EXAMPLES_PER_TYPE = 20  # Number of examples to use per pattern type (1-40)\n",
    "MULTILAYER_PATTERN_TYPE = \"Entrapment cognition\"  # Choose from available types\n",
    "MULTILAYER_PATTERN_INDEX = 2  # Which example within the type\n",
    "MULTILAYER_TEXT_TYPE = \"negative\"  # \"positive\", \"negative\", or \"transition\"\n",
    "MULTILAYER_LAYERS = \"0:10:2\"  # Enhanced options: \"all\", [-1, -2, -3], (0, 5), \"0:5\", \"0:10:2\"\n",
    "MULTILAYER_TEMPLATE = \"cognitive_pattern\"\n",
    "MULTILAYER_STRATEGY = TokenSelectionStrategy.KEYWORDS\n",
    "MULTILAYER_MAX_TOKENS = 35\n",
    "MULTILAYER_INJECTION_LAYER = 1  # Layer where extracted activations are injected (0-based from start)\n",
    "# ====================================\n",
    "\n",
    "# Apply runtime filtering\n",
    "filtered_patterns, filtered_pattern_types = filter_patterns_by_count(pattern_types, NUM_EXAMPLES_PER_TYPE)\n",
    "\n",
    "try:\n",
    "    selected_pattern = get_pattern_by_type(filtered_pattern_types, MULTILAYER_PATTERN_TYPE, MULTILAYER_PATTERN_INDEX)\n",
    "    multilayer_text = get_pattern_text(selected_pattern, MULTILAYER_TEXT_TYPE)\n",
    "    pattern_name = selected_pattern['cognitive_pattern_name']\n",
    "    \n",
    "    # Process the layer specification\n",
    "    layers_to_use = process_layers_to_interpret(MULTILAYER_LAYERS)\n",
    "    \n",
    "    print(f\"\\\\nüß≠ EXPERIMENT 3: Multi-Layer Analysis\")\n",
    "    print(f\"üìä Using {NUM_EXAMPLES_PER_TYPE} examples per pattern type (runtime filtered)\")\n",
    "    print(f\"üîç Pattern Type: {MULTILAYER_PATTERN_TYPE}\")\n",
    "    print(f\"üß† Pattern: {pattern_name}\")\n",
    "    print(f\"üìù Text type: {MULTILAYER_TEXT_TYPE}\")\n",
    "    print(f\"üìñ Text: {multilayer_text}\")\n",
    "    print(f\"üèóÔ∏è Analyzing layers: {MULTILAYER_LAYERS} ‚Üí {layers_to_use}\")\n",
    "    print(f\"üíâ Injection layer: {MULTILAYER_INJECTION_LAYER}\")\n",
    "    print(f\"üîß Token strategy: {MULTILAYER_STRATEGY.value}\")\n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Show pattern details\n",
    "    print(\"\\\\nüîç PATTERN DETAILS:\")\n",
    "    show_pattern_info(selected_pattern)\n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "\n",
    "    try:\n",
    "        # Analyze across multiple layers\n",
    "        multilayer_results = selfie_patcher.interpret_text(\n",
    "            text=multilayer_text,\n",
    "            layers_to_interpret=layers_to_use,\n",
    "            interpretation_template=MULTILAYER_TEMPLATE,\n",
    "            max_new_tokens=MULTILAYER_MAX_TOKENS,\n",
    "            batch_size=2,\n",
    "            k=MULTILAYER_INJECTION_LAYER\n",
    "        )\n",
    "        \n",
    "        print(\"\\\\nüéä EXPERIMENT 3 RESULTS:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Group results by layer\n",
    "        for layer in sorted(set(layers_to_use)):\n",
    "            layer_results = multilayer_results[multilayer_results['layer'] == layer]\n",
    "            print(f\"\\\\nüèóÔ∏è LAYER {layer}:\")\n",
    "            for idx, row in layer_results.iterrows():\n",
    "                print(f\"   Token '{row['token_decoded']}' ‚Üí {row['interpretation'].strip()}\")\n",
    "        \n",
    "        # Display summary table\n",
    "        print(\"\\\\nüìä Summary Table:\")\n",
    "        display(multilayer_results[['layer', 'token_decoded', 'interpretation']])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in multi-layer analysis: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading pattern: {e}\")\n",
    "    print(\"Available pattern types:\")\n",
    "    for pattern_type in filtered_pattern_types.keys():\n",
    "        print(f\"  - {pattern_type}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for the notebook\n",
    "\n",
    "def reset_environment():\n",
    "    \"\"\"Reset the SelfIE environment\"\"\"\n",
    "    selfie_patcher.reset_hooks()  # No-op for SelfIE but maintains compatibility\n",
    "    print(\"üîÑ Environment reset\")\n",
    "\n",
    "def show_model_info():\n",
    "    \"\"\"Display current model information\"\"\"\n",
    "    selfie_patcher.check_model_info()\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU memory\"\"\"\n",
    "    SelfIEPatcher.clear_memory()\n",
    "\n",
    "print(\"Utility functions loaded:\")\n",
    "print(\"- reset_environment() - Reset the environment\")\n",
    "print(\"- show_model_info() - Display model information\")\n",
    "print(\"- clear_memory() - Clear GPU memory\")\n",
    "\n",
    "# Uncomment any line below to run:\n",
    "# reset_environment()\n",
    "# show_model_info()\n",
    "# clear_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
