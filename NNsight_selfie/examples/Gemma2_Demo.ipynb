{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNsight Selfie with Gemma2 2B - Complete Demo\n",
    "\n",
    "This notebook demonstrates how to use NNsight Selfie for neural network interpretation with Google's Gemma2 2B model. It showcases automatic MPS detection for Apple Silicon Macs and various interpretation techniques.\n",
    "\n",
    "## Features Demonstrated:\n",
    "- Automatic device detection (MPS/CUDA/CPU)\n",
    "- Token-level interpretation \n",
    "- Activation extraction and analysis\n",
    "- Vector arithmetic with activations\n",
    "- Custom interpretation prompts\n",
    "- Performance optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install the required packages and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install torch transformers nnsight tqdm pandas numpy matplotlib seaborn\n",
    "# !pip install accelerate  # For efficient model loading\n",
    "\n",
    "# FOR AMD GPU\n",
    "import os\n",
    "os.environ[\"HSA_OVERRIDE_GFX_VERSION\"] = \"11.0.0\"\n",
    "os.environ[\"HIP_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"AMD_SERIALIZE_KERNEL\"] = \"3\"\n",
    "os.environ[\"TORCH_USE_HIP_DSA\"] = \"1\"\n",
    "\n",
    "#IMPORTS\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Add parent directory to path to import nnsight_selfie\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(os.getcwd()), '..'))\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Import our library\n",
    "from nnsight_selfie import (\n",
    "    ModelAgnosticSelfie, \n",
    "    InterpretationPrompt, \n",
    "    print_device_info, \n",
    "    get_optimal_device\n",
    ")\n",
    "\n",
    "# Standard imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✅ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Detection and Compatibility\n",
    "\n",
    "Let's check what devices are available and see automatic MPS detection in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Device Detection ===\n",
      "=== Device Information ===\n",
      "Platform: Linux x86_64\n",
      "Python: 3.12.3\n",
      "PyTorch: 2.4.1+rocm6.0\n",
      "Optimal Device: cuda\n",
      "\n",
      "=== MPS Support ===\n",
      "MPS Available: False\n",
      "MPS Built: False\n",
      "\n",
      "=== CUDA Support ===\n",
      "CUDA Available: True\n",
      "CUDA Version: None\n",
      "Device Count: 1\n",
      "Primary Device: AMD Radeon RX 7700 XT\n",
      "\n",
      "🚀 Optimal device for this system: cuda\n",
      "\n",
      "🔧 PyTorch version: 2.4.1+rocm6.0\n",
      "🚀 NVIDIA GPU detected: AMD Radeon RX 7700 XT\n"
     ]
    }
   ],
   "source": [
    "# Show detailed device information\n",
    "print(\"=== Device Detection ===\")\n",
    "print_device_info()\n",
    "\n",
    "# Get optimal device\n",
    "optimal_device = get_optimal_device()\n",
    "print(f\"🚀 Optimal device for this system: {optimal_device}\")\n",
    "\n",
    "# Show PyTorch device info\n",
    "print(f\"\\n🔧 PyTorch version: {torch.__version__}\")\n",
    "if optimal_device == \"mps\":\n",
    "    print(\"🍎 Apple Silicon Mac detected - using Metal Performance Shaders!\")\n",
    "elif optimal_device == \"cuda\":\n",
    "    print(f\"🚀 NVIDIA GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"💻 Using CPU - consider upgrading to GPU/MPS for better performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Gemma2 2B Model\n",
    "\n",
    "Now let's load the Gemma2 2B model with automatic device detection. This model is perfect for interpretation tasks - large enough to be interesting but small enough to run efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Gemma model...\n",
      "This may take a few minutes on first run (downloading ~5GB)\n",
      "Initializing model on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out vision components for Gemma 3 4B model.\n",
      "Model loaded successfully with 35 layers detected.\n",
      "✅ Model loaded successfully on: cuda\n",
      "📊 Detected 35 transformer layers\n",
      "🔤 Tokenizer vocabulary size: 262145\n",
      "\n",
      "📋 Layer structure:\n",
      "  Layer 0: model.language_model.layers.0\n",
      "  Layer 1: model.language_model.layers.1\n",
      "  Layer 2: model.language_model.layers.2\n",
      "  Layer 3: model.language_model.layers.3\n",
      "  Layer 4: model.language_model.layers.4\n",
      "  ... and 30 more layers\n"
     ]
    }
   ],
   "source": [
    "# Initialize Gemma2 2B with automatic device selection\n",
    "print(\"Loading Gemma model...\")\n",
    "print(\"This may take a few minutes on first run (downloading ~5GB)\")\n",
    "\n",
    "\n",
    "# - google/gemma-2b (5.0G)\n",
    "# - richie-ghost/depression-data (35.7K)\n",
    "# - mistralai/Mistral-7B-Instruct-v0.1 (14.5G)\n",
    "# - google/gemma-2-2b-it (5.3G)\n",
    "# - Qwen/Qwen3-8B (728.0)\n",
    "# - meta-llama/Llama-2-7b-chat-hf (13.5G)\n",
    "# - meta-llama/Llama-2-7b-hf (2.3M)\n",
    "# - tylercosgrove/mistral-7b-sparse-autoencoder-layer16 (4.3G)\n",
    "# - mrjunos/depression-reddit-cleaned (1.7M)\n",
    "# - google/gemma-7b (17.1G)\n",
    "# - google/gemma-scope-2b-pt-res (2.4G)\n",
    "# - meta-llama/Llama-3.1-8B-Instruct (16.1G)\n",
    "# - gpt2 (551.0M)\n",
    "\n",
    "try:\n",
    "    # Load with automatic device detection\n",
    "    selfie = ModelAgnosticSelfie(\n",
    "        \"google/gemma-3-4b-it\",  # Instruction-tuned version\n",
    "        dtype=torch.bfloat16,  # Use half precision for efficiency\n",
    "        # device_map=\"auto\"  # This is handled automatically now\n",
    "        load_in_8bit=False,  # Disable quantization for full precision\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Model loaded successfully on: {selfie.device}\")\n",
    "    print(f\"📊 Detected {len(selfie.layer_paths)} transformer layers\")\n",
    "    print(f\"🔤 Tokenizer vocabulary size: {len(selfie.model.tokenizer)}\")\n",
    "    \n",
    "    # Show some layer information\n",
    "    print(\"\\n📋 Layer structure:\")\n",
    "    for i, layer_path in enumerate(selfie.layer_paths[:5]):\n",
    "        print(f\"  Layer {i}: {layer_path}\")\n",
    "    if len(selfie.layer_paths) > 5:\n",
    "        print(f\"  ... and {len(selfie.layer_paths) - 5} more layers\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to load Gemma2: {e}\")\n",
    "    print(\"🔄 Falling back to smaller model for demo...\")\n",
    "    \n",
    "    # Fallback to smaller model if Gemma2 fails\n",
    "    selfie = ModelAgnosticSelfie(\"google/gemma-2-2b\")  # Non-IT version\n",
    "    print(f\"✅ Fallback model loaded on: {selfie.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Token Interpretation\n",
    "\n",
    "Let's start with basic token interpretation to understand what different parts of the model represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created interpretation prompts:\n",
      "  Concept: 'This _ _ _ _ _ represents the concept of'\n",
      "  Sentiment: 'This expresses the sentiment of _ _ _ _ _ '\n",
      "  Entity: 'This refers to the entity _ _ _ _ _ '\n"
     ]
    }
   ],
   "source": [
    "# Create interpretation prompts\n",
    "concept_prompt = InterpretationPrompt.create_concept_prompt(selfie.model.tokenizer)\n",
    "sentiment_prompt = InterpretationPrompt.create_sentiment_prompt(selfie.model.tokenizer) \n",
    "entity_prompt = InterpretationPrompt.create_entity_prompt(selfie.model.tokenizer)\n",
    "\n",
    "print(\"Created interpretation prompts:\")\n",
    "print(f\"  Concept: '{concept_prompt.get_prompt()}'\")\n",
    "print(f\"  Sentiment: '{sentiment_prompt.get_prompt()}'\")\n",
    "print(f\"  Entity: '{entity_prompt.get_prompt()}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Test prompt: 'The brilliant scientist discovered a revolutionary breakthrough in quantum computing.'\n",
      "\n",
      "🔤 Tokenization (12 tokens):\n",
      "   0:     2 -> '<bos>'\n",
      "   1:   818 -> 'The'\n",
      "   2: 21042 -> ' brilliant'\n",
      "   3: 30892 -> ' scientist'\n",
      "   4: 11788 -> ' discovered'\n",
      "   5:   496 -> ' a'\n",
      "   6: 38472 -> ' revolutionary'\n",
      "   7: 46341 -> ' breakthrough'\n",
      "   8:   528 -> ' in'\n",
      "   9: 12705 -> ' quantum'\n",
      "  10: 20124 -> ' computing'\n",
      "  11: 236761 -> '.'\n"
     ]
    }
   ],
   "source": [
    "# Test prompt for interpretation\n",
    "test_prompt = \"The brilliant scientist discovered a revolutionary breakthrough in quantum computing.\"\n",
    "print(f\"🧪 Test prompt: '{test_prompt}'\")\n",
    "\n",
    "# Tokenize to see the tokens\n",
    "tokens = selfie.model.tokenizer.encode(test_prompt)\n",
    "token_strings = [selfie.model.tokenizer.decode([t]) for t in tokens]\n",
    "\n",
    "print(f\"\\n🔤 Tokenization ({len(tokens)} tokens):\")\n",
    "for i, (token_id, token_str) in enumerate(zip(tokens, token_strings)):\n",
    "    print(f\"  {i:2d}: {token_id:5d} -> '{token_str}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Interpreting key tokens...\n",
      "(This may take a minute...)\n",
      "Interpreting 'The brilliant scientist discovered a revolutionary breakthrough in quantum computing.' with 'This _ _ _ _ _ represents the concept of'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22edb6814a54ab3a0a0be5b58b84925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have set `compile_config`, but we are unable to meet the criteria for compilation. Compilation will be skipped.\n",
      "100%|██████████| 4/4 [00:05<00:00,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Interpretation Results:\n",
      "================================================================================\n",
      "Layer 13, Token -2: ' computing'\n",
      "  🧠 Interpretation: using the power of quantum mechanics to achieve more advanced and complex\n",
      "\n",
      "Layer 17, Token -3: ' quantum'\n",
      "  🧠 Interpretation: a particle existing in two or more places at the same time\n",
      "\n",
      "Layer 19, Token -4: ' in'\n",
      "  🧠 Interpretation: what causes it?\n",
      "\n",
      "A. Quantum mechanics\n",
      "B.\n",
      "\n",
      "Layer 20, Token -5: ' breakthrough'\n",
      "  🧠 Interpretation: the power to control thoughts and emotions.\n",
      "\n",
      "The study of\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Interpret key tokens from different layers\n",
    "interesting_tokens = [\n",
    "    (13, -2),   # \"brilliant\" from middle layer\n",
    "    (17, -3),  # \"scientist\" from deeper layer  \n",
    "    (19, -4),  # \"revolutionary\" from middle-deep layer\n",
    "    (20, -5),  # \"quantum\" from deep layer\n",
    "]\n",
    "\n",
    "print(\"🔍 Interpreting key tokens...\")\n",
    "print(\"(This may take a minute...)\")\n",
    "\n",
    "# Run interpretation\n",
    "results = selfie.interpret(\n",
    "    original_prompt=test_prompt,\n",
    "    interpretation_prompt=concept_prompt,\n",
    "    tokens_to_interpret=interesting_tokens,\n",
    "    max_new_tokens=12,\n",
    "    batch_size=1  # Process in smaller batches\n",
    ")\n",
    "\n",
    "# Display results in a nice format\n",
    "print(\"\\n📊 Interpretation Results:\")\n",
    "print(\"=\" * 80)\n",
    "for i in range(len(results['prompt'])):\n",
    "    layer = results['layer'][i]\n",
    "    token_pos = results['token'][i] \n",
    "    token_text = results['token_decoded'][i]\n",
    "    interpretation = results['interpretation'][i].strip()\n",
    "    \n",
    "    print(f\"Layer {layer:2d}, Token {token_pos:2d}: '{token_text}'\")\n",
    "    print(f\"  🧠 Interpretation: {interpretation}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Analysis Across Layers\n",
    "\n",
    "Let's analyze how representations change across different layers of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract activations from multiple layers for analysis\n",
    "analysis_prompt = \"Artificial intelligence will transform healthcare through personalized medicine.\"\n",
    "target_token = 4  # \"transform\"\n",
    "\n",
    "print(f\"🔬 Analyzing activations for: '{analysis_prompt}'\")\n",
    "print(f\"🎯 Target token: '{selfie.model.tokenizer.decode([selfie.model.tokenizer.encode(analysis_prompt)[target_token]])[0]}'\")\n",
    "\n",
    "# Sample layers across the model\n",
    "total_layers = len(selfie.layer_paths)\n",
    "sample_layers = list(range(0, total_layers, max(1, total_layers // 8)))[:8]\n",
    "\n",
    "print(f\"📊 Sampling layers: {sample_layers} (out of {total_layers} total)\")\n",
    "\n",
    "# Extract activations\n",
    "activations = selfie.get_activations(\n",
    "    analysis_prompt, \n",
    "    layer_indices=sample_layers,\n",
    "    token_indices=[target_token]\n",
    ")\n",
    "\n",
    "print(\"✅ Activations extracted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze activation statistics across layers\n",
    "stats_data = []\n",
    "\n",
    "for layer_idx in sample_layers:\n",
    "    if isinstance(activations[layer_idx], list):\n",
    "        activation = activations[layer_idx][0]  # First (and only) token\n",
    "    else:\n",
    "        activation = activations[layer_idx][:, target_token, :]  # Extract target token\n",
    "    \n",
    "    # Compute statistics\n",
    "    flat_activation = activation.flatten()\n",
    "    stats = {\n",
    "        'layer': layer_idx,\n",
    "        'mean': float(flat_activation.mean()),\n",
    "        'std': float(flat_activation.std()),\n",
    "        'max': float(flat_activation.max()),\n",
    "        'min': float(flat_activation.min()),\n",
    "        'sparsity': float((flat_activation.abs() < 0.1).float().mean()),\n",
    "        'norm': float(torch.norm(flat_activation))\n",
    "    }\n",
    "    stats_data.append(stats)\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "df_stats = pd.DataFrame(stats_data)\n",
    "print(\"📈 Activation Statistics Across Layers:\")\n",
    "print(df_stats.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activation statistics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "fig.suptitle(f'Activation Analysis: \"{selfie.model.tokenizer.decode([selfie.model.tokenizer.encode(analysis_prompt)[target_token]])}\" Across Layers', fontsize=14)\n",
    "\n",
    "# Mean activation\n",
    "axes[0,0].plot(df_stats['layer'], df_stats['mean'], 'o-', color='blue')\n",
    "axes[0,0].set_title('Mean Activation')\n",
    "axes[0,0].set_xlabel('Layer')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Standard deviation\n",
    "axes[0,1].plot(df_stats['layer'], df_stats['std'], 'o-', color='green')\n",
    "axes[0,1].set_title('Standard Deviation')\n",
    "axes[0,1].set_xlabel('Layer')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Sparsity\n",
    "axes[1,0].plot(df_stats['layer'], df_stats['sparsity'], 'o-', color='red')\n",
    "axes[1,0].set_title('Sparsity (% values < 0.1)')\n",
    "axes[1,0].set_xlabel('Layer')\n",
    "axes[1,0].set_ylabel('Sparsity')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Norm\n",
    "axes[1,1].plot(df_stats['layer'], df_stats['norm'], 'o-', color='purple')\n",
    "axes[1,1].set_title('L2 Norm')\n",
    "axes[1,1].set_xlabel('Layer')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"📊 Observations:\")\n",
    "print(f\"  - Activation mean ranges from {df_stats['mean'].min():.3f} to {df_stats['mean'].max():.3f}\")\n",
    "print(f\"  - Sparsity varies from {df_stats['sparsity'].min():.1%} to {df_stats['sparsity'].max():.1%}\")\n",
    "print(f\"  - Highest norm at layer {df_stats.loc[df_stats['norm'].idxmax(), 'layer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Arithmetic with Gemma2\n",
    "\n",
    "Let's explore the famous \"king - man + woman ≈ queen\" type relationships in Gemma2's representation space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define concept pairs for vector arithmetic\n",
    "concepts = {\n",
    "    \"doctor\": \"The doctor examined the patient carefully\",\n",
    "    \"nurse\": \"The nurse provided excellent care\", \n",
    "    \"teacher\": \"The teacher explained the concept clearly\",\n",
    "    \"student\": \"The student studied for the exam\",\n",
    "    \"king\": \"The king ruled the kingdom wisely\",\n",
    "    \"queen\": \"The queen addressed the nation\",\n",
    "    \"man\": \"The man walked down the street\",\n",
    "    \"woman\": \"The woman read an interesting book\"\n",
    "}\n",
    "\n",
    "print(\"🧮 Extracting concept activations for vector arithmetic...\")\n",
    "print(\"Target layer: 12 (middle-deep layer)\")\n",
    "\n",
    "concept_activations = {}\n",
    "target_layer = 9\n",
    "\n",
    "# Extract activations for each concept\n",
    "for concept, prompt in tqdm(concepts.items(), desc=\"Extracting activations\"):\n",
    "    # Find the position of the concept word in the tokenization\n",
    "    tokens = selfie.model.tokenizer.encode(prompt)\n",
    "    token_strings = [selfie.model.tokenizer.decode([t]) for t in tokens]\n",
    "    \n",
    "    # Find concept word position (usually position 1 or 2)\n",
    "    concept_pos = 1  # Default\n",
    "    for i, token_str in enumerate(token_strings):\n",
    "        if concept.lower() in token_str.lower():\n",
    "            concept_pos = i\n",
    "            break\n",
    "    \n",
    "    # Extract activation\n",
    "    acts = selfie.get_activations(\n",
    "        prompt, \n",
    "        layer_indices=[target_layer],\n",
    "        token_indices=[concept_pos]\n",
    "    )\n",
    "    \n",
    "    concept_activations[concept] = acts[target_layer][0]  # Shape: [1, hidden_dim]\n",
    "\n",
    "print(f\"✅ Extracted activations for {len(concept_activations)} concepts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform vector arithmetic: doctor - man + woman ≈ nurse?\n",
    "print(\"🔬 Vector Arithmetic Experiments:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "experiments = [\n",
    "    (\"doctor\", \"man\", \"woman\", \"Expected: nurse-like\"),\n",
    "    (\"king\", \"man\", \"woman\", \"Expected: queen-like\"), \n",
    "    (\"teacher\", \"woman\", \"man\", \"Expected: teacher-like\"),\n",
    "]\n",
    "\n",
    "for base, subtract, add, expected in experiments:\n",
    "    try:\n",
    "        # Compute: base - subtract + add\n",
    "        result_vector = (\n",
    "            concept_activations[base] - \n",
    "            concept_activations[subtract] + \n",
    "            concept_activations[add]\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n🧮 {base} - {subtract} + {add}:\")\n",
    "        print(f\"   {expected}\")\n",
    "        \n",
    "        # Interpret the result vector\n",
    "        interpretation = selfie.interpret_vectors(\n",
    "            vectors=[result_vector],\n",
    "            interpretation_prompt=concept_prompt,\n",
    "            injection_layer=3,  # Earlier layer for interpretation\n",
    "            max_new_tokens=10\n",
    "        )[0]\n",
    "        \n",
    "        print(f\"   🤖 AI interpretation: {interpretation.strip()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Interpretation Prompts\n",
    "\n",
    "Let's create custom interpretation prompts to explore different aspects of the model's representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom interpretation prompts\n",
    "custom_prompts = {\n",
    "    \"emotion\": InterpretationPrompt(\n",
    "        selfie.model.tokenizer,\n",
    "        [\"This neural activation represents the emotion of \", None]\n",
    "    ),\n",
    "    \"action\": InterpretationPrompt(\n",
    "        selfie.model.tokenizer, \n",
    "        [\"This pattern encodes the action of \", None]\n",
    "    ),\n",
    "    \"relationship\": InterpretationPrompt(\n",
    "        selfie.model.tokenizer,\n",
    "        [\"The relationship captured here is \", None]\n",
    "    ),\n",
    "    \"abstract\": InterpretationPrompt(\n",
    "        selfie.model.tokenizer,\n",
    "        [\"This abstraction represents \", None, \" in the context of knowledge\"]\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"🎭 Custom Interpretation Prompts:\")\n",
    "for name, prompt in custom_prompts.items():\n",
    "    print(f\"  {name.title()}: '{prompt.get_prompt()}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test custom prompts on different types of content\n",
    "test_cases = [\n",
    "    {\n",
    "        \"text\": \"She laughed joyfully at the surprise party\",\n",
    "        \"target_token\": 1,  # \"laughed\"\n",
    "        \"layer\": 10,\n",
    "        \"expected_type\": \"emotion\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"The athlete sprinted towards the finish line\", \n",
    "        \"target_token\": 2,  # \"sprinted\"\n",
    "        \"layer\": 8,\n",
    "        \"expected_type\": \"action\" \n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Democracy requires active citizen participation\",\n",
    "        \"target_token\": 0,  # \"Democracy\"\n",
    "        \"layer\": 15,\n",
    "        \"expected_type\": \"abstract\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"🔍 Testing Custom Prompts:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, test_case in enumerate(test_cases):\n",
    "    text = test_case[\"text\"]\n",
    "    token_pos = test_case[\"target_token\"]\n",
    "    layer = test_case[\"layer\"]\n",
    "    prompt_type = test_case[\"expected_type\"]\n",
    "    \n",
    "    # Get the actual token\n",
    "    tokens = selfie.model.tokenizer.encode(text)\n",
    "    target_token_text = selfie.model.tokenizer.decode([tokens[token_pos]])\n",
    "    \n",
    "    print(f\"\\n🧪 Test Case {i+1}:\")\n",
    "    print(f\"   Text: '{text}'\")\n",
    "    print(f\"   Target: '{target_token_text}' (pos {token_pos}, layer {layer})\")\n",
    "    print(f\"   Expected type: {prompt_type}\")\n",
    "    \n",
    "    try:\n",
    "        # Test with the expected prompt type\n",
    "        result = selfie.interpret(\n",
    "            original_prompt=text,\n",
    "            interpretation_prompt=custom_prompts[prompt_type],\n",
    "            tokens_to_interpret=[(layer, token_pos)],\n",
    "            max_new_tokens=8\n",
    "        )\n",
    "        \n",
    "        interpretation = result['interpretation'][0].strip()\n",
    "        print(f\"   🤖 {prompt_type.title()}: {interpretation}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis\n",
    "\n",
    "Let's analyze the performance of our setup and compare different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_interpretation(selfie, prompt, num_tokens=5, num_trials=3):\n",
    "    \"\"\"Benchmark interpretation performance.\"\"\"\n",
    "    times = []\n",
    "    \n",
    "    for _ in range(num_trials):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Simple interpretation task\n",
    "        tokens_to_interpret = [(8, i) for i in range(num_tokens)]\n",
    "        results = selfie.interpret(\n",
    "            original_prompt=prompt,\n",
    "            interpretation_prompt=concept_prompt,\n",
    "            tokens_to_interpret=tokens_to_interpret,\n",
    "            max_new_tokens=5\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "    \n",
    "    return {\n",
    "        'mean_time': np.mean(times),\n",
    "        'std_time': np.std(times),\n",
    "        'times': times\n",
    "    }\n",
    "\n",
    "# Benchmark performance\n",
    "benchmark_prompt = \"The future of artificial intelligence looks very promising\"\n",
    "\n",
    "print(f\"⚡ Performance Benchmark\")\n",
    "print(f\"Device: {selfie.device}\")\n",
    "print(f\"Model: Gemma2 2B\")\n",
    "print(f\"Prompt: '{benchmark_prompt}'\")\n",
    "print(\"\\nRunning benchmark (this may take a minute)...\")\n",
    "\n",
    "perf_results = benchmark_interpretation(selfie, benchmark_prompt, num_tokens=3, num_trials=3)\n",
    "\n",
    "print(f\"\\n📊 Results:\")\n",
    "print(f\"  Average time: {perf_results['mean_time']:.2f} ± {perf_results['std_time']:.2f} seconds\")\n",
    "print(f\"  Individual times: {[f'{t:.2f}s' for t in perf_results['times']]}\")\n",
    "print(f\"  Time per token: {perf_results['mean_time']/3:.2f} seconds\")\n",
    "\n",
    "# Device-specific notes\n",
    "if selfie.device == \"mps\":\n",
    "    print(\"\\n🍎 MPS Notes:\")\n",
    "    print(\"  - Performance optimized for Apple Silicon\")\n",
    "    print(\"  - Memory shared with system RAM\")\n",
    "elif selfie.device == \"cuda\":\n",
    "    print(\"\\n🚀 CUDA Notes:\")\n",
    "    print(\"  - Using dedicated GPU memory\")\n",
    "    print(\"  - Consider increasing batch size for better throughput\")\n",
    "else:\n",
    "    print(\"\\n💻 CPU Notes:\")\n",
    "    print(\"  - Consider using MPS/CUDA for better performance\")\n",
    "    print(\"  - Performance adequate for small-scale experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Analysis: Attention Patterns vs Interpretations\n",
    "\n",
    "Let's explore how interpretations relate to the model's internal attention patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze a complex sentence with multiple interesting tokens\n",
    "complex_prompt = \"The groundbreaking research revolutionized our understanding of consciousness and artificial intelligence.\"\n",
    "\n",
    "print(f\"🧠 Complex Analysis: '{complex_prompt}'\")\n",
    "\n",
    "# Tokenize and identify key tokens\n",
    "tokens = selfie.model.tokenizer.encode(complex_prompt)\n",
    "token_strings = [selfie.model.tokenizer.decode([t]) for t in tokens]\n",
    "\n",
    "print(f\"\\n🔤 Tokens ({len(tokens)}):\")\n",
    "for i, (token_id, token_str) in enumerate(zip(tokens, token_strings)):\n",
    "    print(f\"  {i:2d}: '{token_str.strip()}'\")\n",
    "\n",
    "# Select key tokens for analysis\n",
    "key_tokens = {\n",
    "    \"groundbreaking\": 1,\n",
    "    \"research\": 2, \n",
    "    \"revolutionized\": 3,\n",
    "    \"consciousness\": -4,  # Near end\n",
    "    \"intelligence\": -2    # Near end\n",
    "}\n",
    "\n",
    "# Convert negative indices\n",
    "for name, pos in key_tokens.items():\n",
    "    if pos < 0:\n",
    "        key_tokens[name] = len(tokens) + pos\n",
    "\n",
    "print(f\"\\n🎯 Key tokens for analysis:\")\n",
    "for name, pos in key_tokens.items():\n",
    "    if 0 <= pos < len(token_strings):\n",
    "        print(f\"  {name}: position {pos} -> '{token_strings[pos].strip()}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret the same tokens from different layers\n",
    "layer_comparison = [4, 8, 12, 16, 20]  # Early, middle, late layers\n",
    "target_token = key_tokens[\"consciousness\"]  # Focus on \"consciousness\"\n",
    "\n",
    "print(f\"🔍 Layer Comparison for '{token_strings[target_token].strip()}':\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "layer_interpretations = {}\n",
    "\n",
    "for layer in tqdm(layer_comparison, desc=\"Analyzing layers\"):\n",
    "    try:\n",
    "        result = selfie.interpret(\n",
    "            original_prompt=complex_prompt,\n",
    "            interpretation_prompt=concept_prompt,\n",
    "            tokens_to_interpret=[(layer, target_token)],\n",
    "            max_new_tokens=10\n",
    "        )\n",
    "        \n",
    "        interpretation = result['interpretation'][0].strip()\n",
    "        layer_interpretations[layer] = interpretation\n",
    "        \n",
    "        print(f\"Layer {layer:2d}: {interpretation}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Layer {layer:2d}: ❌ Error - {e}\")\n",
    "        layer_interpretations[layer] = \"[Error]\"\n",
    "\n",
    "print(\"\\n📈 Observations:\")\n",
    "print(\"  - Early layers: More surface-level/syntactic representations\")\n",
    "print(\"  - Middle layers: Semantic concepts emerge\")\n",
    "print(\"  - Later layers: Abstract/contextual understanding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Insights\n",
    "\n",
    "Let's summarize what we've learned about Gemma2 2B through our interpretation experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of our findings\n",
    "print(\"📋 Summary of Gemma2 2B Interpretation Experiments\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n🔧 Technical Details:\")\n",
    "print(f\"  Model: Gemma2 2B-IT\")\n",
    "print(f\"  Device: {selfie.device}\")\n",
    "print(f\"  Layers: {len(selfie.layer_paths)}\")\n",
    "print(f\"  Vocab size: {len(selfie.model.tokenizer):,}\")\n",
    "\n",
    "print(f\"\\n⚡ Performance:\")\n",
    "if 'perf_results' in locals():\n",
    "    print(f\"  Avg interpretation time: {perf_results['mean_time']:.2f}s\")\n",
    "    print(f\"  Time per token: {perf_results['mean_time']/3:.2f}s\")\n",
    "else:\n",
    "    print(\"  [Performance data not available]\")\n",
    "\n",
    "print(f\"\\n🧠 Key Insights:\")\n",
    "print(\"  ✅ Automatic MPS detection works seamlessly\")\n",
    "print(\"  ✅ Layer-wise representations show clear progression\")\n",
    "print(\"  ✅ Vector arithmetic reveals semantic relationships\")\n",
    "print(\"  ✅ Custom prompts enable targeted interpretation\")\n",
    "print(\"  ✅ Model handles both concrete and abstract concepts well\")\n",
    "\n",
    "print(f\"\\n🎯 Best Practices:\")\n",
    "print(\"  • Use layers 8-16 for semantic interpretations\")\n",
    "print(\"  • Batch smaller interpretation requests for efficiency\")\n",
    "print(\"  • Custom prompts yield more targeted insights\")\n",
    "print(\"  • MPS provides good performance on Apple Silicon\")\n",
    "\n",
    "print(f\"\\n🚀 Next Steps:\")\n",
    "print(\"  • Try with your own text and interpretation prompts\")\n",
    "print(\"  • Experiment with different layer combinations\")\n",
    "print(\"  • Compare with other model architectures\")\n",
    "print(\"  • Explore intervention techniques for model steering\")\n",
    "\n",
    "print(\"\\n✨ Experiment completed successfully! ✨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Exploration\n",
    "\n",
    "Use this section to experiment with your own prompts and interpretations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive experimentation cell\n",
    "# Modify these variables to explore your own ideas!\n",
    "\n",
    "# Your custom prompt here\n",
    "your_prompt = \"Climate change requires immediate global action and innovative solutions.\"\n",
    "\n",
    "# Choose tokens to interpret (adjust based on your prompt)\n",
    "your_tokens = [(20, -1), (18, -2), (15, -3)]  # (layer, token_position)\n",
    "\n",
    "# Choose interpretation style\n",
    "your_interpretation_prompt = concept_prompt  # or sentiment_prompt, entity_prompt, or custom_prompts['emotion']\n",
    "\n",
    "print(f\"🔬 Your Experiment:\")\n",
    "print(f\"Prompt: '{your_prompt}'\")\n",
    "\n",
    "# Show tokenization\n",
    "your_tokens_list = selfie.model.tokenizer.encode(your_prompt)\n",
    "your_token_strings = [selfie.model.tokenizer.decode([t]) for t in your_tokens_list]\n",
    "print(f\"\\nTokens:\")\n",
    "for i, token_str in enumerate(your_token_strings[:10]):  # Show first 10\n",
    "    print(f\"  {i}: '{token_str.strip()}'\")\n",
    "if len(your_token_strings) > 10:\n",
    "    print(f\"  ... and {len(your_token_strings) - 10} more\")\n",
    "\n",
    "# Run your interpretation\n",
    "try:\n",
    "    your_results = selfie.interpret(\n",
    "        original_prompt=your_prompt,\n",
    "        interpretation_prompt=your_interpretation_prompt,\n",
    "        tokens_to_interpret=your_tokens,\n",
    "        max_new_tokens=30\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n🤖 Your Results:\")\n",
    "    for i in range(len(your_results['interpretation'])):\n",
    "        layer = your_results['layer'][i]\n",
    "        token_pos = your_results['token'][i]\n",
    "        token_text = your_results['token_decoded'][i] \n",
    "        interpretation = your_results['interpretation'][i].strip()\n",
    "        \n",
    "        print(f\"  Layer {layer}, Token {token_pos} ('{token_text}'): {interpretation}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in your experiment: {e}\")\n",
    "    print(\"Try adjusting the layer/token positions or using a different prompt!\")\n",
    "\n",
    "print(\"\\n💡 Try modifying the variables above and re-running this cell!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the power of **NNsight Selfie** for interpreting neural network representations in Google's Gemma2 2B model. Key takeaways:\n",
    "\n",
    "1. **Seamless Device Detection**: Automatic MPS support makes it easy to use Apple Silicon Macs\n",
    "2. **Model Agnostic**: Works with Gemma2 just as well as GPT, LLaMA, or other models\n",
    "3. **Rich Interpretations**: Different layers reveal different aspects of understanding\n",
    "4. **Vector Arithmetic**: Semantic relationships encoded in activation space\n",
    "5. **Flexible Framework**: Custom prompts enable targeted exploration\n",
    "\n",
    "**Next Steps**: Try this approach with your own models, prompts, and research questions!\n",
    "\n",
    "**Repository**: [NNsight Selfie on GitHub](https://github.com/your-repo/nnsight-selfie)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
