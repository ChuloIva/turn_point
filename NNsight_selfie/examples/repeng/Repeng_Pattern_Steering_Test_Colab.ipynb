{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "setup"
   },
   "source": [
    "# RePENG Pattern Steering Test - Google Colab Version\n",
    "\n",
    "This notebook clones the repository and runs RePENG pattern steering tests in Google Colab.\n",
    "\n",
    "## Setup: Clone Repository and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Check if already cloned\n",
    "if not os.path.exists('/content/turn_point'):\n",
    "    print('Cloning repository...')\n",
    "    !git clone https://github.com/ChuloIva/turn_point.git /content/turn_point\n",
    "    print('Repository cloned successfully!')\n",
    "else:\n",
    "    print('Repository already exists, pulling latest changes...')\n",
    "    !cd /content/turn_point && git pull\n",
    "\n",
    "# Change to the project directory\n",
    "os.chdir('/content/turn_point')\n",
    "print(f'Current directory: {os.getcwd()}')\n",
    "\n",
    "# List directory contents to verify\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print('Installing dependencies...')\n",
    "\n",
    "# Install nnsight and other dependencies\n",
    "!pip install nnsight torch transformers datasets accelerate\n",
    "\n",
    "# Install the local package in development mode\n",
    "!cd /content/turn_point/NNsight_selfie && pip install -e .\n",
    "\n",
    "print('Dependencies installed successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "main-code"
   },
   "source": [
    "## Main Code: RePENG Pattern Steering Test\n",
    "\n",
    "This section contains the adapted code from the original notebook, optimized for Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "imports-setup"
   },
   "outputs": [],
   "source": [
    "# Google Colab optimized imports and setup\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import nnsight\n",
    "\n",
    "# Device detection for Google Colab (usually CUDA)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    device_type = \"cuda\"\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    device_type = \"cpu\"\n",
    "    print(\"Using CPU (GPU not available)\")\n",
    "\n",
    "print(f\"Using device: {device} ({device_type})\")\n",
    "\n",
    "# Device-agnostic memory clearing function\n",
    "def clear_cache():\n",
    "    \"\"\"Clear device cache in a device-agnostic way\"\"\"\n",
    "    if device_type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    # CPU doesn't need explicit cache clearing\n",
    "\n",
    "# Set up paths for Colab\n",
    "REPO_ROOT = '/content/turn_point'\n",
    "os.chdir(REPO_ROOT)\n",
    "\n",
    "# Add to Python path\n",
    "if REPO_ROOT not in sys.path:\n",
    "    sys.path.insert(0, REPO_ROOT)\n",
    "\n",
    "# Import the local package\n",
    "try:\n",
    "    from nnsight_selfie import (\n",
    "        InterpretationPrompt,\n",
    "        compute_pattern_steering_vectors,\n",
    "        inject_with_interpretation_prompt,\n",
    "        list_patterns,\n",
    "    )\n",
    "    print('Successfully imported nnsight_selfie package')\n",
    "except ImportError as e:\n",
    "    print(f'Import error: {e}')\n",
    "    print('Trying alternative import method...')\n",
    "    \n",
    "    # Alternative import with explicit path\n",
    "    sys.path.insert(0, os.path.join(REPO_ROOT, 'NNsight_selfie'))\n",
    "    from nnsight_selfie import (\n",
    "        InterpretationPrompt,\n",
    "        compute_pattern_steering_vectors,\n",
    "        inject_with_interpretation_prompt,\n",
    "        list_patterns,\n",
    "    )\n",
    "    print('Successfully imported with alternative method')\n",
    "\n",
    "# Set model and patterns path\n",
    "MODEL_NAME = os.environ.get('MODEL_NAME', 'google/Gemma-3-4b-it')\n",
    "\n",
    "# Find patterns file\n",
    "PATTERNS_PATH = None\n",
    "pattern_candidates = [\n",
    "    os.path.join(REPO_ROOT, 'data/final/positive_patterns.jsonl'),\n",
    "    os.path.join(REPO_ROOT, 'NNsight_selfie/data/final/positive_patterns.jsonl'),\n",
    "    os.path.join(REPO_ROOT, '../data/final/positive_patterns.jsonl'),\n",
    "]\n",
    "\n",
    "for p in pattern_candidates:\n",
    "    if os.path.exists(p):\n",
    "        PATTERNS_PATH = p\n",
    "        print(f'Found patterns file: {PATTERNS_PATH}')\n",
    "        break\n",
    "\n",
    "if not PATTERNS_PATH:\n",
    "    print('‚ùå Patterns file not found! Searching directory structure...')\n",
    "    !find /content/turn_point -name \"positive_patterns.jsonl\" -type f\n",
    "else:\n",
    "    print(f'‚úÖ Using patterns file: {PATTERNS_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "load-model"
   },
   "outputs": [],
   "source": [
    "# Load model (optimized for Colab)\n",
    "print('Loading model (bfloat16)...')\n",
    "print(f'Model: {MODEL_NAME}')\n",
    "\n",
    "# Use device_map='auto' for efficient GPU usage in Colab\n",
    "model = nnsight.LanguageModel(\n",
    "    MODEL_NAME,\n",
    "    device_map='auto',\n",
    "    dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,  # Important for Colab memory management\n",
    ")\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "# Apply Gemma 3 4B-it vision filter behavior\n",
    "if 'gemma' in MODEL_NAME.lower() and '3' in MODEL_NAME and '4b' in MODEL_NAME.lower() and 'it' in MODEL_NAME.lower():\n",
    "    try:\n",
    "        setattr(model, 'model_name', MODEL_NAME)\n",
    "        print('Gemma 3 4B-it detected; extractor will filter out vision components.')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print('‚úÖ Model loaded successfully!')\n",
    "\n",
    "# Check GPU memory usage\n",
    "if device_type == \"cuda\":\n",
    "    print(f'GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB')\n",
    "    print(f'GPU memory cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "quick-test"
   },
   "outputs": [],
   "source": [
    "# Quick test with minimal memory usage (Colab optimized)\n",
    "print('=== COLAB OPTIMIZED QUICK TEST ===')\n",
    "print('Running minimal test with 1 pattern and 1 example')\n",
    "\n",
    "# Use specific layers to minimize memory usage\n",
    "specific_layers = [15]  # Single layer for testing\n",
    "print(f'Extracting from layer: {specific_layers}')\n",
    "\n",
    "try:\n",
    "    # Import RePENG modules\n",
    "    from nnsight_selfie.repeng.repeng_activation_extractor import RepengActivationExtractor\n",
    "    from nnsight_selfie.repeng.repeng_steering_vectors import RepengSteeringVectorGenerator\n",
    "    from nnsight_selfie.repeng.patterns_dataset import build_all_datasets\n",
    "    \n",
    "    # Load patterns\n",
    "    patterns = list_patterns(PATTERNS_PATH)\n",
    "    print(f'Available patterns: {len(patterns)}')\n",
    "    \n",
    "    # Create extractor with minimal layers\n",
    "    extractor = RepengActivationExtractor(model, tokenizer, layer_indices=specific_layers)\n",
    "    \n",
    "    # Get minimal dataset (1 pattern, 1 example)\n",
    "    datasets = build_all_datasets(PATTERNS_PATH, ['pos-neg'], max_patterns=1)\n",
    "    pattern_name = list(datasets.keys())[0]\n",
    "    dataset = datasets[pattern_name]['pos-neg'][:1]  # Only first example\n",
    "    \n",
    "    print(f\"Processing 1 example from pattern: {pattern_name}\")\n",
    "    \n",
    "    # Extract activations\n",
    "    activations, inputs = extractor.extract_dataset_activations(\n",
    "        dataset, batch_size=1, show_progress=True\n",
    "    )\n",
    "    \n",
    "    # Generate steering vector\n",
    "    generator = RepengSteeringVectorGenerator(model_type=getattr(model, \"model_name\", \"unknown\"))\n",
    "    steering = generator.generate_steering_vectors(activations, method='pca_diff')\n",
    "    \n",
    "    print(f'‚úÖ SUCCESS: Generated steering vector for {len(steering.directions)} layers')\n",
    "    \n",
    "    # Store for testing\n",
    "    test_bundle = type('Bundle', (), {\n",
    "        'steering_vector': steering, \n",
    "        'pattern_name': pattern_name,\n",
    "        'layers': list(steering.directions.keys())\n",
    "    })()\n",
    "    \n",
    "    print(f'Ready for injection testing!')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'‚ùå ERROR: {e}')\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Clear memory after test\n",
    "clear_cache()\n",
    "if device_type == \"cuda\":\n",
    "    print(f'GPU memory after cleanup: {torch.cuda.memory_allocated() / 1024**3:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "test-injection"
   },
   "outputs": [],
   "source": [
    "# Test pattern injection (if quick test succeeded)\n",
    "if 'test_bundle' in locals():\n",
    "    print('=== TESTING PATTERN INJECTION ===')\n",
    "    \n",
    "    # Build interpretation prompt\n",
    "    interp = InterpretationPrompt.create_simple(\n",
    "        tokenizer, \n",
    "        prefix='This neural pattern represents ', \n",
    "        suffix=' in emotion'\n",
    "    )\n",
    "    prompt_text = interp.get_prompt()\n",
    "    \n",
    "    print(f'Prompt: {prompt_text}')\n",
    "    print(f'Pattern: {test_bundle.pattern_name}')\n",
    "    print(f'Layers: {test_bundle.layers}')\n",
    "    \n",
    "    # Test different injection strengths\n",
    "    strengths = [0.5, 1.0, 2.0]\n",
    "    \n",
    "    for strength in strengths:\n",
    "        try:\n",
    "            clear_cache()\n",
    "            \n",
    "            print(f'\\nTesting injection strength: {strength}')\n",
    "            \n",
    "            # Inject and generate\n",
    "            res = inject_with_interpretation_prompt(\n",
    "                model, tokenizer,\n",
    "                prompt_text=prompt_text,\n",
    "                steering_vector=test_bundle.steering_vector,\n",
    "                interpretation_prompt=interp,\n",
    "                injection_strength=strength,\n",
    "                max_new_tokens=30,\n",
    "                do_sample=False\n",
    "            )\n",
    "            \n",
    "            generated = res['generated_text']\n",
    "            print(f'Generated ({strength}x): \"{generated}\"')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Error at strength {strength}: {str(e)[:100]}...')\n",
    "    \n",
    "    print('\\n‚úÖ Injection testing complete!')\n",
    "    \n",
    "else:\n",
    "    print('‚ùå No test bundle available. Run the quick test cell first.')\n",
    "\n",
    "# Final cleanup\n",
    "clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "full-dataset-generation"
   },
   "outputs": [],
   "source": [
    "# Full dataset generation - ENABLED for Colab\n",
    "print('=== FULL DATASET GENERATION FOR COLAB ===')\n",
    "print('Generating steering vectors for ALL patterns with layers 18-30')\n",
    "\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration optimized for Colab\n",
    "target_layers = list(range(18, 31))  # Layers 18-30 (13 layers)\n",
    "patterns_per_batch = 1  # Process 1 pattern at a time for memory safety\n",
    "examples_per_pattern = 8  # Reduced for Colab memory limits\n",
    "total_patterns = len(list_patterns(PATTERNS_PATH))\n",
    "\n",
    "print(f'Target layers: {target_layers} ({len(target_layers)} layers)')\n",
    "print(f'Processing {patterns_per_batch} pattern per batch, {examples_per_pattern} examples each')\n",
    "print(f'Total patterns to process: {total_patterns}')\n",
    "\n",
    "# Create cache directory for Colab\n",
    "cache_dir = f\"colab_full_dataset_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "print(f'Created cache directory: {cache_dir}')\n",
    "\n",
    "# Import required modules\n",
    "from nnsight_selfie.repeng.repeng_activation_extractor import RepengActivationExtractor\n",
    "from nnsight_selfie.repeng.repeng_steering_vectors import RepengSteeringVectorGenerator\n",
    "from nnsight_selfie.repeng.patterns_dataset import build_all_datasets\n",
    "\n",
    "batch_files = []\n",
    "processed_patterns = []\n",
    "failed_patterns = []\n",
    "\n",
    "print(f'\\nüöÄ Starting full dataset generation for Colab...')\n",
    "print(f'Estimated time: ~{total_patterns * 3} minutes (3 min per pattern)')\n",
    "\n",
    "try:\n",
    "    for batch_start in range(0, total_patterns, patterns_per_batch):\n",
    "        batch_end = min(batch_start + patterns_per_batch, total_patterns)\n",
    "        batch_num = batch_start // patterns_per_batch + 1\n",
    "        \n",
    "        print(f'\\n--- Batch {batch_num}/{(total_patterns + patterns_per_batch - 1) // patterns_per_batch}: Pattern {batch_start+1}/{total_patterns} ---')\n",
    "        \n",
    "        # Clear memory before each batch (critical for Colab)\n",
    "        clear_cache()\n",
    "        \n",
    "        # Monitor GPU memory in Colab\n",
    "        if device_type == \"cuda\":\n",
    "            mem_before = torch.cuda.memory_allocated() / 1024**3\n",
    "            print(f'  GPU memory before batch: {mem_before:.2f} GB')\n",
    "        \n",
    "        # Get datasets for this batch (1 pattern at a time)\n",
    "        datasets = build_all_datasets(PATTERNS_PATH, ['pos-neg'], max_patterns=patterns_per_batch)\n",
    "        \n",
    "        if not datasets:\n",
    "            print(f'  ‚ö†Ô∏è No datasets found for batch {batch_num}')\n",
    "            continue\n",
    "        \n",
    "        # Create extractor for this batch\n",
    "        extractor = RepengActivationExtractor(model, tokenizer, layer_indices=target_layers)\n",
    "        generator = RepengSteeringVectorGenerator(model_type=getattr(model, \"model_name\", \"unknown\"))\n",
    "        \n",
    "        # Process the pattern in this batch\n",
    "        batch_bundles = []\n",
    "        for pattern_name, pair_map in datasets.items():\n",
    "            if 'pos-neg' in pair_map:\n",
    "                dataset = pair_map['pos-neg'][:examples_per_pattern]  # Limit examples for Colab\n",
    "                \n",
    "                print(f'  Processing: {pattern_name}')\n",
    "                print(f'    Examples: {len(dataset)} (limited to {examples_per_pattern})')\n",
    "                print(f'    Layers: {len(target_layers)} layers ({target_layers[0]}-{target_layers[-1]})')\n",
    "                \n",
    "                try:\n",
    "                    # Extract activations for this pattern\n",
    "                    activations, inputs = extractor.extract_dataset_activations(\n",
    "                        dataset, batch_size=1, show_progress=True\n",
    "                    )\n",
    "                    \n",
    "                    # Generate steering vector\n",
    "                    steering = generator.generate_steering_vectors(activations, method='pca_diff')\n",
    "                    \n",
    "                    # Create bundle\n",
    "                    bundle = {\n",
    "                        'steering_vector': steering, \n",
    "                        'pattern_name': pattern_name,\n",
    "                        'num_examples': len(dataset),\n",
    "                        'layers': list(steering.directions.keys()),\n",
    "                        'batch_num': batch_num,\n",
    "                        'target_layers': target_layers,\n",
    "                        'method': 'pca_diff'\n",
    "                    }\n",
    "                    \n",
    "                    batch_bundles.append(bundle)\n",
    "                    processed_patterns.append(pattern_name)\n",
    "                    \n",
    "                    print(f'    ‚úÖ SUCCESS: Generated steering vector ({len(steering.directions)} layers)')\n",
    "                    \n",
    "                    # Clear activations immediately after processing (critical for Colab)\n",
    "                    del activations, inputs, steering\n",
    "                    clear_cache()\n",
    "                    \n",
    "                    # Check memory usage after processing\n",
    "                    if device_type == \"cuda\":\n",
    "                        mem_after = torch.cuda.memory_allocated() / 1024**3\n",
    "                        print(f'    GPU memory after processing: {mem_after:.2f} GB')\n",
    "                        \n",
    "                        if mem_after > 12:  # Warning if using more than 12GB in Colab\n",
    "                            print(f'    ‚ö†Ô∏è HIGH MEMORY USAGE: {mem_after:.2f} GB - may crash soon!')\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f'    ‚ùå FAILED: {str(e)[:100]}...')\n",
    "                    failed_patterns.append((pattern_name, str(e)))\n",
    "                    continue\n",
    "        \n",
    "        # Save this batch to disk immediately (even if empty)\n",
    "        batch_file = os.path.join(cache_dir, f'batch_{batch_num:03d}.pkl')\n",
    "        batch_data = {\n",
    "            'batch_num': batch_num,\n",
    "            'bundles': batch_bundles,\n",
    "            'model_name': MODEL_NAME,\n",
    "            'target_layers': target_layers,\n",
    "            'examples_per_pattern': examples_per_pattern,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'processed_patterns': [b['pattern_name'] for b in batch_bundles]\n",
    "        }\n",
    "        \n",
    "        with open(batch_file, 'wb') as f:\n",
    "            pickle.dump(batch_data, f)\n",
    "        \n",
    "        batch_files.append(batch_file)\n",
    "        \n",
    "        if batch_bundles:\n",
    "            print(f'  üíæ Saved batch {batch_num} to disk ({len(batch_bundles)} patterns)')\n",
    "        else:\n",
    "            print(f'  üíæ Saved empty batch {batch_num} to disk')\n",
    "        \n",
    "        # Aggressively clear memory after each batch (critical for Colab)\n",
    "        del extractor, generator, datasets, batch_bundles, batch_data\n",
    "        clear_cache()\n",
    "        \n",
    "        # Progress update\n",
    "        progress_pct = (batch_num / ((total_patterns + patterns_per_batch - 1) // patterns_per_batch)) * 100\n",
    "        print(f'  üìä Progress: {progress_pct:.1f}% ({len(processed_patterns)} processed, {len(failed_patterns)} failed)')\n",
    "        \n",
    "        # Final memory check for Colab\n",
    "        if device_type == \"cuda\":\n",
    "            mem_final = torch.cuda.memory_allocated() / 1024**3\n",
    "            print(f'  üßπ GPU memory after cleanup: {mem_final:.2f} GB')\n",
    "    \n",
    "    # Create comprehensive index file\n",
    "    index_file = os.path.join(cache_dir, 'colab_dataset_index.pkl')\n",
    "    index_data = {\n",
    "        'batch_files': batch_files,\n",
    "        'total_patterns_attempted': total_patterns,\n",
    "        'successful_patterns': len(processed_patterns),\n",
    "        'failed_patterns': len(failed_patterns),\n",
    "        'target_layers': target_layers,\n",
    "        'examples_per_pattern': examples_per_pattern,\n",
    "        'model_name': MODEL_NAME,\n",
    "        'method': 'pca_diff',\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'cache_dir': cache_dir,\n",
    "        'processed_pattern_names': processed_patterns,\n",
    "        'failed_pattern_details': failed_patterns,\n",
    "        'colab_optimized': True\n",
    "    }\n",
    "    \n",
    "    with open(index_file, 'wb') as f:\n",
    "        pickle.dump(index_data, f)\n",
    "    \n",
    "    print(f'\\nüéâ FULL DATASET GENERATION COMPLETE!')\n",
    "    print(f'üìÇ Cache directory: {cache_dir}')\n",
    "    print(f'üìä Results:')\n",
    "    print(f'   - Successful: {len(processed_patterns)}/{total_patterns} patterns')\n",
    "    print(f'   - Failed: {len(failed_patterns)}/{total_patterns} patterns')\n",
    "    print(f'   - Layers: {len(target_layers)} layers (18-30)')\n",
    "    print(f'   - Examples per pattern: {examples_per_pattern}')\n",
    "    print(f'   - Batch files: {len(batch_files)}')\n",
    "    \n",
    "    # Calculate cache size\n",
    "    cache_files = [f for f in os.listdir(cache_dir) if f.endswith('.pkl')]\n",
    "    total_size = sum(os.path.getsize(os.path.join(cache_dir, f)) for f in cache_files)\n",
    "    print(f'   - Total cache size: {total_size / 1024 / 1024:.2f} MB')\n",
    "    \n",
    "    if failed_patterns:\n",
    "        print(f'\\n‚ö†Ô∏è Failed patterns:')\n",
    "        for pattern, error in failed_patterns[:5]:  # Show first 5 failures\n",
    "            print(f'   - {pattern}: {error[:80]}...')\n",
    "        if len(failed_patterns) > 5:\n",
    "            print(f'   ... and {len(failed_patterns) - 5} more failures')\n",
    "    \n",
    "    print(f'\\nüìã Index file: {index_file}')\n",
    "    print(f'üíæ Ready for analysis and testing!')\n",
    "    \n",
    "    # Store results in global variables for testing\n",
    "    colab_bundles = []\n",
    "    for batch_file in batch_files:\n",
    "        with open(batch_file, 'rb') as f:\n",
    "            batch_data = pickle.load(f)\n",
    "            for bundle_dict in batch_data['bundles']:\n",
    "                bundle = type('Bundle', (), bundle_dict)()\n",
    "                colab_bundles.append(bundle)\n",
    "    \n",
    "    bundles = colab_bundles  # Make available for testing cells\n",
    "    print(f'‚úÖ Loaded {len(bundles)} steering vectors for testing')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'\\nüí• CRITICAL ERROR: {e}')\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Save partial results\n",
    "    if processed_patterns:\n",
    "        partial_index = os.path.join(cache_dir, 'partial_colab_index.pkl')\n",
    "        with open(partial_index, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'partial_results': True,\n",
    "                'processed_patterns': processed_patterns,\n",
    "                'failed_patterns': failed_patterns,\n",
    "                'batch_files': batch_files,\n",
    "                'target_layers': target_layers,\n",
    "                'error': str(e),\n",
    "                'colab_optimized': True\n",
    "            }, f)\n",
    "        print(f'üíæ Saved partial results to: {partial_index}')\n",
    "\n",
    "# Final cleanup\n",
    "clear_cache()\n",
    "print(f'\\nüßπ Memory cleanup complete')\n",
    "\n",
    "# Colab-specific memory summary\n",
    "if device_type == \"cuda\":\n",
    "    print(f'üñ•Ô∏è  Final GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB')\n",
    "    print(f'üí° If you encounter memory issues, restart runtime and try with fewer examples_per_pattern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "summary"
   },
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a Google Colab-compatible version of the RePENG Pattern Steering Test.\n",
    "\n",
    "### What it does:\n",
    "1. **Clones the repository** from GitHub\n",
    "2. **Installs dependencies** automatically\n",
    "3. **Loads the model** with Colab-optimized settings\n",
    "4. **Runs quick tests** with minimal memory usage\n",
    "5. **Tests pattern injection** with different strengths\n",
    "6. **Optionally generates** full dataset (memory intensive)\n",
    "\n",
    "### Key Colab optimizations:\n",
    "- Automatic repository cloning and setup\n",
    "- Memory-efficient model loading\n",
    "- GPU memory monitoring\n",
    "- Reduced batch sizes for stability\n",
    "- Clear memory management\n",
    "\n",
    "### Usage tips:\n",
    "- Run cells in order\n",
    "- Start with quick test before full generation\n",
    "- Use Colab Pro for better memory limits\n",
    "- Monitor GPU memory usage"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "gpuType": "T4",
   "name": "Repeng_Pattern_Steering_Test_Colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
