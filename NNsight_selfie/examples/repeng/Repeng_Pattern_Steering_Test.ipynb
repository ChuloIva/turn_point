{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AMD GPU Environment Variables\n",
    "import os\n",
    "os.environ[\"HSA_OVERRIDE_GFX_VERSION\"] = \"11.0.0\"\n",
    "os.environ[\"HIP_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"AMD_SERIALIZE_KERNEL\"] = \"3\"\n",
    "os.environ[\"TORCH_USE_HIP_DSA\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RePENG Pattern Steering Test (NNsight)\n",
    "# This notebook computes PCA-diff steering vectors per cognitive pattern and injects them at InterpretationPrompt placeholder positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda (cuda)\n",
      "Loading model (bfloat16)...\n",
      "Gemma 3 4B-it detected; extractor will filter out vision components.\n",
      "Loaded\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import nnsight\n",
    "import torch\n",
    "\n",
    "# Device detection for cross-platform compatibility\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    device_type = \"cuda\"\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    device_type = \"mps\"\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    device_type = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device} ({device_type})\")\n",
    "\n",
    "# Device-agnostic memory clearing function\n",
    "def clear_cache():\n",
    "    \"\"\"Clear device cache in a device-agnostic way\"\"\"\n",
    "    if device_type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    elif device_type == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "    # CPU doesn't need explicit cache clearing\n",
    "\n",
    "# Robust import of local package without relying on __file__ (undefined in notebooks)\n",
    "try:\n",
    "    from nnsight_selfie import (\n",
    "        InterpretationPrompt,\n",
    "        compute_pattern_steering_vectors,\n",
    "        inject_with_interpretation_prompt,\n",
    "        list_patterns,\n",
    "    )\n",
    "except ModuleNotFoundError:\n",
    "    cwd = os.getcwd()\n",
    "    candidates = [\n",
    "        os.path.abspath(os.path.join(cwd, 'NNsight_selfie')),\n",
    "        os.path.abspath(os.path.join(cwd, '../NNsight_selfie')),\n",
    "        os.path.abspath(os.path.join(cwd, '..')),\n",
    "        os.path.abspath(os.path.join(cwd, '..', '..')),\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        pkgdir = os.path.join(c, 'nnsight_selfie')\n",
    "        if os.path.isdir(pkgdir):\n",
    "            if c not in sys.path:\n",
    "                sys.path.insert(0, c)\n",
    "            break\n",
    "    from nnsight_selfie import (\n",
    "        InterpretationPrompt,\n",
    "        compute_pattern_steering_vectors,\n",
    "        inject_with_interpretation_prompt,\n",
    "        list_patterns,\n",
    "    )\n",
    "\n",
    "MODEL_NAME = os.environ.get('MODEL_NAME', 'google/Gemma-3-4b-it')\n",
    "\n",
    "# Resolve patterns path robustly\n",
    "PATTERNS_PATH = os.environ.get('PATTERNS_PATH')\n",
    "if not PATTERNS_PATH:\n",
    "    cwd = os.getcwd()\n",
    "    pattern_candidates = [\n",
    "        os.path.join(cwd, 'data/final/positive_patterns.jsonl'),\n",
    "        os.path.join(cwd, '../data/final/positive_patterns.jsonl'),\n",
    "        os.path.join(cwd, '../../data/final/positive_patterns.jsonl'),\n",
    "    ]\n",
    "    for p in pattern_candidates:\n",
    "        if os.path.exists(p):\n",
    "            PATTERNS_PATH = p\n",
    "            break\n",
    "\n",
    "print('Loading model (bfloat16)...')\n",
    "model = nnsight.LanguageModel(\n",
    "    MODEL_NAME,\n",
    "    device_map='auto',\n",
    "    dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=False,\n",
    ")\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "# Apply Gemma 3 4B-it vision filter behavior used in ModelAgnosticSelfie by tagging model_name\n",
    "if 'gemma' in MODEL_NAME.lower() and '3' in MODEL_NAME and '4b' in MODEL_NAME.lower() and 'it' in MODEL_NAME.lower():\n",
    "    try:\n",
    "        setattr(model, 'model_name', MODEL_NAME)\n",
    "        print('Gemma 3 4B-it detected; extractor will filter out vision components.')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print('Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FULL DATASET GENERATION WITH CACHING ===\n",
      "Generating steering vectors for ALL patterns with layers 18-30\n",
      "Total patterns available: 13\n",
      "Target layers: [18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] (13 layers)\n",
      "Processing 1 pattern per batch, 10 examples each\n",
      "Total patterns to process: 13\n",
      "Created cache directory: full_dataset_cache_20250910_223246\n",
      "\n",
      "üöÄ Starting full dataset generation...\n",
      "Estimated time: ~26 minutes (2 min per pattern)\n",
      "\n",
      "--- Batch 1/13: Pattern 1/13 ---\n",
      "Limited to 1 patterns for memory optimization\n",
      "Filtered vision components. Using 35 layers.\n",
      "Initialized activation extractor for 13 layers\n",
      "  Processing: Executive Fatigue & Avolition\n",
      "    Examples: 10 (limited to 10)\n",
      "    Layers: 13 layers (18-30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting activations:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ee080f08494bd8b5759092c75966d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5502d0057f0347c68919e7f961a8355a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92d922690c346ae8a1e5326708fe40e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate FULL dataset with disk caching - layers 18-30\n",
    "print('=== FULL DATASET GENERATION WITH CACHING ===')\n",
    "print('Generating steering vectors for ALL patterns with layers 18-30')\n",
    "\n",
    "patterns = list_patterns(PATTERNS_PATH)\n",
    "print(f'Total patterns available: {len(patterns)}')\n",
    "\n",
    "# Clear GPU memory\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "clear_cache()\n",
    "\n",
    "# Configuration for FULL dataset generation\n",
    "target_layers = list(range(18, 31))  # Layers 18-30 (13 layers total)\n",
    "patterns_per_batch = 1  # Process 1 pattern at a time for memory safety\n",
    "examples_per_pattern = 10  # Use more examples per pattern for better vectors\n",
    "total_patterns = len(patterns)  # Process ALL patterns\n",
    "\n",
    "print(f'Target layers: {target_layers} ({len(target_layers)} layers)')\n",
    "print(f'Processing {patterns_per_batch} pattern per batch, {examples_per_pattern} examples each')\n",
    "print(f'Total patterns to process: {total_patterns}')\n",
    "\n",
    "# Create cache directory for full dataset\n",
    "cache_dir = f\"full_dataset_cache_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "print(f'Created cache directory: {cache_dir}')\n",
    "\n",
    "# Import required modules\n",
    "from nnsight_selfie.repeng.repeng_activation_extractor import RepengActivationExtractor\n",
    "from nnsight_selfie.repeng.repeng_steering_vectors import RepengSteeringVectorGenerator\n",
    "from nnsight_selfie.repeng.patterns_dataset import build_all_datasets\n",
    "\n",
    "batch_files = []\n",
    "processed_patterns = []\n",
    "failed_patterns = []\n",
    "\n",
    "print(f'\\nüöÄ Starting full dataset generation...')\n",
    "print(f'Estimated time: ~{total_patterns * 2} minutes (2 min per pattern)')\n",
    "\n",
    "try:\n",
    "    for batch_start in range(0, total_patterns, patterns_per_batch):\n",
    "        batch_end = min(batch_start + patterns_per_batch, total_patterns)\n",
    "        batch_num = batch_start // patterns_per_batch + 1\n",
    "        \n",
    "        print(f'\\n--- Batch {batch_num}/{(total_patterns + patterns_per_batch - 1) // patterns_per_batch}: Pattern {batch_start+1}/{total_patterns} ---')\n",
    "        \n",
    "        # Clear memory before each batch\n",
    "        clear_cache()\n",
    "        \n",
    "        # Get datasets for this batch (1 pattern at a time)\n",
    "        datasets = build_all_datasets(PATTERNS_PATH, ['pos-neg'], max_patterns=patterns_per_batch)\n",
    "        \n",
    "        if not datasets:\n",
    "            print(f'  ‚ö†Ô∏è No datasets found for batch {batch_num}')\n",
    "            continue\n",
    "            \n",
    "        # Create extractor for this batch\n",
    "        extractor = RepengActivationExtractor(model, tokenizer, layer_indices=target_layers)\n",
    "        generator = RepengSteeringVectorGenerator(model_type=getattr(model, \"model_name\", \"unknown\"))\n",
    "        \n",
    "        # Process the pattern in this batch\n",
    "        batch_bundles = []\n",
    "        for pattern_name, pair_map in datasets.items():\n",
    "            if 'pos-neg' in pair_map:\n",
    "                dataset = pair_map['pos-neg'][:examples_per_pattern]  # Limit examples\n",
    "                \n",
    "                print(f'  Processing: {pattern_name}')\n",
    "                print(f'    Examples: {len(dataset)} (limited to {examples_per_pattern})')\n",
    "                print(f'    Layers: {len(target_layers)} layers ({target_layers[0]}-{target_layers[-1]})')\n",
    "                \n",
    "                try:\n",
    "                    # Extract activations for this pattern\n",
    "                    activations, inputs = extractor.extract_dataset_activations(\n",
    "                        dataset, batch_size=1, show_progress=True\n",
    "                    )\n",
    "                    \n",
    "                    # Generate steering vector\n",
    "                    steering = generator.generate_steering_vectors(activations, method='pca_diff')\n",
    "                    \n",
    "                    # Create bundle\n",
    "                    bundle = {\n",
    "                        'steering_vector': steering, \n",
    "                        'pattern_name': pattern_name,\n",
    "                        'num_examples': len(dataset),\n",
    "                        'layers': list(steering.directions.keys()),\n",
    "                        'batch_num': batch_num,\n",
    "                        'target_layers': target_layers,\n",
    "                        'method': 'pca_diff'\n",
    "                    }\n",
    "                    \n",
    "                    batch_bundles.append(bundle)\n",
    "                    processed_patterns.append(pattern_name)\n",
    "                    \n",
    "                    print(f'    ‚úÖ SUCCESS: Generated steering vector ({len(steering.directions)} layers)')\n",
    "                    \n",
    "                    # Clear activations immediately after processing\n",
    "                    del activations, inputs, steering\n",
    "                    clear_cache()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f'    ‚ùå FAILED: {str(e)[:100]}...')\n",
    "                    failed_patterns.append((pattern_name, str(e)))\n",
    "                    continue\n",
    "        \n",
    "        # Save this batch to disk immediately (even if empty)\n",
    "        batch_file = os.path.join(cache_dir, f'batch_{batch_num:03d}.pkl')\n",
    "        batch_data = {\n",
    "            'batch_num': batch_num,\n",
    "            'bundles': batch_bundles,\n",
    "            'model_name': MODEL_NAME,\n",
    "            'target_layers': target_layers,\n",
    "            'examples_per_pattern': examples_per_pattern,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'processed_patterns': [b['pattern_name'] for b in batch_bundles]\n",
    "        }\n",
    "        \n",
    "        with open(batch_file, 'wb') as f:\n",
    "            pickle.dump(batch_data, f)\n",
    "        \n",
    "        batch_files.append(batch_file)\n",
    "        \n",
    "        if batch_bundles:\n",
    "            print(f'  üíæ Saved batch {batch_num} to disk ({len(batch_bundles)} patterns)')\n",
    "        else:\n",
    "            print(f'  üíæ Saved empty batch {batch_num} to disk')\n",
    "        \n",
    "        # Aggressively clear memory after each batch\n",
    "        del extractor, generator, datasets, batch_bundles, batch_data\n",
    "        clear_cache()\n",
    "        \n",
    "        # Progress update\n",
    "        progress_pct = (batch_num / ((total_patterns + patterns_per_batch - 1) // patterns_per_batch)) * 100\n",
    "        print(f'  üìä Progress: {progress_pct:.1f}% ({len(processed_patterns)} processed, {len(failed_patterns)} failed)')\n",
    "    \n",
    "    # Create comprehensive index file\n",
    "    index_file = os.path.join(cache_dir, 'full_dataset_index.pkl')\n",
    "    index_data = {\n",
    "        'batch_files': batch_files,\n",
    "        'total_patterns_attempted': total_patterns,\n",
    "        'successful_patterns': len(processed_patterns),\n",
    "        'failed_patterns': len(failed_patterns),\n",
    "        'target_layers': target_layers,\n",
    "        'examples_per_pattern': examples_per_pattern,\n",
    "        'model_name': MODEL_NAME,\n",
    "        'method': 'pca_diff',\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'cache_dir': cache_dir,\n",
    "        'processed_pattern_names': processed_patterns,\n",
    "        'failed_pattern_details': failed_patterns\n",
    "    }\n",
    "    \n",
    "    with open(index_file, 'wb') as f:\n",
    "        pickle.dump(index_data, f)\n",
    "    \n",
    "    print(f'\\nüéâ FULL DATASET GENERATION COMPLETE!')\n",
    "    print(f'üìÇ Cache directory: {cache_dir}')\n",
    "    print(f'üìä Results:')\n",
    "    print(f'   - Successful: {len(processed_patterns)}/{total_patterns} patterns')\n",
    "    print(f'   - Failed: {len(failed_patterns)}/{total_patterns} patterns')\n",
    "    print(f'   - Layers: {len(target_layers)} layers (18-30)')\n",
    "    print(f'   - Examples per pattern: {examples_per_pattern}')\n",
    "    print(f'   - Batch files: {len(batch_files)}')\n",
    "    \n",
    "    # Calculate cache size\n",
    "    cache_files = [f for f in os.listdir(cache_dir) if f.endswith('.pkl')]\n",
    "    total_size = sum(os.path.getsize(os.path.join(cache_dir, f)) for f in cache_files)\n",
    "    print(f'   - Total cache size: {total_size / 1024 / 1024:.2f} MB')\n",
    "    \n",
    "    if failed_patterns:\n",
    "        print(f'\\n‚ö†Ô∏è Failed patterns:')\n",
    "        for pattern, error in failed_patterns[:5]:  # Show first 5 failures\n",
    "            print(f'   - {pattern}: {error[:80]}...')\n",
    "        if len(failed_patterns) > 5:\n",
    "            print(f'   ... and {len(failed_patterns) - 5} more failures')\n",
    "    \n",
    "    print(f'\\nüìã Index file: {index_file}')\n",
    "    print(f'üíæ Ready for analysis and testing!')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'\\nüí• CRITICAL ERROR: {e}')\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Save partial results\n",
    "    if processed_patterns:\n",
    "        partial_index = os.path.join(cache_dir, 'partial_index.pkl')\n",
    "        with open(partial_index, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'partial_results': True,\n",
    "                'processed_patterns': processed_patterns,\n",
    "                'failed_patterns': failed_patterns,\n",
    "                'batch_files': batch_files,\n",
    "                'target_layers': target_layers,\n",
    "                'error': str(e)\n",
    "            }, f)\n",
    "        print(f'üíæ Saved partial results to: {partial_index}')\n",
    "\n",
    "# Final cleanup\n",
    "clear_cache()\n",
    "print(f'\\nüßπ Memory cleanup complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cached steering vectors and test injection\n",
    "print('Loading cached steering vectors...')\n",
    "\n",
    "# Find the most recent cache directory\n",
    "import glob\n",
    "cache_dirs = glob.glob('steering_cache_*')\n",
    "if not cache_dirs:\n",
    "    print('‚ùå No cache directories found. Run cell 4 first.')\n",
    "else:\n",
    "    # Use most recent cache directory\n",
    "    cache_dir = sorted(cache_dirs)[-1]\n",
    "    print(f'Using cache directory: {cache_dir}')\n",
    "    \n",
    "    # Load index file\n",
    "    index_file = os.path.join(cache_dir, 'index.pkl')\n",
    "    with open(index_file, 'rb') as f:\n",
    "        index_data = pickle.load(f)\n",
    "    \n",
    "    print(f'Found {len(index_data[\"batch_files\"])} batch files')\n",
    "    print(f'Total patterns: {index_data[\"total_patterns\"]}')\n",
    "    print(f'Target layers: {index_data[\"target_layers\"]}')\n",
    "    \n",
    "    # Load all bundles from cache (memory efficient - load one batch at a time for testing)\n",
    "    all_bundles = []\n",
    "    for batch_file in index_data['batch_files']:\n",
    "        with open(batch_file, 'rb') as f:\n",
    "            batch_data = pickle.load(f)\n",
    "            # Convert back to object-like format for compatibility\n",
    "            for bundle_dict in batch_data['bundles']:\n",
    "                bundle = type('Bundle', (), bundle_dict)()\n",
    "                all_bundles.append(bundle)\n",
    "    \n",
    "    bundles = all_bundles\n",
    "    print(f'‚úÖ Loaded {len(bundles)} steering vectors from cache')\n",
    "    \n",
    "    # Build an interpretation prompt with placeholders\n",
    "    interp = InterpretationPrompt.create_simple(tokenizer, prefix='This neural pattern represents ', suffix=' in emotion')\n",
    "    prompt_text = interp.get_prompt()\n",
    "    print(f'\\nPrompt: {prompt_text}')\n",
    "    print(f'Insert positions: {interp.get_insert_locations()[:10]}')\n",
    "    \n",
    "    # Test with first few patterns (load from cache as needed)\n",
    "    test_results = []\n",
    "    max_tests = min(3, len(bundles))  # Test up to 3 patterns\n",
    "    \n",
    "    for i in range(max_tests):\n",
    "        test_bundle = bundles[i]\n",
    "        \n",
    "        print(f'\\n--- Testing Pattern {i+1}: {test_bundle.pattern_name} ---')\n",
    "        print(f'Layers: {test_bundle.layers}')\n",
    "        print(f'Training examples: {test_bundle.num_examples}')\n",
    "        print(f'From batch: {test_bundle.batch_num}')\n",
    "        \n",
    "        try:\n",
    "            # Clear memory before each test\n",
    "            clear_cache()\n",
    "            \n",
    "            # Inject and generate\n",
    "            res = inject_with_interpretation_prompt(\n",
    "                model, tokenizer,\n",
    "                prompt_text=prompt_text,\n",
    "                steering_vector=test_bundle.steering_vector,\n",
    "                interpretation_prompt=interp,\n",
    "                injection_strength=1.0,\n",
    "                max_new_tokens=30,\n",
    "                do_sample=False\n",
    "            )\n",
    "            \n",
    "            generated = res['generated_text'][:200]  # Truncate for display\n",
    "            test_results.append({\n",
    "                'pattern': test_bundle.pattern_name,\n",
    "                'generated': generated,\n",
    "                'success': True,\n",
    "                'batch': test_bundle.batch_num\n",
    "            })\n",
    "            \n",
    "            print(f'Generated: \"{generated}\"')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Error with {test_bundle.pattern_name}: {e}')\n",
    "            test_results.append({\n",
    "                'pattern': test_bundle.pattern_name,\n",
    "                'generated': f'Error: {str(e)}',\n",
    "                'success': False,\n",
    "                'batch': test_bundle.batch_num\n",
    "            })\n",
    "    \n",
    "    print(f'\\n=== SUMMARY ===')\n",
    "    print(f'Cache directory: {cache_dir}')\n",
    "    print(f'Total patterns in cache: {len(bundles)}')\n",
    "    print(f'Patterns tested: {len(test_results)}')\n",
    "    successful_tests = sum(1 for r in test_results if r['success'])\n",
    "    print(f'Successful injections: {successful_tests}/{len(test_results)}')\n",
    "    \n",
    "    print('\\nResults:')\n",
    "    for i, result in enumerate(test_results):\n",
    "        status = '‚úì' if result['success'] else '‚úó'\n",
    "        batch_info = f\" (batch {result['batch']})\" if 'batch' in result else \"\"\n",
    "        print(f'{status} {result[\"pattern\"]}{batch_info}: {result[\"generated\"][:100]}...')\n",
    "    \n",
    "    # Show available patterns for further testing\n",
    "    print(f'\\nAvailable patterns for testing:')\n",
    "    for i, bundle in enumerate(bundles[:10]):  # Show first 10\n",
    "        print(f'  {i+1}. {bundle.pattern_name} (batch {bundle.batch_num}, {bundle.num_examples} examples)')\n",
    "    \n",
    "    if len(bundles) > 10:\n",
    "        print(f'  ... and {len(bundles) - 10} more patterns')\n",
    "    \n",
    "    print(f'\\nMemory usage: Only loaded steering vectors (no activations)')\n",
    "    clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced testing: Compare patterns and analyze steering effects\n",
    "print('=== ADVANCED PATTERN ANALYSIS ===\\n')\n",
    "\n",
    "if 'bundles' in locals() and len(bundles) > 0:\n",
    "    # Test multiple patterns with different injection strengths\n",
    "    print('Testing different injection strengths...')\n",
    "    \n",
    "    test_patterns = bundles[:3]  # Use first 3 patterns\n",
    "    strengths = [0.5, 1.0, 2.0]  # Different injection strengths\n",
    "    \n",
    "    results_matrix = {}\n",
    "    \n",
    "    for pattern_idx, test_bundle in enumerate(test_patterns):\n",
    "        pattern_name = test_bundle.pattern_name\n",
    "        results_matrix[pattern_name] = {}\n",
    "        \n",
    "        print(f'\\n--- Pattern {pattern_idx + 1}: {pattern_name} ---')\n",
    "        \n",
    "        for strength in strengths:\n",
    "            try:\n",
    "                clear_cache()\n",
    "                \n",
    "                # Test injection\n",
    "                res = inject_with_interpretation_prompt(\n",
    "                    model, tokenizer,\n",
    "                    prompt_text=prompt_text,\n",
    "                    steering_vector=test_bundle.steering_vector,\n",
    "                    interpretation_prompt=interp,\n",
    "                    injection_strength=strength,\n",
    "                    max_new_tokens=25,\n",
    "                    do_sample=False\n",
    "                )\n",
    "                \n",
    "                generated = res['generated_text'].strip()\n",
    "                results_matrix[pattern_name][strength] = generated\n",
    "                print(f'  Strength {strength}: \"{generated[:80]}...\"')\n",
    "                \n",
    "            except Exception as e:\n",
    "                results_matrix[pattern_name][strength] = f'Error: {str(e)}'\n",
    "                print(f'  Strength {strength}: Error - {str(e)[:50]}...')\n",
    "    \n",
    "    # Summary comparison\n",
    "    print(f'\\n=== STRENGTH COMPARISON SUMMARY ===')\n",
    "    for pattern_name, strength_results in results_matrix.items():\n",
    "        print(f'\\n{pattern_name}:')\n",
    "        for strength, result in strength_results.items():\n",
    "            print(f'  {strength}x: {result[:60]}...')\n",
    "    \n",
    "    # Save detailed results\n",
    "    detailed_results = {\n",
    "        'test_timestamp': datetime.now().isoformat(),\n",
    "        'model_name': MODEL_NAME,\n",
    "        'prompt_template': prompt_text,\n",
    "        'patterns_tested': len(test_patterns),\n",
    "        'strengths_tested': strengths,\n",
    "        'results_matrix': results_matrix,\n",
    "        'pattern_metadata': {\n",
    "            bundle.pattern_name: {\n",
    "                'layers': bundle.layers,\n",
    "                'examples': bundle.num_examples,\n",
    "                'batch': bundle.batch_num\n",
    "            } for bundle in test_patterns\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save to cache directory\n",
    "    if 'cache_dir' in locals():\n",
    "        results_file = os.path.join(cache_dir, 'test_results.pkl')\n",
    "        with open(results_file, 'wb') as f:\n",
    "            pickle.dump(detailed_results, f)\n",
    "        print(f'\\nüíæ Saved detailed test results to: {results_file}')\n",
    "    \n",
    "    # Cache usage summary\n",
    "    print(f'\\n=== CACHE SUMMARY ===')\n",
    "    if 'cache_dir' in locals():\n",
    "        cache_files = os.listdir(cache_dir)\n",
    "        total_size = sum(os.path.getsize(os.path.join(cache_dir, f)) for f in cache_files)\n",
    "        print(f'Cache directory: {cache_dir}')\n",
    "        print(f'Files: {len(cache_files)}')\n",
    "        print(f'Total size: {total_size / 1024 / 1024:.2f} MB')\n",
    "        print(f'Patterns cached: {len(bundles)}')\n",
    "        print(f'Layers per pattern: {len(bundles[0].layers) if bundles else 0}')\n",
    "        \n",
    "        # Show how to reload everything\n",
    "        print(f'\\nüìã To reload this session later:')\n",
    "        print(f'```python')\n",
    "        print(f'import pickle')\n",
    "        print(f'with open(\"{os.path.join(cache_dir, \"index.pkl\")}\", \"rb\") as f:')\n",
    "        print(f'    index_data = pickle.load(f)')\n",
    "        print(f'# Then load individual batches as needed')\n",
    "        print(f'```')\n",
    "    \n",
    "else:\n",
    "    print('‚ùå No bundles loaded. Run previous cells first.')\n",
    "\n",
    "# Final cleanup\n",
    "clear_cache()\n",
    "print(f'\\nüßπ Final cleanup complete - memory freed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use just 1 pattern and 1 example per pattern\n",
    "specific_layers = [15]\n",
    "print(f'Extracting from specific layers: {specific_layers}')\n",
    "\n",
    "try:\n",
    "    # Import using the existing working path\n",
    "    from nnsight_selfie.repeng.repeng_activation_extractor import RepengActivationExtractor\n",
    "    from nnsight_selfie.repeng.repeng_steering_vectors import RepengSteeringVectorGenerator\n",
    "    from nnsight_selfie.repeng.patterns_dataset import build_all_datasets\n",
    "    \n",
    "    # Create extractor with specific layers only\n",
    "    extractor = RepengActivationExtractor(model, tokenizer, layer_indices=specific_layers)\n",
    "    \n",
    "    # Get just 1 example from 1 pattern manually\n",
    "    datasets = build_all_datasets(PATTERNS_PATH, ['pos-neg'], max_patterns=1)\n",
    "    pattern_name = list(datasets.keys())[0]\n",
    "    dataset = datasets[pattern_name]['pos-neg'][:1]  # Take only first example\n",
    "    \n",
    "    print(f\"Processing 1 example from pattern: {pattern_name}\")\n",
    "    \n",
    "    activations, inputs = extractor.extract_dataset_activations(\n",
    "        dataset, batch_size=1, show_progress=True\n",
    "    )\n",
    "    \n",
    "    # Generate steering vector\n",
    "    generator = RepengSteeringVectorGenerator(model_type=getattr(model, \"model_name\", \"unknown\"))\n",
    "    steering = generator.generate_steering_vectors(activations, method='pca_diff')\n",
    "    \n",
    "    print(f'SUCCESS: Generated steering vector for {len(steering.directions)} layers')\n",
    "    \n",
    "    # Store for later use\n",
    "    bundles = [type('Bundle', (), {\n",
    "        'steering_vector': steering, \n",
    "        'pattern_name': pattern_name\n",
    "    })()]\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'ERROR: {e}')\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
