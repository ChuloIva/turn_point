{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Arithmetic Lab üßÆ\n",
    "**Compact notebook for vector arithmetic experiments. Each experiment is self-contained with its own config.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SETUP\nimport sys, warnings, torch, numpy as np\nfrom typing import List, Dict, Any, Tuple\nsys.path.insert(0, '..')\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\nfrom nnsight_selfie import ModelAgnosticSelfie, InterpretationPrompt, get_optimal_device\n\n# LOAD MODEL\nMODEL_NAME = \"google/gemma-3-4b-it\"  # Change as needed\nselfie = ModelAgnosticSelfie(MODEL_NAME, dtype=torch.bfloat16, load_in_8bit=False)\nprint(f\"‚úÖ {MODEL_NAME} loaded on {selfie.device} ({len(selfie.layer_paths)} layers)\")\n\n# HELPER FUNCTIONS\ndef show_tokens(text):\n    tokens = selfie.model.tokenizer.encode(text)\n    for i, token_id in enumerate(tokens):\n        token_str = selfie.model.tokenizer.decode([token_id])\n        print(f\"  {i:2d}: '{token_str.strip()}'\")\n    return tokens\n\ndef get_vector(text, token_pos, layer):\n    \"\"\"Get activation vector for a specific token position and layer.\n    \n    Args:\n        text: Input text (ALWAYS uses raw text - no chat template for activation capture!)\n        token_pos: Token position to extract from\n        layer: Layer index to extract from\n    \"\"\"\n    acts = selfie.get_activations(text, layer_indices=[layer], token_indices=[token_pos])\n    return acts[layer][0]\n\ndef interpret_vector(vector, prompt, injection_layer=3, max_tokens=30, use_chat_template=False):\n    \"\"\"Interpret a vector using the selfie interpretation system.\n    \n    Args:\n        vector: Activation vector to interpret\n        prompt: InterpretationPrompt object\n        injection_layer: Layer to inject the vector at\n        max_tokens: Maximum tokens to generate\n        use_chat_template: Whether to apply chat template formatting to INTERPRETATION (not capture)\n    \"\"\"\n    return selfie.interpret_vectors([vector], prompt, injection_layer, max_new_tokens=max_tokens, \n                                  use_chat_template=use_chat_template)[0].strip()\n\ndef experiment(base_text, base_pos, sub_text, sub_pos, add_text, add_pos, \n               extract_layer, inject_layer, interp_prompt, description=\"\", use_chat_template=False):\n    \"\"\"Run a vector arithmetic experiment.\n    \n    Args:\n        base_text, sub_text, add_text: Raw text for activation capture (no chat template applied)\n        base_pos, sub_pos, add_pos: Token positions in the RAW text\n        use_chat_template: Whether to use chat template for INTERPRETATION only\n    \"\"\"\n    print(f\"üßÆ {description}\")\n    print(f\"   Extract: L{extract_layer} | Inject: L{inject_layer}\")\n    if use_chat_template:\n        print(f\"   üìù Using chat template for interpretation\")\n    \n    # Always capture from raw text (no chat template) - token positions stay consistent\n    base_vec = get_vector(base_text, base_pos, extract_layer)\n    sub_vec = get_vector(sub_text, sub_pos, extract_layer) \n    add_vec = get_vector(add_text, add_pos, extract_layer)\n    \n    result_vec = base_vec - sub_vec + add_vec\n    # Apply chat template only to interpretation\n    interpretation = interpret_vector(result_vec, interp_prompt, inject_layer, use_chat_template=use_chat_template)\n    \n    print(f\"   ü§ñ {interpretation}\")\n    return result_vec, interpretation\n\nprint(\"\\nüöÄ Ready for experiments!\")\nprint(f\"üí° Chat template available: {selfie._should_use_chat_template()}\")\nprint(\"   Chat template will be applied to INTERPRETATION PROMPTS only (not activation capture)\")\nprint(\"   This ensures token positions remain consistent for activation extraction\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üî¨ EXPERIMENT 1: King - Man + Woman = ?\n",
    "**The classic vector arithmetic example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 1 CONFIG\n",
    "EXTRACT_LAYER = 12\n",
    "INJECT_LAYER = 3\n",
    "\n",
    "# INTERPRETATION PROMPT\n",
    "interp_prompt = InterpretationPrompt.create_entity_prompt(selfie.model.tokenizer)\n",
    "# Custom: InterpretationPrompt(selfie.model.tokenizer, [\"This refers to \", None])\n",
    "\n",
    "# CONCEPT STRINGS\n",
    "base_text = \"The king ruled the kingdom wisely\"\n",
    "sub_text = \"The man walked down the street\"\n",
    "add_text = \"The woman read an interesting book\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìù Tokenizing concept strings:\")\n",
    "print(\"\\nBASE (king):\")\n",
    "base_tokens = show_tokens(base_text)\n",
    "print(\"\\nSUBTRACT (man):\")\n",
    "sub_tokens = show_tokens(sub_text) \n",
    "print(\"\\nADD (woman):\")\n",
    "add_tokens = show_tokens(add_text)\n",
    "\n",
    "# SELECT TOKEN POSITIONS (modify these after seeing tokenization above)\n",
    "base_pos = 2  # \"king\" \n",
    "sub_pos = 2   # \"man\"\n",
    "add_pos = 2   # \"woman\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN EXPERIMENT (with chat template option)\n",
    "result = experiment(base_text, base_pos, sub_text, sub_pos, add_text, add_pos,\n",
    "                   EXTRACT_LAYER, INJECT_LAYER, interp_prompt, \n",
    "                   \"King - Man + Woman (Expected: Queen-like)\", use_chat_template=True)\n",
    "\n",
    "# Without chat template (original behavior):\n",
    "# result = experiment(base_text, base_pos, sub_text, sub_pos, add_text, add_pos,\n",
    "#                    EXTRACT_LAYER, INJECT_LAYER, interp_prompt, \n",
    "#                    \"King - Man + Woman (Expected: Queen-like)\", use_chat_template=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üî¨ EXPERIMENT 2: Doctor - Man + Woman = ?\n",
    "**Professional role gender swap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 2 CONFIG  \n",
    "EXTRACT_LAYER = 15\n",
    "INJECT_LAYER = 5\n",
    "\n",
    "# INTERPRETATION PROMPT\n",
    "interp_prompt = InterpretationPrompt(selfie.model.tokenizer, [\"This person is a \", None])\n",
    "\n",
    "# CONCEPT STRINGS\n",
    "base_text = \"The doctor examined the patient carefully\"\n",
    "sub_text = \"The man walked down the street\"\n",
    "add_text = \"The woman read an interesting book\"\n",
    "\n",
    "print(\"üìù Tokenizing concept strings:\")\n",
    "print(\"\\nBASE (doctor):\")\n",
    "base_tokens = show_tokens(base_text)\n",
    "print(\"\\nSUBTRACT (man):\")\n",
    "sub_tokens = show_tokens(sub_text)\n",
    "print(\"\\nADD (woman):\") \n",
    "add_tokens = show_tokens(add_text)\n",
    "\n",
    "# SELECT TOKEN POSITIONS\n",
    "base_pos = 1  # \"doctor\"\n",
    "sub_pos = 1   # \"man\" \n",
    "add_pos = 1   # \"woman\"\n",
    "\n",
    "# RUN EXPERIMENT\n",
    "result = experiment(base_text, base_pos, sub_text, sub_pos, add_text, add_pos,\n",
    "                   EXTRACT_LAYER, INJECT_LAYER, interp_prompt,\n",
    "                   \"Doctor - Man + Woman (Expected: Nurse-like)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üî¨ EXPERIMENT 3: Custom Experiment\n",
    "**Your own vector arithmetic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 3 CONFIG\n",
    "EXTRACT_LAYER = 10 \n",
    "INJECT_LAYER = 2\n",
    "\n",
    "# INTERPRETATION PROMPT (customize as needed)\n",
    "# interp_prompt = InterpretationPrompt.create_concept_prompt(selfie.model.tokenizer)\n",
    "# interp_prompt = InterpretationPrompt.create_sentiment_prompt(selfie.model.tokenizer)\n",
    "interp_prompt = InterpretationPrompt(selfie.model.tokenizer, [\"This represents \", None, \" in society\"])\n",
    "\n",
    "# CONCEPT STRINGS (modify as needed)\n",
    "base_text = \"The teacher explained the concept clearly\"\n",
    "sub_text = \"The woman read an interesting book\" \n",
    "add_text = \"The man walked down the street\"\n",
    "\n",
    "print(\"üìù Tokenizing concept strings:\")\n",
    "print(\"\\nBASE:\")\n",
    "base_tokens = show_tokens(base_text)\n",
    "print(\"\\nSUBTRACT:\")\n",
    "sub_tokens = show_tokens(sub_text)\n",
    "print(\"\\nADD:\")\n",
    "add_tokens = show_tokens(add_text)\n",
    "\n",
    "# SELECT TOKEN POSITIONS\n",
    "base_pos = 1  # Adjust based on tokenization above\n",
    "sub_pos = 1   # Adjust based on tokenization above\n",
    "add_pos = 1   # Adjust based on tokenization above\n",
    "\n",
    "# RUN EXPERIMENT\n",
    "result = experiment(base_text, base_pos, sub_text, sub_pos, add_text, add_pos,\n",
    "                   EXTRACT_LAYER, INJECT_LAYER, interp_prompt,\n",
    "                   \"Custom: Teacher - Woman + Man\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üî¨ EXPERIMENT 4: Multi-Layer Aggregation\n",
    "**Advanced: Sum vectors from multiple layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Tokenizing concept strings:\n",
      "\n",
      "BASE (scientist):\n",
      "   0: '<bos>'\n",
      "   1: 'The'\n",
      "   2: 'king'\n",
      "   3: 'sat'\n",
      "   4: 'on'\n",
      "   5: 'his'\n",
      "   6: 'throne'\n",
      "\n",
      "SUBTRACT (man):\n",
      "   0: '<bos>'\n",
      "   1: 'The'\n",
      "   2: 'man'\n",
      "   3: 'sat'\n",
      "   4: 'on'\n",
      "   5: 'his'\n",
      "   6: 'throne'\n",
      "\n",
      "ADD (woman):\n",
      "   0: '<bos>'\n",
      "   1: 'The'\n",
      "   2: 'woman'\n",
      "   3: 'sat'\n",
      "   4: 'on'\n",
      "   5: 'her'\n",
      "   6: 'throne'\n"
     ]
    }
   ],
   "source": [
    "# EXPERIMENT 4 CONFIG\n",
    "EXTRACT_LAYERS = [3, 5, 13, 24,]  # Multiple layers to extract from\n",
    "INJECT_LAYER = 3\n",
    "AGGREGATION = \"sum\"  # \"sum\", \"mean\", or \"concat\"\n",
    "\n",
    "# INTERPRETATION PROMPT\n",
    "# interp_prompt = InterpretationPrompt.create_entity_prompt(selfie.model.tokenizer)\n",
    "\n",
    "interp_prompt = InterpretationPrompt(selfie.model.tokenizer, [\" \", None, \" in society\"])\n",
    "\n",
    "# CONCEPT STRINGS\n",
    "base_text = \"The king sat on his throne\"\n",
    "sub_text = \"The man sat on his throne\"\n",
    "add_text = \"The woman sat on her throne\"\n",
    "\n",
    "print(\"üìù Tokenizing concept strings:\")\n",
    "print(\"\\nBASE (scientist):\")\n",
    "show_tokens(base_text)\n",
    "print(\"\\nSUBTRACT (man):\")\n",
    "show_tokens(sub_text)\n",
    "print(\"\\nADD (woman):\")\n",
    "show_tokens(add_text)\n",
    "\n",
    "# SELECT TOKEN POSITIONS\n",
    "base_pos = 2  # \"scientist\"\n",
    "sub_pos = 2   # \"man\"\n",
    "add_pos = 2   # \"woman\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extract: L[3, 5, 13, 24] (sum) | Inject: L3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ü§ñ 's best season in the league so far.</h1>\n",
      "\n",
      "This is an incomplete sentence.  To make it a grammatically correct sentence, you need\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# MULTI-LAYER EXTRACTION FUNCTION\n",
    "def get_multi_layer_vector(text, token_pos, layers, agg_method=\"sum\"):\n",
    "    acts = selfie.get_activations(text, layer_indices=layers, token_indices=[token_pos])\n",
    "    vectors = [acts[layer][0] for layer in layers]\n",
    "    \n",
    "    if agg_method == \"sum\":\n",
    "        return torch.stack(vectors).sum(dim=0)\n",
    "    elif agg_method == \"mean\":\n",
    "        return torch.stack(vectors).mean(dim=0)\n",
    "    elif agg_method == \"concat\":\n",
    "        return torch.cat(vectors, dim=0)\n",
    "\n",
    "# RUN MULTI-LAYER EXPERIMENT\n",
    "# print(f\"\\nüßÆ Multi-layer: Scientist - Man + Woman\")\n",
    "print(f\"   Extract: L{EXTRACT_LAYERS} ({AGGREGATION}) | Inject: L{INJECT_LAYER}\")\n",
    "\n",
    "base_vec = get_multi_layer_vector(base_text, base_pos, EXTRACT_LAYERS, AGGREGATION)\n",
    "sub_vec = get_multi_layer_vector(sub_text, sub_pos, EXTRACT_LAYERS, AGGREGATION)\n",
    "add_vec = get_multi_layer_vector(add_text, add_pos, EXTRACT_LAYERS, AGGREGATION)\n",
    "\n",
    "result_vec = base_vec - sub_vec + add_vec\n",
    "interpretation = interpret_vector(result_vec, interp_prompt, INJECT_LAYER)\n",
    "\n",
    "print(f\"   ü§ñ {interpretation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üî¨ EXPERIMENT 5: Emotion/Abstract Concepts\n",
    "**Testing abstract concept arithmetic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 5 CONFIG\n",
    "EXTRACT_LAYER = 13\n",
    "INJECT_LAYER = 3\n",
    "\n",
    "# CUSTOM INTERPRETATION PROMPT FOR EMOTIONS/CONCEPTS\n",
    "interp_prompt = InterpretationPrompt(selfie.model.tokenizer, [\"This emotion or concept is \", None])\n",
    "\n",
    "# CONCEPT STRINGS (abstract/emotional)\n",
    "base_text = \"She felt incredibly happy about the news\"\n",
    "sub_text = \"He was extremely sad about the loss\" \n",
    "add_text = \"They became very angry at the situation\"\n",
    "\n",
    "print(\"üìù Tokenizing concept strings:\")\n",
    "print(\"\\nBASE (happy):\")\n",
    "show_tokens(base_text)\n",
    "print(\"\\nSUBTRACT (sad):\")\n",
    "show_tokens(sub_text)\n",
    "print(\"\\nADD (angry):\")\n",
    "show_tokens(add_text)\n",
    "\n",
    "# SELECT TOKEN POSITIONS (look for emotion words)\n",
    "base_pos = 4  # \"happy\" (adjust based on tokenization)\n",
    "sub_pos = 4   # \"sad\"\n",
    "add_pos = 4   # \"angry\"\n",
    "\n",
    "# RUN EXPERIMENT\n",
    "result = experiment(base_text, base_pos, sub_text, sub_pos, add_text, add_pos,\n",
    "                   EXTRACT_LAYER, INJECT_LAYER, interp_prompt,\n",
    "                   \"Emotion: Happy - Sad + Angry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéõÔ∏è Quick Experiment Template\n",
    "**Copy this cell and modify for rapid experimentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUICK TEMPLATE - COPY AND MODIFY\n",
    "EXTRACT_LAYER = 12\n",
    "INJECT_LAYER = 3\n",
    "\n",
    "# Choose interpretation style:\n",
    "interp_prompt = InterpretationPrompt.create_entity_prompt(selfie.model.tokenizer)\n",
    "# interp_prompt = InterpretationPrompt.create_concept_prompt(selfie.model.tokenizer)\n",
    "# interp_prompt = InterpretationPrompt.create_sentiment_prompt(selfie.model.tokenizer)\n",
    "# interp_prompt = InterpretationPrompt(selfie.model.tokenizer, [\"Custom: \", None, \" here\"])\n",
    "\n",
    "# Your strings here:\n",
    "base_text = \"Your base concept sentence\"\n",
    "sub_text = \"Your subtract concept sentence\"\n",
    "add_text = \"Your add concept sentence\"\n",
    "\n",
    "# Tokenize first, then set positions:\n",
    "print(\"BASE:\"); show_tokens(base_text)\n",
    "print(\"SUB:\"); show_tokens(sub_text) \n",
    "print(\"ADD:\"); show_tokens(add_text)\n",
    "\n",
    "# Set positions based on tokenization above:\n",
    "base_pos = 1\n",
    "sub_pos = 1\n",
    "add_pos = 1\n",
    "\n",
    "# Run:\n",
    "# experiment(base_text, base_pos, sub_text, sub_pos, add_text, add_pos,\n",
    "#           EXTRACT_LAYER, INJECT_LAYER, interp_prompt, \"Your description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üÜö CHAT TEMPLATE COMPARISON\n",
    "**Demonstrating the difference between using chat templates vs raw text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Running the same experiment with and without chat template:\n",
      "\n",
      "--- WITHOUT CHAT TEMPLATE ---\n",
      "üßÆ Teacher - Woman + Man\n",
      "   Extract: L11 | Inject: L3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ü§ñ clearly, but slowly, and with a meticulous attention to detail.\n",
      "He was _emphatic_ in his explanation.\n",
      "Her words were _artic\n",
      "\n",
      "--- WITH CHAT TEMPLATE ---\n",
      "üßÆ Teacher - Woman + Man\n",
      "   Extract: L11 | Inject: L3\n",
      "   üìù Using chat template formatting\n",
      "   üìù Using chat template: <bos><start_of_turn>user\n",
      "The teacher explained the concept clearly<end_of_turn>\n",
      "\n",
      "   üìù Using chat template: <bos><start_of_turn>user\n",
      "The woman read an interesting book<end_of_turn>\n",
      "\n",
      "   üìù Using chat template: <bos><start_of_turn>user\n",
      "The man walked down the street<end_of_turn>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ü§ñ _ _ _\n",
      "model\n",
      "Please provide the full prompt! I need the rest of the text to complete the sentence. üòä \n",
      "\n",
      "You started with \"This refers to...\" and\n",
      "\n",
      "üìä RESULTS COMPARISON:\n",
      "   Without template: 'clearly, but slowly, and with a meticulous attention to detail.\n",
      "He was _emphatic_ in his explanation.\n",
      "Her words were _artic'\n",
      "   With template:    '_ _ _\n",
      "model\n",
      "Please provide the full prompt! I need the rest of the text to complete the sentence. üòä \n",
      "\n",
      "You started with \"This refers to...\" and'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# COMPARISON EXPERIMENT CONFIG\n",
    "EXTRACT_LAYER = 11\n",
    "INJECT_LAYER = 3\n",
    "\n",
    "# CONCEPT STRINGS\n",
    "base_text = \"The teacher explained the concept clearly\"\n",
    "sub_text = \"The woman read an interesting book\"\n",
    "add_text = \"The man walked down the street\"\n",
    "\n",
    "# TOKEN POSITIONS\n",
    "base_pos = 3  # \"teacher\"\n",
    "sub_pos = 3   # \"woman\" \n",
    "add_pos = 3   # \"man\"\n",
    "\n",
    "# INTERPRETATION PROMPT\n",
    "interp_prompt = InterpretationPrompt.create_entity_prompt(selfie.model.tokenizer)\n",
    "\n",
    "print(\"üî¨ Running the same experiment with and without chat template:\")\n",
    "print(\"\\n--- WITHOUT CHAT TEMPLATE ---\")\n",
    "result_no_template = experiment(base_text, base_pos, sub_text, sub_pos, add_text, add_pos,\n",
    "                               EXTRACT_LAYER, INJECT_LAYER, interp_prompt,\n",
    "                               \"Teacher - Woman + Man\", use_chat_template=False)\n",
    "\n",
    "print(\"\\n--- WITH CHAT TEMPLATE ---\")\n",
    "result_with_template = experiment(base_text, base_pos, sub_text, sub_pos, add_text, add_pos,\n",
    "                                 EXTRACT_LAYER, INJECT_LAYER, interp_prompt,\n",
    "                                 \"Teacher - Woman + Man\", use_chat_template=True)\n",
    "\n",
    "print(f\"\\nüìä RESULTS COMPARISON:\")\n",
    "print(f\"   Without template: '{result_no_template[1]}'\")\n",
    "print(f\"   With template:    '{result_with_template[1]}'\")\n",
    "\n",
    "# if selfie._should_use_chat_template():\n",
    "#     print(f\"\\nüí° This model ({MODEL_NAME}) supports chat templates - using them may improve results!\")\n",
    "# else:\n",
    "#     print(f\"\\nüí° This model ({MODEL_NAME}) doesn't have a chat template - both methods should behave similarly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß Utilities\n",
    "**Helper functions for advanced use**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH LAYER COMPARISON\n",
    "def compare_layers(text, token_pos, layers):\n",
    "    \"\"\"Compare same concept across multiple layers.\"\"\"\n",
    "    print(f\"üîç Layer comparison for token {token_pos} in: '{text}'\")\n",
    "    \n",
    "    vectors = {}\n",
    "    for layer in layers:\n",
    "        vectors[layer] = get_vector(text, token_pos, layer)\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    import torch.nn.functional as F\n",
    "    for i, l1 in enumerate(layers):\n",
    "        for l2 in layers[i+1:]:\n",
    "            sim = F.cosine_similarity(vectors[l1].unsqueeze(0), vectors[l2].unsqueeze(0)).item()\n",
    "            print(f\"   L{l1} ‚Üî L{l2}: {sim:.3f}\")\n",
    "\n",
    "# SAVE EXPERIMENT RESULTS\n",
    "def save_experiment(name, base_text, sub_text, add_text, base_pos, sub_pos, add_pos,\n",
    "                   extract_layer, inject_layer, interpretation, filename=\"results.txt\"):\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(f\"\\n=== {name} ===\\n\")\n",
    "        f.write(f\"Base: '{base_text}' pos {base_pos}\\n\")\n",
    "        f.write(f\"Sub: '{sub_text}' pos {sub_pos}\\n\") \n",
    "        f.write(f\"Add: '{add_text}' pos {add_pos}\\n\")\n",
    "        f.write(f\"Layers: {extract_layer} ‚Üí {inject_layer}\\n\")\n",
    "        f.write(f\"Result: {interpretation}\\n\")\n",
    "    print(f\"üíæ Saved to {filename}\")\n",
    "\n",
    "print(\"üîß Utilities loaded\")\n",
    "print(\"\\nExamples:\")\n",
    "print(\"compare_layers('The king ruled wisely', 1, [8, 12, 16])\")\n",
    "print(\"save_experiment('King-Man+Woman', ...)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}