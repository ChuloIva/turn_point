{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Psychiatry Vector Arithmetic Laboratory\n",
    "\n",
    "An interactive laboratory for exploring neural representations in therapeutic contexts using vector arithmetic. This notebook allows you to experiment with positive thought patterns, cognitive transformations, and advanced activation manipulation.\n",
    "\n",
    "## üî¨ Lab Features:\n",
    "- Load and experiment with positive thought patterns from JSONL dataset\n",
    "- Multi-layer activation extraction and injection\n",
    "- Advanced vector arithmetic operations\n",
    "- Modular parameter controls for easy experimentation\n",
    "- Custom interpretation prompts for therapeutic contexts\n",
    "- Interactive visualization of cognitive pattern transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Lab Setup and Initialization\n",
    "\n",
    "Initialize all required components. Run this once, then experiment freely below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Psychiatry Vector Arithmetic Lab initialized!\n",
      "üß† Ready to explore therapeutic neural representations\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install torch transformers nnsight tqdm pandas numpy matplotlib seaborn\n",
    "# !pip install accelerate  # For efficient model loading\n",
    "\n",
    "# FOR AMD GPU\n",
    "# import os\n",
    "# os.environ[\"HSA_OVERRIDE_GFX_VERSION\"] = \"11.0.0\"\n",
    "# os.environ[\"HIP_VISIBLE_DEVICES\"] = \"0\"\n",
    "# os.environ[\"AMD_SERIALIZE_KERNEL\"] = \"3\"\n",
    "# os.environ[\"TORCH_USE_HIP_DSA\"] = \"1\"\n",
    "\n",
    "# IMPORTS\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Add current project directory to path to import nnsight_selfie\n",
    "sys.path.insert(0, '..')  # Go up one level from examples/ to the project root\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Import our library\n",
    "from nnsight_selfie import (\n",
    "    ModelAgnosticSelfie, \n",
    "    InterpretationPrompt, \n",
    "    print_device_info, \n",
    "    get_optimal_device\n",
    ")\n",
    "\n",
    "# Standard imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ Psychiatry Vector Arithmetic Lab initialized!\")\n",
    "print(\"üß† Ready to explore therapeutic neural representations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Device Detection ===\n",
      "=== Device Information ===\n",
      "Platform: Linux x86_64\n",
      "Python: 3.12.3\n",
      "PyTorch: 2.4.1+rocm6.0\n",
      "Optimal Device: cuda\n",
      "\n",
      "=== MPS Support ===\n",
      "MPS Available: False\n",
      "MPS Built: False\n",
      "\n",
      "=== CUDA Support ===\n",
      "CUDA Available: True\n",
      "CUDA Version: None\n",
      "Device Count: 1\n",
      "Primary Device: AMD Radeon RX 7700 XT\n",
      "\n",
      "üöÄ Optimal device: cuda\n",
      "\n",
      "üì• Loading google/gemma-3-4b-it...\n",
      "This may take a few minutes on first run\n",
      "Initializing model on device: cuda\n",
      "Filtered out vision components for Gemma 3 4B model.\n",
      "Model loaded successfully with 35 layers detected.\n",
      "‚úÖ Model loaded on: cuda\n",
      "üìä Layers: 35\n",
      "üî§ Vocab: 262,145 tokens\n"
     ]
    }
   ],
   "source": [
    "# Show device information and load model\n",
    "print(\"=== Device Detection ===\")\n",
    "print_device_info()\n",
    "\n",
    "optimal_device = get_optimal_device()\n",
    "print(f\"üöÄ Optimal device: {optimal_device}\")\n",
    "\n",
    "# Load model (adjust model name as needed)\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"  # Change this to experiment with different models\n",
    "\n",
    "print(f\"\\nüì• Loading {MODEL_NAME}...\")\n",
    "print(\"This may take a few minutes on first run\")\n",
    "\n",
    "try:\n",
    "    selfie = ModelAgnosticSelfie(\n",
    "        MODEL_NAME,\n",
    "        dtype=torch.bfloat16,\n",
    "        load_in_8bit=False\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded on: {selfie.device}\")\n",
    "    print(f\"üìä Layers: {len(selfie.layer_paths)}\")\n",
    "    print(f\"üî§ Vocab: {len(selfie.model.tokenizer):,} tokens\")\n",
    "    \n",
    "    # Store model info for experiments\n",
    "    MODEL_INFO = {\n",
    "        'name': MODEL_NAME,\n",
    "        'device': selfie.device,\n",
    "        'total_layers': len(selfie.layer_paths),\n",
    "        'vocab_size': len(selfie.model.tokenizer)\n",
    "    }\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load {MODEL_NAME}: {e}\")\n",
    "    print(\"üîÑ Consider using a smaller model or checking memory\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading positive patterns from: /home/koalacrown/Desktop/Code/Projects/turnaround/turn_point/data/final/positive_patterns.jsonl\n",
      "‚úÖ Loaded 520 positive thought patterns\n",
      "\n",
      "üìä Pattern Distribution:\n",
      "  Types: 13 unique types\n",
      "  Names: 13 unique patterns\n",
      "\n",
      "üîù Top Pattern Types:\n",
      "  - Cognitive depletion pattern: 40 examples\n",
      "  - Intrusive suicidal fixation: 40 examples\n",
      "  - Negative self-evaluative loop: 40 examples\n",
      "  - Internal dialectical processing: 40 examples\n",
      "  - Fragmented perceptual reasoning: 40 examples\n"
     ]
    }
   ],
   "source": [
    "# Load positive patterns dataset\n",
    "DATASET_PATH = \"/home/koalacrown/Desktop/Code/Projects/turnaround/turn_point/data/final/positive_patterns.jsonl\"\n",
    "\n",
    "print(f\"üìÇ Loading positive patterns from: {DATASET_PATH}\")\n",
    "\n",
    "try:\n",
    "    positive_patterns = []\n",
    "    with open(DATASET_PATH, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            pattern = json.loads(line.strip())\n",
    "            positive_patterns.append(pattern)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(positive_patterns)} positive thought patterns\")\n",
    "    \n",
    "    # Show pattern categories\n",
    "    pattern_types = defaultdict(int)\n",
    "    pattern_names = defaultdict(int)\n",
    "    \n",
    "    for pattern in positive_patterns:\n",
    "        pattern_types[pattern['cognitive_pattern_type']] += 1\n",
    "        pattern_names[pattern['cognitive_pattern_name']] += 1\n",
    "    \n",
    "    print(f\"\\nüìä Pattern Distribution:\")\n",
    "    print(f\"  Types: {len(pattern_types)} unique types\")\n",
    "    print(f\"  Names: {len(pattern_names)} unique patterns\")\n",
    "    \n",
    "    # Show top pattern types\n",
    "    print(f\"\\nüîù Top Pattern Types:\")\n",
    "    for ptype, count in sorted(pattern_types.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "        print(f\"  - {ptype}: {count} examples\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Dataset not found at {DATASET_PATH}\")\n",
    "    print(\"Please ensure the positive_patterns.jsonl file exists\")\n",
    "    positive_patterns = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≠ Therapeutic Interpretation Prompts\n",
    "\n",
    "Create specialized interpretation prompts for therapeutic and psychological contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≠ Therapeutic Interpretation Prompts Created:\n",
      "  Cognitive Pattern: 'This neural pattern represents the cognitive process of _ '\n",
      "  Emotional State: 'This activation encodes the emotional state of _ '\n",
      "  Therapeutic Concept: 'In therapeutic terms, this represents _ '\n",
      "  Mindfulness: 'From a mindfulness perspective, this embodies _ '\n",
      "  Resilience: 'This neural signature reflects resilience through _ '\n",
      "  Self Compassion: 'This pattern demonstrates self-compassion by _ '\n",
      "  Growth Mindset: 'This activation shows growth mindset through _ '\n",
      "  Coping Mechanism: 'As a coping mechanism, this represents _ '\n",
      "\n",
      "‚úÖ 8 therapeutic prompts ready for experimentation\n"
     ]
    }
   ],
   "source": [
    "# Create therapeutic interpretation prompts\n",
    "THERAPEUTIC_PROMPTS = {\n",
    "    \"cognitive_pattern\": InterpretationPrompt(\n",
    "        selfie.model.tokenizer,\n",
    "        [\"This neural pattern represents the cognitive process of \", None]\n",
    "    ),\n",
    "    \n",
    "    \"emotional_state\": InterpretationPrompt(\n",
    "        selfie.model.tokenizer,\n",
    "        [\"This activation encodes the emotional state of \", None]\n",
    "    ),\n",
    "    \n",
    "    \"therapeutic_concept\": InterpretationPrompt(\n",
    "        selfie.model.tokenizer,\n",
    "        [\"In therapeutic terms, this represents \", None]\n",
    "    ),\n",
    "    \n",
    "    \"mindfulness\": InterpretationPrompt(\n",
    "        selfie.model.tokenizer,\n",
    "        [\"From a mindfulness perspective, this embodies \", None]\n",
    "    ),\n",
    "    \n",
    "    \"resilience\": InterpretationPrompt(\n",
    "        selfie.model.tokenizer,\n",
    "        [\"This neural signature reflects resilience through \", None]\n",
    "    ),\n",
    "    \n",
    "    \"self_compassion\": InterpretationPrompt(\n",
    "        selfie.model.tokenizer,\n",
    "        [\"This pattern demonstrates self-compassion by \", None]\n",
    "    ),\n",
    "    \n",
    "    \"growth_mindset\": InterpretationPrompt(\n",
    "        selfie.model.tokenizer,\n",
    "        [\"This activation shows growth mindset through \", None]\n",
    "    ),\n",
    "    \n",
    "    \"coping_mechanism\": InterpretationPrompt(\n",
    "        selfie.model.tokenizer,\n",
    "        [\"As a coping mechanism, this represents \", None]\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"üé≠ Therapeutic Interpretation Prompts Created:\")\n",
    "for name, prompt in THERAPEUTIC_PROMPTS.items():\n",
    "    print(f\"  {name.replace('_', ' ').title()}: '{prompt.get_prompt()}'\")\n",
    "\n",
    "print(f\"\\n‚úÖ {len(THERAPEUTIC_PROMPTS)} therapeutic prompts ready for experimentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Experimental Utilities\n",
    "\n",
    "Modular functions for easy experimentation with different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Psychiatry Vector Lab initialized and ready for experiments!\n"
     ]
    }
   ],
   "source": [
    "class PsychiatryVectorLab:\n",
    "    \"\"\"Main laboratory class for psychiatric vector arithmetic experiments.\"\"\"\n",
    "    \n",
    "    def __init__(self, selfie_model, positive_patterns, therapeutic_prompts):\n",
    "        self.selfie = selfie_model\n",
    "        self.patterns = positive_patterns\n",
    "        self.prompts = therapeutic_prompts\n",
    "        self.activation_cache = {}\n",
    "        \n",
    "    def get_random_pattern(self, pattern_type: Optional[str] = None, pattern_name: Optional[str] = None):\n",
    "        \"\"\"Get a random positive pattern, optionally filtered by type or name.\"\"\"\n",
    "        candidates = self.patterns.copy()\n",
    "        \n",
    "        if pattern_type:\n",
    "            candidates = [p for p in candidates if pattern_type.lower() in p['cognitive_pattern_type'].lower()]\n",
    "        \n",
    "        if pattern_name:\n",
    "            candidates = [p for p in candidates if pattern_name.lower() in p['cognitive_pattern_name'].lower()]\n",
    "        \n",
    "        if not candidates:\n",
    "            print(f\"‚ö†Ô∏è No patterns found matching criteria: type='{pattern_type}', name='{pattern_name}'\")\n",
    "            return None\n",
    "            \n",
    "        return random.choice(candidates)\n",
    "    \n",
    "    def tokenize_and_display(self, text: str, max_tokens: int = 20) -> Tuple[List[int], List[str]]:\n",
    "        \"\"\"Tokenize text and display tokens for selection.\"\"\"\n",
    "        tokens = self.selfie.model.tokenizer.encode(text)\n",
    "        token_strings = [self.selfie.model.tokenizer.decode([t]) for t in tokens]\n",
    "        \n",
    "        print(f\"\\nüî§ Tokenization ({len(tokens)} tokens):\")\n",
    "        display_limit = min(len(tokens), max_tokens)\n",
    "        \n",
    "        for i in range(display_limit):\n",
    "            token_str = token_strings[i]\n",
    "            print(f\"  {i:2d}: '{token_str.strip()}'\")\n",
    "        \n",
    "        if len(tokens) > max_tokens:\n",
    "            print(f\"  ... and {len(tokens) - max_tokens} more tokens\")\n",
    "            \n",
    "        return tokens, token_strings\n",
    "    \n",
    "    def extract_activations(self, \n",
    "                          text: str,\n",
    "                          layers: List[int],\n",
    "                          token_positions: List[int],\n",
    "                          cache_key: Optional[str] = None) -> Dict[int, List[torch.Tensor]]:\n",
    "        \"\"\"Extract activations from multiple layers and token positions.\"\"\"\n",
    "        \n",
    "        if cache_key and cache_key in self.activation_cache:\n",
    "            print(f\"üìã Using cached activations: {cache_key}\")\n",
    "            return self.activation_cache[cache_key]\n",
    "        \n",
    "        print(f\"üßÆ Extracting activations from {len(layers)} layers, {len(token_positions)} tokens...\")\n",
    "        \n",
    "        activations = self.selfie.get_activations(\n",
    "            text,\n",
    "            layer_indices=layers,\n",
    "            token_indices=token_positions\n",
    "        )\n",
    "        \n",
    "        if cache_key:\n",
    "            self.activation_cache[cache_key] = activations\n",
    "            print(f\"üíæ Cached activations: {cache_key}\")\n",
    "        \n",
    "        return activations\n",
    "    \n",
    "    def interpret_activation(self,\n",
    "                           text: str,\n",
    "                           layer: int,\n",
    "                           token_position: int,\n",
    "                           prompt_name: str = \"cognitive_pattern\",\n",
    "                           max_tokens: int = 15) -> str:\n",
    "        \"\"\"Interpret a single activation using specified therapeutic prompt.\"\"\"\n",
    "        \n",
    "        if prompt_name not in self.prompts:\n",
    "            available = list(self.prompts.keys())\n",
    "            print(f\"‚ùå Unknown prompt: {prompt_name}. Available: {available}\")\n",
    "            return \"[Error: Unknown prompt]\"\n",
    "        \n",
    "        try:\n",
    "            result = self.selfie.interpret(\n",
    "                original_prompt=text,\n",
    "                interpretation_prompt=self.prompts[prompt_name],\n",
    "                tokens_to_interpret=[(layer, token_position)],\n",
    "                max_new_tokens=max_tokens\n",
    "            )\n",
    "            \n",
    "            return result['interpretation'][0].strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Interpretation failed: {e}\")\n",
    "            return f\"[Error: {str(e)}]\"\n",
    "    \n",
    "    def vector_arithmetic(self,\n",
    "                         base_activations: Dict[int, List[torch.Tensor]],\n",
    "                         base_layer: int,\n",
    "                         base_token: int,\n",
    "                         operations: List[Tuple[str, Dict, int, int, float]]) -> torch.Tensor:\n",
    "        \"\"\"Perform vector arithmetic: base + sum(op_weight * activation).\n",
    "        \n",
    "        Args:\n",
    "            base_activations: Activations dict from base text\n",
    "            base_layer: Layer index for base vector\n",
    "            base_token: Token index for base vector\n",
    "            operations: List of (operation, activations_dict, layer, token, weight)\n",
    "                       where operation is \"+\" or \"-\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get base vector\n",
    "        if isinstance(base_activations[base_layer], list):\n",
    "            result = base_activations[base_layer][base_token].clone()\n",
    "        else:\n",
    "            result = base_activations[base_layer][:, base_token, :].clone()\n",
    "        \n",
    "        print(f\"üî¢ Starting with base vector from layer {base_layer}, token {base_token}\")\n",
    "        \n",
    "        # Apply operations\n",
    "        for op, activations, layer, token, weight in operations:\n",
    "            if isinstance(activations[layer], list):\n",
    "                vector = activations[layer][token]\n",
    "            else:\n",
    "                vector = activations[layer][:, token, :]\n",
    "            \n",
    "            if op == \"+\":\n",
    "                result = result + weight * vector\n",
    "                print(f\"  ‚ûï Added {weight}x vector from layer {layer}, token {token}\")\n",
    "            elif op == \"-\":\n",
    "                result = result - weight * vector\n",
    "                print(f\"  ‚ûñ Subtracted {weight}x vector from layer {layer}, token {token}\")\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è Unknown operation: {op}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def interpret_vector(self,\n",
    "                        vector: torch.Tensor,\n",
    "                        injection_layer: int,\n",
    "                        prompt_name: str = \"cognitive_pattern\",\n",
    "                        max_tokens: int = 20) -> str:\n",
    "        \"\"\"Interpret a computed vector by injecting it into the model.\"\"\"\n",
    "        \n",
    "        if prompt_name not in self.prompts:\n",
    "            return f\"[Error: Unknown prompt {prompt_name}]\"\n",
    "        \n",
    "        try:\n",
    "            interpretation = self.selfie.interpret_vectors(\n",
    "                vectors=[vector],\n",
    "                interpretation_prompt=self.prompts[prompt_name],\n",
    "                injection_layer=injection_layer,\n",
    "                max_new_tokens=max_tokens\n",
    "            )[0]\n",
    "            \n",
    "            return interpretation.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"[Error: {str(e)}]\"\n",
    "\n",
    "# Initialize the lab\n",
    "lab = PsychiatryVectorLab(selfie, positive_patterns, THERAPEUTIC_PROMPTS)\n",
    "print(\"üß™ Psychiatry Vector Lab initialized and ready for experiments!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Dataset Explorer\n",
    "\n",
    "Explore the positive patterns dataset to understand available therapeutic contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dataset Explorer\n",
      "==================================================\n",
      "\n",
      "üß† Cognitive Pattern Types (13 total):\n",
      "  Cognitive depletion pattern........  40 examples\n",
      "  Intrusive suicidal fixation........  40 examples\n",
      "  Negative self-evaluative loop......  40 examples\n",
      "  Internal dialectical processing....  40 examples\n",
      "  Fragmented perceptual reasoning....  40 examples\n",
      "  Hyper-attuned interoception........  40 examples\n",
      "  Autobiographical integration.......  40 examples\n",
      "  Over-elaborative recounting........  40 examples\n",
      "  Entrapment cognition...............  40 examples\n",
      "  Existential rumination.............  40 examples\n",
      "  Learned helplessness loop..........  40 examples\n",
      "  Instrumental suicidal reasoning....  40 examples\n",
      "  Cognitive disorganization..........  40 examples\n",
      "\n",
      "üìù Example Patterns:\n",
      "\n",
      "üî∏ Example 1: Executive Fatigue & Avolition\n",
      "  Type: Cognitive depletion pattern\n",
      "  Positive Pattern: I'm recognizing that my energy levels are flagging today, which is totally normal. I've been pushing...\n",
      "  Negative Example: Ugh, just the thought of checking my email is draining me already. It's like try...\n",
      "\n",
      "üî∏ Example 2: Persistent Suicidal Ideation Focus\n",
      "  Type: Intrusive suicidal fixation\n",
      "  Positive Pattern: I've noticed how often my mind drifts to the idea of death as a coping mechanism when I'm feeling ov...\n",
      "  Negative Example: Ugh, there they go again - those incessant whispers about what would be better i...\n",
      "\n",
      "üî∏ Example 3: Self-Critical Rumination\n",
      "  Type: Negative self-evaluative loop\n",
      "  Positive Pattern: I can see how I could have done things differently, but instead of beating myself up over it, let's ...\n",
      "  Negative Example: Why did I have to mess that up again? It's just so typical of me to fail, like t...\n",
      "\n",
      "üéØ Quick Pattern Access:\n",
      "Use lab.get_random_pattern() or specify filters:\n",
      "  - lab.get_random_pattern(pattern_type='rumination')\n",
      "  - lab.get_random_pattern(pattern_name='Self-Critical')\n"
     ]
    }
   ],
   "source": [
    "# Explore the dataset patterns\n",
    "print(\"üìä Dataset Explorer\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if positive_patterns:\n",
    "    # Show pattern types distribution\n",
    "    pattern_type_counts = defaultdict(int)\n",
    "    for pattern in positive_patterns:\n",
    "        pattern_type_counts[pattern['cognitive_pattern_type']] += 1\n",
    "    \n",
    "    print(f\"\\nüß† Cognitive Pattern Types ({len(pattern_type_counts)} total):\")\n",
    "    for ptype, count in sorted(pattern_type_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {ptype:.<35} {count:>3} examples\")\n",
    "    \n",
    "    # Show some example patterns\n",
    "    print(f\"\\nüìù Example Patterns:\")\n",
    "    for i, pattern in enumerate(positive_patterns[:3]):\n",
    "        print(f\"\\nüî∏ Example {i+1}: {pattern['cognitive_pattern_name']}\")\n",
    "        print(f\"  Type: {pattern['cognitive_pattern_type']}\")\n",
    "        print(f\"  Positive Pattern: {pattern['positive_thought_pattern'][:100]}...\")\n",
    "        if 'reference_negative_example' in pattern:\n",
    "            print(f\"  Negative Example: {pattern['reference_negative_example'][:80]}...\")\n",
    "    \n",
    "    # Interactive pattern selector\n",
    "    print(f\"\\nüéØ Quick Pattern Access:\")\n",
    "    print(\"Use lab.get_random_pattern() or specify filters:\")\n",
    "    print(\"  - lab.get_random_pattern(pattern_type='rumination')\")\n",
    "    print(\"  - lab.get_random_pattern(pattern_name='Self-Critical')\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No patterns loaded. Please check the dataset path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Experiment 1: Single Pattern Analysis\n",
    "\n",
    "Analyze individual positive thought patterns across different layers and tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ EXPERIMENT 1: Single Pattern Analysis\n",
      "============================================================\n",
      "üéØ Selected Pattern: Persistent Suicidal Ideation Focus\n",
      "üìù Type: Intrusive suicidal fixation\n",
      "üìÑ Description: Thoughts repeatedly return to death as the central theme, crowding out other cognition.\n",
      "\n",
      "üí≠ Analyzing: 'I've noticed that my mind tends to wander back to the possibility of death, but I'm taking it as a s...'\n",
      "\n",
      "üî§ Tokenization (125 tokens):\n",
      "   0: '<bos>'\n",
      "   1: 'I'\n",
      "   2: '''\n",
      "   3: 've'\n",
      "   4: 'noticed'\n",
      "   5: 'that'\n",
      "   6: 'my'\n",
      "   7: 'mind'\n",
      "   8: 'tends'\n",
      "   9: 'to'\n",
      "  10: 'wander'\n",
      "  11: 'back'\n",
      "  12: 'to'\n",
      "  13: 'the'\n",
      "  14: 'possibility'\n",
      "  15: 'of'\n",
      "  16: 'death'\n",
      "  17: ','\n",
      "  18: 'but'\n",
      "  19: 'I'\n",
      "  20: '''\n",
      "  21: 'm'\n",
      "  22: 'taking'\n",
      "  23: 'it'\n",
      "  24: 'as'\n",
      "  25: 'a'\n",
      "  26: 'sign'\n",
      "  27: 'that'\n",
      "  28: 'I'\n",
      "  29: '''\n",
      "  30: 'm'\n",
      "  31: 'still'\n",
      "  32: 'grappling'\n",
      "  33: 'with'\n",
      "  34: 'some'\n",
      "  35: 'deep'\n",
      "  36: '-'\n",
      "  37: 'seated'\n",
      "  38: 'concerns'\n",
      "  39: 'and'\n",
      "  40: 'fears'\n",
      "  41: '.'\n",
      "  42: 'Rather'\n",
      "  43: 'than'\n",
      "  44: 'getting'\n",
      "  45: 'stuck'\n",
      "  46: 'on'\n",
      "  47: 'them'\n",
      "  48: ','\n",
      "  49: 'I'\n",
      "  50: '''\n",
      "  51: 'm'\n",
      "  52: 'using'\n",
      "  53: 'these'\n",
      "  54: 'thoughts'\n",
      "  55: 'as'\n",
      "  56: 'an'\n",
      "  57: 'opportunity'\n",
      "  58: 'to'\n",
      "  59: 'reflect'\n",
      "  60: 'on'\n",
      "  61: 'what'\n",
      "  62: '''\n",
      "  63: 's'\n",
      "  64: 'driving'\n",
      "  65: 'this'\n",
      "  66: 'preoccupation'\n",
      "  67: 'and'\n",
      "  68: 'how'\n",
      "  69: 'I'\n",
      "  70: 'can'\n",
      "  71: 'work'\n",
      "  72: 'through'\n",
      "  73: 'it'\n",
      "  74: 'in'\n",
      "  75: 'a'\n",
      "  76: 'healthy'\n",
      "  77: 'way'\n",
      "  78: '‚Äì'\n",
      "  79: 'whether'\n",
      "  80: 'that'\n",
      "  81: 'means'\n",
      "  82: 'seeking'\n",
      "  83: 'support'\n",
      "  84: 'from'\n",
      "  85: 'loved'\n",
      "  86: 'ones'\n",
      "  87: 'or'\n",
      "  88: 'engaging'\n",
      "  89: 'in'\n",
      "  ... and 35 more tokens\n",
      "\n",
      "üéØ Analyzing tokens at positions: [2, 4, 6, 122, 123]\n",
      "   2: '''\n",
      "   4: 'noticed'\n",
      "   6: 'my'\n",
      "  122: 'coping'\n",
      "  123: 'mechanisms'\n",
      "üßÆ Extracting activations from 6 layers, 5 tokens...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a5447a074c4523a318956c62d5a78a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have set `compile_config`, but we are unable to meet the criteria for compilation. Compilation will be skipped.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Cached activations: single_pattern_Persistent Suicidal \n",
      "\n",
      "üß† Interpretations using 'cognitive_pattern' prompt:\n",
      "================================================================================\n",
      "Interpreting 'I've noticed that my mind tends to wander back to the possibility of death, but I'm taking it as a sign that I'm still grappling with some deep-seated concerns and fears. Rather than getting stuck on them, I'm using these thoughts as an opportunity to reflect on what's driving this preoccupation and how I can work through it in a healthy way ‚Äì whether that means seeking support from loved ones or engaging in self-care practices like journaling or meditation. It's not always easy, but acknowledging my emotions and being kind to myself is helping me build resilience and better coping mechanisms.' with 'This neural pattern represents the cognitive process of _ '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 10, Token  2 ('''):\n",
      "  ü§ñ symmetrical thinking_ and _cognitive dissonance_.\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "This image depicts a person trying\n",
      "\n",
      "Interpreting 'I've noticed that my mind tends to wander back to the possibility of death, but I'm taking it as a sign that I'm still grappling with some deep-seated concerns and fears. Rather than getting stuck on them, I'm using these thoughts as an opportunity to reflect on what's driving this preoccupation and how I can work through it in a healthy way ‚Äì whether that means seeking support from loved ones or engaging in self-care practices like journaling or meditation. It's not always easy, but acknowledging my emotions and being kind to myself is helping me build resilience and better coping mechanisms.' with 'This neural pattern represents the cognitive process of _ '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 10, Token  4 ('noticed'):\n",
      "  ü§ñ that it is a very large, intricate piece of equipment. \n",
      "A. observation\n",
      "B.\n",
      "\n",
      "Interpreting 'I've noticed that my mind tends to wander back to the possibility of death, but I'm taking it as a sign that I'm still grappling with some deep-seated concerns and fears. Rather than getting stuck on them, I'm using these thoughts as an opportunity to reflect on what's driving this preoccupation and how I can work through it in a healthy way ‚Äì whether that means seeking support from loved ones or engaging in self-care practices like journaling or meditation. It's not always easy, but acknowledging my emotions and being kind to myself is helping me build resilience and better coping mechanisms.' with 'This neural pattern represents the cognitive process of _ '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 10, Token  6 ('my'):\n",
      "  ü§ñ friend, family or colleagues \n",
      "\n",
      "In this scenario, we need to identify the neuroscientific processes that\n",
      "\n",
      "Interpreting 'I've noticed that my mind tends to wander back to the possibility of death, but I'm taking it as a sign that I'm still grappling with some deep-seated concerns and fears. Rather than getting stuck on them, I'm using these thoughts as an opportunity to reflect on what's driving this preoccupation and how I can work through it in a healthy way ‚Äì whether that means seeking support from loved ones or engaging in self-care practices like journaling or meditation. It's not always easy, but acknowledging my emotions and being kind to myself is helping me build resilience and better coping mechanisms.' with 'This neural pattern represents the cognitive process of _ '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 15, Token  2 ('''):\n",
      "  ü§ñ **a. Attentional Focus**.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "*   **Att\n",
      "\n",
      "Interpreting 'I've noticed that my mind tends to wander back to the possibility of death, but I'm taking it as a sign that I'm still grappling with some deep-seated concerns and fears. Rather than getting stuck on them, I'm using these thoughts as an opportunity to reflect on what's driving this preoccupation and how I can work through it in a healthy way ‚Äì whether that means seeking support from loved ones or engaging in self-care practices like journaling or meditation. It's not always easy, but acknowledging my emotions and being kind to myself is helping me build resilience and better coping mechanisms.' with 'This neural pattern represents the cognitive process of _ '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 15, Token  4 ('noticed'):\n",
      "  ü§ñ that the organism performs when it is exposed to a novel and potentially threatening stimulus.\n",
      "This neural pattern\n",
      "\n",
      "Interpreting 'I've noticed that my mind tends to wander back to the possibility of death, but I'm taking it as a sign that I'm still grappling with some deep-seated concerns and fears. Rather than getting stuck on them, I'm using these thoughts as an opportunity to reflect on what's driving this preoccupation and how I can work through it in a healthy way ‚Äì whether that means seeking support from loved ones or engaging in self-care practices like journaling or meditation. It's not always easy, but acknowledging my emotions and being kind to myself is helping me build resilience and better coping mechanisms.' with 'This neural pattern represents the cognitive process of _ '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 15, Token  6 ('my'):\n",
      "  ü§ñ thinking.\n",
      "\n",
      "The word that best fits this description is **cognition**.\n",
      "\n",
      "Here's why\n",
      "\n",
      "Interpreting 'I've noticed that my mind tends to wander back to the possibility of death, but I'm taking it as a sign that I'm still grappling with some deep-seated concerns and fears. Rather than getting stuck on them, I'm using these thoughts as an opportunity to reflect on what's driving this preoccupation and how I can work through it in a healthy way ‚Äì whether that means seeking support from loved ones or engaging in self-care practices like journaling or meditation. It's not always easy, but acknowledging my emotions and being kind to myself is helping me build resilience and better coping mechanisms.' with 'This neural pattern represents the cognitive process of _ '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 25, Token  2 ('''):\n",
      "  ü§ñ 24 hour time in English_.\n",
      "\n",
      "Neural patterns are complex and multifaceted, and this particular pattern\n",
      "\n",
      "Interpreting 'I've noticed that my mind tends to wander back to the possibility of death, but I'm taking it as a sign that I'm still grappling with some deep-seated concerns and fears. Rather than getting stuck on them, I'm using these thoughts as an opportunity to reflect on what's driving this preoccupation and how I can work through it in a healthy way ‚Äì whether that means seeking support from loved ones or engaging in self-care practices like journaling or meditation. It's not always easy, but acknowledging my emotions and being kind to myself is helping me build resilience and better coping mechanisms.' with 'This neural pattern represents the cognitive process of _ '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 25, Token  4 ('noticed'):\n",
      "  ü§ñ a growing number of neuroscientists believe that the basic building blocks of human experience are formed through pattern recognition\n",
      "\n",
      "Interpreting 'I've noticed that my mind tends to wander back to the possibility of death, but I'm taking it as a sign that I'm still grappling with some deep-seated concerns and fears. Rather than getting stuck on them, I'm using these thoughts as an opportunity to reflect on what's driving this preoccupation and how I can work through it in a healthy way ‚Äì whether that means seeking support from loved ones or engaging in self-care practices like journaling or meditation. It's not always easy, but acknowledging my emotions and being kind to myself is helping me build resilience and better coping mechanisms.' with 'This neural pattern represents the cognitive process of _ '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 25, Token  6 ('my'):\n",
      "  ü§ñ computer or software application that takes input and produces output based on a predefined set of rules or learned patterns\n",
      "\n",
      "‚úÖ Single pattern analysis complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== EXPERIMENT PARAMETERS =====\n",
    "# Modify these to change the experiment\n",
    "\n",
    "# Pattern selection (set to None for random)\n",
    "EXPERIMENT_PATTERN_TYPE = None  # e.g., \"rumination\", \"intrusive\", \"negative self-evaluative\"\n",
    "EXPERIMENT_PATTERN_NAME = None  # e.g., \"Self-Critical\", \"Executive Fatigue\"\n",
    "\n",
    "# Layer analysis parameters  \n",
    "ANALYSIS_LAYERS = [5, 10, 15, 20, 25, 30]  # Which layers to analyze\n",
    "MAX_TOKENS_TO_SHOW = 90  # How many tokens to display for selection\n",
    "\n",
    "# Interpretation parameters\n",
    "INTERPRETATION_PROMPT = \"cognitive_pattern\"  # Which therapeutic prompt to use\n",
    "MAX_INTERPRETATION_TOKENS = 20\n",
    "\n",
    "# ===== EXPERIMENT EXECUTION =====\n",
    "\n",
    "print(\"üî¨ EXPERIMENT 1: Single Pattern Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get pattern\n",
    "pattern = lab.get_random_pattern(\n",
    "    pattern_type=EXPERIMENT_PATTERN_TYPE,\n",
    "    pattern_name=EXPERIMENT_PATTERN_NAME\n",
    ")\n",
    "\n",
    "if not pattern:\n",
    "    print(\"‚ùå No pattern found. Please adjust your filters.\")\n",
    "else:\n",
    "    print(f\"üéØ Selected Pattern: {pattern['cognitive_pattern_name']}\")\n",
    "    print(f\"üìù Type: {pattern['cognitive_pattern_type']}\")\n",
    "    print(f\"üìÑ Description: {pattern['pattern_description']}\")\n",
    "    \n",
    "    # Use the positive thought pattern\n",
    "    text = pattern['positive_thought_pattern']\n",
    "    print(f\"\\nüí≠ Analyzing: '{text[:100]}...'\")\n",
    "    \n",
    "    # Tokenize and show options\n",
    "    tokens, token_strings = lab.tokenize_and_display(text, MAX_TOKENS_TO_SHOW)\n",
    "    \n",
    "    # Select interesting tokens (modify these based on tokenization output above)\n",
    "    INTERESTING_TOKENS = [2, 4, 6, -3, -2]  # Adjust based on your tokenization\n",
    "    \n",
    "    # Convert negative indices\n",
    "    token_positions = []\n",
    "    for pos in INTERESTING_TOKENS:\n",
    "        if pos < 0:\n",
    "            token_positions.append(len(tokens) + pos)\n",
    "        else:\n",
    "            token_positions.append(pos)\n",
    "    \n",
    "    # Filter valid positions\n",
    "    token_positions = [p for p in token_positions if 0 <= p < len(tokens)]\n",
    "    \n",
    "    print(f\"\\nüéØ Analyzing tokens at positions: {token_positions}\")\n",
    "    for pos in token_positions:\n",
    "        if pos < len(token_strings):\n",
    "            print(f\"  {pos:2d}: '{token_strings[pos].strip()}'\")\n",
    "    \n",
    "    # Extract activations\n",
    "    activations = lab.extract_activations(\n",
    "        text=text,\n",
    "        layers=ANALYSIS_LAYERS,\n",
    "        token_positions=token_positions,\n",
    "        cache_key=f\"single_pattern_{pattern['cognitive_pattern_name'][:20]}\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüß† Interpretations using '{INTERPRETATION_PROMPT}' prompt:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Interpret key combinations\n",
    "    for layer in [10, 15, 25]:  # Focus on a few layers\n",
    "        for token_idx, token_pos in enumerate(token_positions[:3]):  # First 3 tokens\n",
    "            if token_pos < len(token_strings):\n",
    "                token_text = token_strings[token_pos].strip()\n",
    "                interpretation = lab.interpret_activation(\n",
    "                    text=text,\n",
    "                    layer=layer,\n",
    "                    token_position=token_pos,\n",
    "                    prompt_name=INTERPRETATION_PROMPT,\n",
    "                    max_tokens=MAX_INTERPRETATION_TOKENS\n",
    "                )\n",
    "                \n",
    "                print(f\"Layer {layer:2d}, Token {token_pos:2d} ('{token_text}'):\")\n",
    "                print(f\"  ü§ñ {interpretation}\")\n",
    "                print()\n",
    "    \n",
    "    print(\"‚úÖ Single pattern analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öóÔ∏è Experiment 2: Cognitive Pattern Vector Arithmetic\n",
    "\n",
    "Perform vector arithmetic between different cognitive patterns to explore therapeutic transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EXPERIMENT PARAMETERS =====\n",
    "\n",
    "# Pattern selection for arithmetic experiments\n",
    "BASE_PATTERN_TYPE = \"negative self-evaluative\"  # Base pattern type\n",
    "POSITIVE_PATTERN_TYPE = \"mindfulness\"  # Pattern to add\n",
    "SUBTRACT_PATTERN_TYPE = \"rumination\"  # Pattern to subtract\n",
    "\n",
    "# Vector arithmetic parameters\n",
    "EXTRACTION_LAYER = 15  # Layer to extract activations from\n",
    "INJECTION_LAYER = 10   # Layer to inject result into\n",
    "TARGET_TOKEN_POSITION = 3  # Which token position to use (adjust after tokenization)\n",
    "\n",
    "# Arithmetic weights\n",
    "POSITIVE_WEIGHT = 1.0\n",
    "NEGATIVE_WEIGHT = 0.8\n",
    "\n",
    "# Interpretation settings\n",
    "ARITHMETIC_INTERPRETATION_PROMPT = \"therapeutic_concept\"\n",
    "\n",
    "# ===== EXPERIMENT EXECUTION =====\n",
    "\n",
    "print(\"‚öóÔ∏è EXPERIMENT 2: Cognitive Pattern Vector Arithmetic\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get patterns for arithmetic\n",
    "base_pattern = lab.get_random_pattern(pattern_type=BASE_PATTERN_TYPE)\n",
    "positive_pattern = lab.get_random_pattern(pattern_type=POSITIVE_PATTERN_TYPE)\n",
    "subtract_pattern = lab.get_random_pattern(pattern_type=SUBTRACT_PATTERN_TYPE)\n",
    "\n",
    "patterns_found = [p for p in [base_pattern, positive_pattern, subtract_pattern] if p]\n",
    "\n",
    "if len(patterns_found) < 2:\n",
    "    print(\"‚ùå Need at least 2 patterns for arithmetic. Please adjust pattern types.\")\n",
    "    print(f\"Found: {len(patterns_found)} patterns\")\n",
    "else:\n",
    "    print(f\"üßÆ Arithmetic Experiment: Base + Positive - Negative\")\n",
    "    \n",
    "    if base_pattern:\n",
    "        base_text = base_pattern['positive_thought_pattern']\n",
    "        print(f\"\\nüî∏ Base Pattern ({base_pattern['cognitive_pattern_name']}):\")\n",
    "        print(f\"  '{base_text[:80]}...'\")\n",
    "        \n",
    "        # Get activations for base\n",
    "        base_tokens, base_token_strings = lab.tokenize_and_display(base_text, 10)\n",
    "        base_activations = lab.extract_activations(\n",
    "            text=base_text,\n",
    "            layers=[EXTRACTION_LAYER],\n",
    "            token_positions=[min(TARGET_TOKEN_POSITION, len(base_tokens)-1)],\n",
    "            cache_key=\"arithmetic_base\"\n",
    "        )\n",
    "    \n",
    "    operations = []\n",
    "    \n",
    "    if positive_pattern:\n",
    "        pos_text = positive_pattern['positive_thought_pattern']\n",
    "        print(f\"\\n‚ûï Positive Pattern ({positive_pattern['cognitive_pattern_name']}):\")\n",
    "        print(f\"  '{pos_text[:80]}...'\")\n",
    "        \n",
    "        pos_tokens, _ = lab.tokenize_and_display(pos_text, 5)\n",
    "        pos_activations = lab.extract_activations(\n",
    "            text=pos_text,\n",
    "            layers=[EXTRACTION_LAYER],\n",
    "            token_positions=[min(TARGET_TOKEN_POSITION, len(pos_tokens)-1)],\n",
    "            cache_key=\"arithmetic_positive\"\n",
    "        )\n",
    "        \n",
    "        operations.append((\"+\", pos_activations, EXTRACTION_LAYER, \n",
    "                         min(TARGET_TOKEN_POSITION, len(pos_tokens)-1), POSITIVE_WEIGHT))\n",
    "    \n",
    "    if subtract_pattern:\n",
    "        neg_text = subtract_pattern['positive_thought_pattern']\n",
    "        print(f\"\\n‚ûñ Subtract Pattern ({subtract_pattern['cognitive_pattern_name']}):\")\n",
    "        print(f\"  '{neg_text[:80]}...'\")\n",
    "        \n",
    "        neg_tokens, _ = lab.tokenize_and_display(neg_text, 5)\n",
    "        neg_activations = lab.extract_activations(\n",
    "            text=neg_text,\n",
    "            layers=[EXTRACTION_LAYER],\n",
    "            token_positions=[min(TARGET_TOKEN_POSITION, len(neg_tokens)-1)],\n",
    "            cache_key=\"arithmetic_negative\"\n",
    "        )\n",
    "        \n",
    "        operations.append((\"-\", neg_activations, EXTRACTION_LAYER,\n",
    "                         min(TARGET_TOKEN_POSITION, len(neg_tokens)-1), NEGATIVE_WEIGHT))\n",
    "    \n",
    "    if base_pattern and operations:\n",
    "        # Perform vector arithmetic\n",
    "        print(f\"\\nüî¢ Performing arithmetic at layer {EXTRACTION_LAYER}:\")\n",
    "        result_vector = lab.vector_arithmetic(\n",
    "            base_activations=base_activations,\n",
    "            base_layer=EXTRACTION_LAYER,\n",
    "            base_token=min(TARGET_TOKEN_POSITION, len(base_tokens)-1),\n",
    "            operations=operations\n",
    "        )\n",
    "        \n",
    "        # Interpret the result\n",
    "        print(f\"\\nüß† Interpreting result vector (injection at layer {INJECTION_LAYER}):\")\n",
    "        interpretation = lab.interpret_vector(\n",
    "            vector=result_vector,\n",
    "            injection_layer=INJECTION_LAYER,\n",
    "            prompt_name=ARITHMETIC_INTERPRETATION_PROMPT,\n",
    "            max_tokens=25\n",
    "        )\n",
    "        \n",
    "        print(f\"ü§ñ Result: {interpretation}\")\n",
    "        \n",
    "        # Try different interpretation prompts\n",
    "        print(f\"\\nüé≠ Alternative Interpretations:\")\n",
    "        for prompt_name in [\"emotional_state\", \"resilience\", \"coping_mechanism\"]:\n",
    "            alt_interpretation = lab.interpret_vector(\n",
    "                vector=result_vector,\n",
    "                injection_layer=INJECTION_LAYER,\n",
    "                prompt_name=prompt_name,\n",
    "                max_tokens=20\n",
    "            )\n",
    "            print(f\"  {prompt_name.replace('_', ' ').title()}: {alt_interpretation}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Vector arithmetic experiment complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Experiment 3: Multi-Layer Activation Blending\n",
    "\n",
    "Extract activations from multiple layers and blend them for complex therapeutic representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EXPERIMENT PARAMETERS =====\n",
    "\n",
    "# Pattern and layer selection\n",
    "BLENDING_PATTERN_TYPE = \"resilience\"  # Type of pattern to analyze\n",
    "SOURCE_LAYERS = [5, 10, 15, 20, 25]  # Layers to extract from\n",
    "LAYER_WEIGHTS = [0.1, 0.2, 0.3, 0.3, 0.1]  # Weights for blending (must sum to 1.0)\n",
    "TARGET_TOKENS = [2, 4, 6]  # Token positions to analyze\n",
    "\n",
    "# Injection parameters\n",
    "INJECTION_LAYERS = [8, 12, 18]  # Where to inject blended vectors\n",
    "BLENDING_PROMPTS = [\"resilience\", \"growth_mindset\", \"self_compassion\"]  # Interpretation contexts\n",
    "\n",
    "# ===== EXPERIMENT EXECUTION =====\n",
    "\n",
    "print(\"üé® EXPERIMENT 3: Multi-Layer Activation Blending\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Validate parameters\n",
    "if abs(sum(LAYER_WEIGHTS) - 1.0) > 0.01:\n",
    "    print(f\"‚ö†Ô∏è Layer weights sum to {sum(LAYER_WEIGHTS):.3f}, normalizing...\")\n",
    "    total = sum(LAYER_WEIGHTS)\n",
    "    LAYER_WEIGHTS = [w/total for w in LAYER_WEIGHTS]\n",
    "\n",
    "if len(SOURCE_LAYERS) != len(LAYER_WEIGHTS):\n",
    "    print(f\"‚ùå Mismatch: {len(SOURCE_LAYERS)} layers but {len(LAYER_WEIGHTS)} weights\")\n",
    "else:\n",
    "    # Get pattern\n",
    "    pattern = lab.get_random_pattern(pattern_type=BLENDING_PATTERN_TYPE)\n",
    "    \n",
    "    if not pattern:\n",
    "        print(f\"‚ùå No pattern found for type: {BLENDING_PATTERN_TYPE}\")\n",
    "    else:\n",
    "        text = pattern['positive_thought_pattern']\n",
    "        print(f\"üéØ Pattern: {pattern['cognitive_pattern_name']}\")\n",
    "        print(f\"üìù Text: '{text[:100]}...'\")\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens, token_strings = lab.tokenize_and_display(text, 12)\n",
    "        \n",
    "        # Validate token positions\n",
    "        valid_tokens = [t for t in TARGET_TOKENS if 0 <= t < len(tokens)]\n",
    "        if len(valid_tokens) != len(TARGET_TOKENS):\n",
    "            print(f\"‚ö†Ô∏è Some token positions invalid, using: {valid_tokens}\")\n",
    "            TARGET_TOKENS = valid_tokens\n",
    "        \n",
    "        print(f\"\\nüéØ Target tokens:\")\n",
    "        for pos in TARGET_TOKENS:\n",
    "            print(f\"  {pos:2d}: '{token_strings[pos].strip()}'\")\n",
    "        \n",
    "        # Extract activations from all source layers\n",
    "        activations = lab.extract_activations(\n",
    "            text=text,\n",
    "            layers=SOURCE_LAYERS,\n",
    "            token_positions=TARGET_TOKENS,\n",
    "            cache_key=f\"blending_{pattern['cognitive_pattern_name'][:15]}\"\n",
    "        )\n",
    "        \n",
    "        # Blend activations for each token\n",
    "        print(f\"\\nüé® Blending activations with weights: {[f'{w:.2f}' for w in LAYER_WEIGHTS]}\")\n",
    "        \n",
    "        blended_vectors = {}\n",
    "        \n",
    "        for token_idx, token_pos in enumerate(TARGET_TOKENS):\n",
    "            # Initialize with zeros\n",
    "            if isinstance(activations[SOURCE_LAYERS[0]], list):\n",
    "                blended = torch.zeros_like(activations[SOURCE_LAYERS[0]][token_idx])\n",
    "            else:\n",
    "                blended = torch.zeros_like(activations[SOURCE_LAYERS[0]][:, token_pos, :])\n",
    "            \n",
    "            # Weighted sum across layers\n",
    "            for layer_idx, (layer, weight) in enumerate(zip(SOURCE_LAYERS, LAYER_WEIGHTS)):\n",
    "                if isinstance(activations[layer], list):\n",
    "                    activation = activations[layer][token_idx]\n",
    "                else:\n",
    "                    activation = activations[layer][:, token_pos, :]\n",
    "                \n",
    "                blended += weight * activation\n",
    "            \n",
    "            blended_vectors[token_pos] = blended\n",
    "            print(f\"  ‚úÖ Blended vector for token {token_pos} ('{token_strings[token_pos].strip()}')\")\n",
    "        \n",
    "        # Interpret blended vectors across different injection layers and prompts\n",
    "        print(f\"\\nüß† Interpretations across {len(INJECTION_LAYERS)} injection layers:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for inject_layer in INJECTION_LAYERS:\n",
    "            print(f\"\\nüìç Injection Layer {inject_layer}:\")\n",
    "            \n",
    "            for token_pos in TARGET_TOKENS[:2]:  # Focus on first 2 tokens\n",
    "                token_text = token_strings[token_pos].strip()\n",
    "                print(f\"\\n  üî∏ Token '{token_text}' interpretations:\")\n",
    "                \n",
    "                for prompt_name in BLENDING_PROMPTS:\n",
    "                    interpretation = lab.interpret_vector(\n",
    "                        vector=blended_vectors[token_pos],\n",
    "                        injection_layer=inject_layer,\n",
    "                        prompt_name=prompt_name,\n",
    "                        max_tokens=18\n",
    "                    )\n",
    "                    \n",
    "                    print(f\"    {prompt_name.replace('_', ' ').title():.<18} {interpretation}\")\n",
    "        \n",
    "        # Analyze blending statistics\n",
    "        print(f\"\\nüìä Blending Statistics:\")\n",
    "        for token_pos in TARGET_TOKENS:\n",
    "            vector = blended_vectors[token_pos]\n",
    "            flat = vector.flatten()\n",
    "            \n",
    "            print(f\"  Token {token_pos:2d} ('{token_strings[token_pos].strip()}'):\")\n",
    "            print(f\"    Norm: {torch.norm(flat):.1f}, Mean: {flat.mean():.3f}, Std: {flat.std():.3f}\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Multi-layer blending experiment complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Experiment 4: Negative-to-Positive Pattern Transformation\n",
    "\n",
    "Use vector arithmetic to transform negative thought patterns into positive ones using the dataset's reference examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EXPERIMENT PARAMETERS =====\n",
    "\n",
    "# Transformation parameters\n",
    "TRANSFORMATION_LAYER = 16  # Layer for extracting transformation vectors\n",
    "INJECTION_LAYER = 12       # Layer for interpretation\n",
    "TOKEN_POSITION = 5         # Focus token (adjust after seeing tokenization)\n",
    "\n",
    "# Pattern selection\n",
    "FOCUS_PATTERN_TYPES = [\"rumination\", \"self-evaluative\", \"intrusive\"]  # Types with good neg/pos examples\n",
    "NUM_TRANSFORMATIONS = 3    # How many transformations to try\n",
    "\n",
    "# Interpretation prompts for transformation analysis\n",
    "TRANSFORMATION_PROMPTS = [\"therapeutic_concept\", \"resilience\", \"mindfulness\"]\n",
    "\n",
    "# ===== EXPERIMENT EXECUTION =====\n",
    "\n",
    "print(\"üîÑ EXPERIMENT 4: Negative-to-Positive Pattern Transformation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "transformation_results = []\n",
    "\n",
    "for transform_idx in range(NUM_TRANSFORMATIONS):\n",
    "    print(f\"\\nüéØ Transformation {transform_idx + 1}/{NUM_TRANSFORMATIONS}\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Find pattern with both negative and positive examples\n",
    "    suitable_patterns = []\n",
    "    for pattern in positive_patterns:\n",
    "        has_negative = 'reference_negative_example' in pattern and pattern['reference_negative_example']\n",
    "        has_positive = pattern['positive_thought_pattern']\n",
    "        is_focus_type = any(ftype.lower() in pattern['cognitive_pattern_type'].lower() \n",
    "                          for ftype in FOCUS_PATTERN_TYPES)\n",
    "        \n",
    "        if has_negative and has_positive and is_focus_type:\n",
    "            suitable_patterns.append(pattern)\n",
    "    \n",
    "    if not suitable_patterns:\n",
    "        print(\"‚ùå No suitable patterns found with both positive and negative examples\")\n",
    "        continue\n",
    "    \n",
    "    # Select random pattern\n",
    "    pattern = random.choice(suitable_patterns)\n",
    "    positive_text = pattern['positive_thought_pattern']\n",
    "    negative_text = pattern['reference_negative_example']\n",
    "    \n",
    "    print(f\"üìã Pattern: {pattern['cognitive_pattern_name']}\")\n",
    "    print(f\"üìù Type: {pattern['cognitive_pattern_type']}\")\n",
    "    \n",
    "    print(f\"\\n‚ùå Negative: '{negative_text[:80]}...'\")\n",
    "    print(f\"‚úÖ Positive: '{positive_text[:80]}...'\")\n",
    "    \n",
    "    # Tokenize both versions\n",
    "    print(f\"\\nüî§ Tokenizing negative version:\")\n",
    "    neg_tokens, neg_token_strings = lab.tokenize_and_display(negative_text, 10)\n",
    "    \n",
    "    print(f\"\\nüî§ Tokenizing positive version:\")\n",
    "    pos_tokens, pos_token_strings = lab.tokenize_and_display(positive_text, 10)\n",
    "    \n",
    "    # Adjust token position if needed\n",
    "    actual_token_pos = min(TOKEN_POSITION, min(len(neg_tokens), len(pos_tokens)) - 1)\n",
    "    \n",
    "    print(f\"\\nüéØ Using token position {actual_token_pos}:\")\n",
    "    print(f\"  Negative: '{neg_token_strings[actual_token_pos].strip()}'\")\n",
    "    print(f\"  Positive: '{pos_token_strings[actual_token_pos].strip()}'\")\n",
    "    \n",
    "    # Extract activations\n",
    "    neg_activations = lab.extract_activations(\n",
    "        text=negative_text,\n",
    "        layers=[TRANSFORMATION_LAYER],\n",
    "        token_positions=[actual_token_pos],\n",
    "        cache_key=f\"neg_{transform_idx}_{pattern['cognitive_pattern_name'][:10]}\"\n",
    "    )\n",
    "    \n",
    "    pos_activations = lab.extract_activations(\n",
    "        text=positive_text,\n",
    "        layers=[TRANSFORMATION_LAYER],\n",
    "        token_positions=[actual_token_pos],\n",
    "        cache_key=f\"pos_{transform_idx}_{pattern['cognitive_pattern_name'][:10]}\"\n",
    "    )\n",
    "    \n",
    "    # Compute transformation vector: positive - negative\n",
    "    print(f\"\\nüßÆ Computing transformation vector (positive - negative):\")\n",
    "    \n",
    "    if isinstance(pos_activations[TRANSFORMATION_LAYER], list):\n",
    "        pos_vector = pos_activations[TRANSFORMATION_LAYER][0]\n",
    "        neg_vector = neg_activations[TRANSFORMATION_LAYER][0]\n",
    "    else:\n",
    "        pos_vector = pos_activations[TRANSFORMATION_LAYER][:, actual_token_pos, :]\n",
    "        neg_vector = neg_activations[TRANSFORMATION_LAYER][:, actual_token_pos, :]\n",
    "    \n",
    "    transformation_vector = pos_vector - neg_vector\n",
    "    \n",
    "    # Analyze transformation vector\n",
    "    flat_transform = transformation_vector.flatten()\n",
    "    print(f\"  Transformation vector stats:\")\n",
    "    print(f\"    Norm: {torch.norm(flat_transform):.1f}\")\n",
    "    print(f\"    Mean: {flat_transform.mean():.3f}\")\n",
    "    print(f\"    Std: {flat_transform.std():.3f}\")\n",
    "    \n",
    "    # Interpret the transformation vector\n",
    "    print(f\"\\nüß† Transformation Vector Interpretations:\")\n",
    "    \n",
    "    transform_interpretations = {}\n",
    "    \n",
    "    for prompt_name in TRANSFORMATION_PROMPTS:\n",
    "        interpretation = lab.interpret_vector(\n",
    "            vector=transformation_vector,\n",
    "            injection_layer=INJECTION_LAYER,\n",
    "            prompt_name=prompt_name,\n",
    "            max_tokens=20\n",
    "        )\n",
    "        \n",
    "        transform_interpretations[prompt_name] = interpretation\n",
    "        print(f\"  {prompt_name.replace('_', ' ').title():.<20} {interpretation}\")\n",
    "    \n",
    "    # Test transformation: apply to negative to get positive-like\n",
    "    print(f\"\\nüîÑ Testing transformation (negative + transform_vector):\")\n",
    "    \n",
    "    transformed_vector = neg_vector + transformation_vector\n",
    "    \n",
    "    for prompt_name in [\"cognitive_pattern\", \"emotional_state\"]:\n",
    "        result_interpretation = lab.interpret_vector(\n",
    "            vector=transformed_vector,\n",
    "            injection_layer=INJECTION_LAYER,\n",
    "            prompt_name=prompt_name,\n",
    "            max_tokens=25\n",
    "        )\n",
    "        \n",
    "        print(f\"  {prompt_name.replace('_', ' ').title()}: {result_interpretation}\")\n",
    "    \n",
    "    # Store results\n",
    "    transformation_results.append({\n",
    "        'pattern_name': pattern['cognitive_pattern_name'],\n",
    "        'pattern_type': pattern['cognitive_pattern_type'],\n",
    "        'negative_text': negative_text[:100],\n",
    "        'positive_text': positive_text[:100],\n",
    "        'token_position': actual_token_pos,\n",
    "        'transformation_norm': float(torch.norm(flat_transform)),\n",
    "        'interpretations': transform_interpretations\n",
    "    })\n",
    "\n",
    "print(f\"\\nüìä Transformation Summary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if transformation_results:\n",
    "    avg_norm = np.mean([r['transformation_norm'] for r in transformation_results])\n",
    "    print(f\"Average transformation vector norm: {avg_norm:.1f}\")\n",
    "    \n",
    "    print(f\"\\nPattern types analyzed:\")\n",
    "    for result in transformation_results:\n",
    "        print(f\"  - {result['pattern_name']} ({result['pattern_type']})\")\n",
    "\n",
    "print(\"\\n‚úÖ Negative-to-positive transformation experiment complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Custom Experiment Sandbox\n",
    "\n",
    "Free-form experimentation space. Modify the parameters and code below to run your own custom experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CUSTOM EXPERIMENT SANDBOX =====\n",
    "# Feel free to modify anything below for your own experiments!\n",
    "\n",
    "print(\"üî¨ CUSTOM EXPERIMENT SANDBOX\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Example: Compare specific patterns\n",
    "PATTERN_A_NAME = \"Self-Critical\"  # Adjust based on dataset\n",
    "PATTERN_B_NAME = \"Executive Fatigue\"  # Adjust based on dataset\n",
    "\n",
    "# Your custom experiment here...\n",
    "pattern_a = None\n",
    "pattern_b = None\n",
    "\n",
    "# Find patterns by name\n",
    "for pattern in positive_patterns:\n",
    "    if PATTERN_A_NAME.lower() in pattern['cognitive_pattern_name'].lower():\n",
    "        pattern_a = pattern\n",
    "        break\n",
    "\n",
    "for pattern in positive_patterns:\n",
    "    if PATTERN_B_NAME.lower() in pattern['cognitive_pattern_name'].lower():\n",
    "        pattern_b = pattern\n",
    "        break\n",
    "\n",
    "if pattern_a and pattern_b:\n",
    "    print(f\"üîç Comparing:\")\n",
    "    print(f\"  A: {pattern_a['cognitive_pattern_name']}\")\n",
    "    print(f\"  B: {pattern_b['cognitive_pattern_name']}\")\n",
    "    \n",
    "    # Extract and compare activations\n",
    "    text_a = pattern_a['positive_thought_pattern']\n",
    "    text_b = pattern_b['positive_thought_pattern']\n",
    "    \n",
    "    # Your analysis code here...\n",
    "    # For example: compute similarity, perform arithmetic, visualize differences\n",
    "    \n",
    "    print(\"\\nüí° Add your custom analysis here!\")\n",
    "    print(\"Examples:\")\n",
    "    print(\"- Extract activations from both patterns\")\n",
    "    print(\"- Compute cosine similarity between vectors\")\n",
    "    print(\"- Perform pattern_a - pattern_b arithmetic\")\n",
    "    print(\"- Visualize activation differences\")\n",
    "    print(\"- Try different interpretation prompts\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå Patterns not found. Available pattern names:\")\n",
    "    unique_names = set(p['cognitive_pattern_name'] for p in positive_patterns)\n",
    "    for name in sorted(unique_names):\n",
    "        print(f\"  - {name}\")\n",
    "\n",
    "print(\"\\nüéâ Sandbox ready for your experiments!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Lab Results Visualization\n",
    "\n",
    "Visualize and analyze results from your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization and analysis of lab results\n",
    "print(\"üìä LAB RESULTS VISUALIZATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if we have transformation results to visualize\n",
    "if 'transformation_results' in locals() and transformation_results:\n",
    "    print(f\"\\nüìà Transformation Analysis (n={len(transformation_results)}):\")\n",
    "    \n",
    "    # Plot transformation vector norms\n",
    "    norms = [r['transformation_norm'] for r in transformation_results]\n",
    "    pattern_names = [r['pattern_name'][:20] for r in transformation_results]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(norms)), norms)\n",
    "    plt.xlabel('Transformation Experiment')\n",
    "    plt.ylabel('Vector Norm')\n",
    "    plt.title('Transformation Vector Magnitudes')\n",
    "    plt.xticks(range(len(pattern_names)), pattern_names, rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Average transformation norm: {np.mean(norms):.2f} ¬± {np.std(norms):.2f}\")\n",
    "\n",
    "# Model and dataset statistics\n",
    "if 'MODEL_INFO' in locals():\n",
    "    print(f\"\\nüèóÔ∏è Model Information:\")\n",
    "    for key, value in MODEL_INFO.items():\n",
    "        print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(f\"\\nüìÇ Dataset Statistics:\")\n",
    "print(f\"  Total patterns: {len(positive_patterns)}\")\n",
    "print(f\"  Cached activations: {len(lab.activation_cache)}\")\n",
    "\n",
    "# Cache information\n",
    "if lab.activation_cache:\n",
    "    print(f\"\\nüíæ Activation Cache:\")\n",
    "    for cache_key in lab.activation_cache.keys():\n",
    "        print(f\"  - {cache_key}\")\n",
    "\n",
    "print(\"\\n‚úÖ Lab session complete! Results visualized.\")\n",
    "print(\"\\nüî¨ To run more experiments:\")\n",
    "print(\"  1. Modify parameters in any experiment cell above\")\n",
    "print(\"  2. Re-run the experiment cell\")\n",
    "print(\"  3. Cached activations will speed up repeated runs\")\n",
    "print(\"  4. Use the sandbox for custom experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Lab Summary and Next Steps\n",
    "\n",
    "Summary of the Psychiatry Vector Arithmetic Laboratory and suggestions for further exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß† PSYCHIATRY VECTOR ARITHMETIC LAB SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüî¨ Lab Components:\")\n",
    "print(f\"  ‚úÖ Model loaded: {MODEL_INFO.get('name', 'Unknown')} on {MODEL_INFO.get('device', 'Unknown')}\")\n",
    "print(f\"  ‚úÖ Dataset: {len(positive_patterns)} positive thought patterns\")\n",
    "print(f\"  ‚úÖ Therapeutic prompts: {len(THERAPEUTIC_PROMPTS)} specialized interpretations\")\n",
    "print(f\"  ‚úÖ Lab utilities: Modular experimentation framework\")\n",
    "\n",
    "print(f\"\\nüß™ Experiments Available:\")\n",
    "print(f\"  1Ô∏è‚É£  Single Pattern Analysis - Layer-wise interpretation of individual patterns\")\n",
    "print(f\"  2Ô∏è‚É£  Vector Arithmetic - Combine/subtract cognitive patterns\")\n",
    "print(f\"  3Ô∏è‚É£  Multi-Layer Blending - Weight and combine activations from multiple layers\")\n",
    "print(f\"  4Ô∏è‚É£  Negative‚ÜíPositive Transformation - Learn therapeutic transformation vectors\")\n",
    "print(f\"  üî¨ Custom Sandbox - Free-form experimentation space\")\n",
    "\n",
    "print(f\"\\nüé≠ Therapeutic Interpretation Contexts:\")\n",
    "for name in THERAPEUTIC_PROMPTS.keys():\n",
    "    print(f\"  - {name.replace('_', ' ').title()}\")\n",
    "\n",
    "print(f\"\\nüìä Cognitive Pattern Types in Dataset:\")\n",
    "if positive_patterns:\n",
    "    pattern_types = list(set(p['cognitive_pattern_type'] for p in positive_patterns))\n",
    "    for ptype in sorted(pattern_types):\n",
    "        count = sum(1 for p in positive_patterns if p['cognitive_pattern_type'] == ptype)\n",
    "        print(f\"  - {ptype} ({count} examples)\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps and Ideas:\")\n",
    "print(f\"  üîÑ Run experiments with different model layers (early vs late)\")\n",
    "print(f\"  üéØ Focus on specific therapeutic domains (anxiety, depression, resilience)\")\n",
    "print(f\"  üìè Compare vector arithmetic results across different models\")\n",
    "print(f\"  üîç Analyze which layers best capture therapeutic concepts\")\n",
    "print(f\"  üé® Create therapeutic 'concept directions' for model steering\")\n",
    "print(f\"  üìä Build visualizations of cognitive pattern spaces\")\n",
    "print(f\"  ü§ù Combine multiple positive patterns for enhanced interventions\")\n",
    "\n",
    "print(f\"\\nüí° Advanced Research Directions:\")\n",
    "print(f\"  - Investigate layer-specific therapeutic representations\")\n",
    "print(f\"  - Build 'cognitive transformation functions' using vector arithmetic\")\n",
    "print(f\"  - Develop automated therapeutic prompt generation\")\n",
    "print(f\"  - Create personalized therapeutic vector profiles\")\n",
    "print(f\"  - Study cross-modal therapeutic representations\")\n",
    "\n",
    "print(f\"\\nüéâ Happy experimenting! The lab is ready for your therapeutic AI research.\")\n",
    "\n",
    "# Quick reference\n",
    "print(f\"\\nüìã Quick Reference:\")\n",
    "print(f\"  lab.get_random_pattern(pattern_type='rumination')\")\n",
    "print(f\"  lab.tokenize_and_display(text)\")\n",
    "print(f\"  lab.extract_activations(text, layers=[10, 15], token_positions=[2, 4])\")\n",
    "print(f\"  lab.interpret_activation(text, layer=15, token_position=3, prompt_name='resilience')\")\n",
    "print(f\"  lab.vector_arithmetic(base_acts, layer, token, operations)\")\n",
    "print(f\"  lab.interpret_vector(vector, injection_layer=12, prompt_name='mindfulness')\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
