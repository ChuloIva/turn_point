{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cognitive Pattern Analysis Notebook\n",
    "\n",
    "This notebook demonstrates the complete pipeline for analyzing cognitive patterns through neural network activations using TransformerLens, PCA, SAE, and selfie interpretation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Import our modules\n",
    "from models.model_loader import ModelLoader\n",
    "from models.activation_capture import ActivationCapturer\n",
    "from data.data_loader import DataLoader\n",
    "from analysis.pca_analysis import PCAAnalyzer\n",
    "from analysis.sae_interface import SAEInterface\n",
    "from analysis.interpretation import SelfieInterpreter, ActivationArithmetic\n",
    "\n",
    "print(\"✅ Imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('./config/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"Model: {config['model']['name']}\")\n",
    "print(f\"Local path: {config['model']['local_path']}\")\n",
    "print(f\"Layers: {config['model']['layers']}\")\n",
    "print(f\"Cognitive patterns: {config['data']['cognitive_patterns']}\")\n",
    "print(f\"Analysis methods: {config['analysis']['methods']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Data Loader and Load Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "data_loader = DataLoader(base_path=config['data']['base_path'])\n",
    "\n",
    "# Load cognitive patterns\n",
    "print(\"Loading cognitive patterns...\")\n",
    "cognitive_patterns = data_loader.load_cognitive_patterns(config['data']['cognitive_patterns'])\n",
    "\n",
    "# Display statistics\n",
    "stats = data_loader.get_pattern_stats()\n",
    "print(\"\\nDataset Statistics:\")\n",
    "for pattern, stat in stats.items():\n",
    "    print(f\"  {pattern}: {stat['count']} samples, avg length: {stat['avg_length']:.1f}\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nSample data:\")\n",
    "for pattern_name, strings in cognitive_patterns.items():\n",
    "    if strings:\n",
    "        print(f\"\\n{pattern_name} (first 2 samples):\")\n",
    "        for i, sample in enumerate(strings[:2]):\n",
    "            print(f\"  {i+1}. {sample[:100]}...\" if len(sample) > 100 else f\"  {i+1}. {sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Model and Activation Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize activation capturer\n",
    "print(\"Initializing activation capturer...\")\n",
    "activation_capturer = ActivationCapturer(\n",
    "    model_name=config['model']['name'],\n",
    "    device=config['model']['device']\n",
    ")\n",
    "\n",
    "# Load the local model\n",
    "print(\"Loading model...\")\n",
    "local_path = config['model']['local_path']\n",
    "activation_capturer.load_model(local_path)\n",
    "\n",
    "# Get model info\n",
    "model_info = activation_capturer.get_model_info()\n",
    "print(\"\\nModel Information:\")\n",
    "for key, value in model_info.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n✅ Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Capture Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture activations for all patterns\n",
    "print(\"Capturing activations...\")\n",
    "\n",
    "layers = config['model']['layers']\n",
    "position = config['capture']['position']\n",
    "max_samples = config['data'].get('max_samples_per_pattern', 100)  # Limit for notebook\n",
    "\n",
    "activations_dict = {}\n",
    "\n",
    "for pattern_name, strings in cognitive_patterns.items():\n",
    "    if not strings:\n",
    "        print(f\"Skipping {pattern_name} - no data\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nProcessing pattern: {pattern_name}\")\n",
    "    \n",
    "    # Limit samples for notebook demo\n",
    "    limited_strings = strings[:max_samples]\n",
    "    print(f\"  Processing {len(limited_strings)} samples\")\n",
    "    \n",
    "    # Capture activations\n",
    "    activations = activation_capturer.capture_activations(\n",
    "        strings=limited_strings,\n",
    "        layer_nums=layers,\n",
    "        cognitive_pattern=pattern_name,\n",
    "        position=position\n",
    "    )\n",
    "    \n",
    "    activations_dict[pattern_name] = activations\n",
    "    \n",
    "    # Show activation shapes\n",
    "    for key, tensor in activations.items():\n",
    "        print(f\"    {key}: {tensor.shape}\")\n",
    "\n",
    "print(\"\\n✅ Activation capture complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PCA analyzer\n",
    "pca_analyzer = PCAAnalyzer(n_components=config['analysis']['pca']['n_components'])\n",
    "\n",
    "print(\"Running PCA analysis...\")\n",
    "pca_results = {}\n",
    "\n",
    "for pattern_name, activations in activations_dict.items():\n",
    "    print(f\"\\nPCA for pattern: {pattern_name}\")\n",
    "    \n",
    "    # Compute PCA\n",
    "    pca_result = pca_analyzer.compute_pca(\n",
    "        activations=activations,\n",
    "        pattern_name=pattern_name,\n",
    "        standardize=config['analysis']['pca']['standardize']\n",
    "    )\n",
    "    \n",
    "    pca_results[pattern_name] = pca_result\n",
    "    \n",
    "    # Show explained variance for each layer\n",
    "    for layer_key in activations.keys():\n",
    "        explained_var = pca_analyzer.get_explained_variance_ratio(pattern_name, layer_key)\n",
    "        if len(explained_var) > 0:\n",
    "            cumulative_var = pca_analyzer.get_cumulative_explained_variance(pattern_name, layer_key)\n",
    "            print(f\"  {layer_key}: First 5 PCs explain {cumulative_var[4]:.1%} of variance\")\n",
    "\n",
    "print(\"\\n✅ PCA analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA explained variance\n",
    "print(\"Creating PCA visualizations...\")\n",
    "\n",
    "# Create plots for each pattern and layer\n",
    "for pattern_name, activations in activations_dict.items():\n",
    "    for layer_key in list(activations.keys())[:2]:  # Limit to first 2 layers for notebook\n",
    "        print(f\"Plotting {pattern_name} - {layer_key}\")\n",
    "        pca_analyzer.plot_explained_variance(pattern_name, layer_key)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA scatter plot comparing patterns\n",
    "if len(list(activations_dict.keys())) > 1:\n",
    "    print(\"Creating PCA comparison plots...\")\n",
    "    \n",
    "    # Get first layer key that exists in multiple patterns\n",
    "    common_layers = None\n",
    "    for pattern_name, activations in activations_dict.items():\n",
    "        if common_layers is None:\n",
    "            common_layers = set(activations.keys())\n",
    "        else:\n",
    "            common_layers &= set(activations.keys())\n",
    "    \n",
    "    if common_layers:\n",
    "        layer_to_plot = list(common_layers)[0]\n",
    "        pattern_names = list(activations_dict.keys())\n",
    "        \n",
    "        pca_analyzer.plot_pca_scatter(\n",
    "            pattern_names=pattern_names,\n",
    "            layer_key=layer_to_plot,\n",
    "            components=(0, 1)\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "        # Compute pattern separation\n",
    "        if len(pattern_names) >= 2:\n",
    "            separation = pca_analyzer.get_pattern_separation(\n",
    "                pattern_names[0], pattern_names[1], layer_to_plot\n",
    "            )\n",
    "            print(f\"Pattern separation ({pattern_names[0]} vs {pattern_names[1]}): {separation:.2f}\")\n",
    "else:\n",
    "    print(\"Need multiple patterns for comparison plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SAE Analysis (Placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SAE interface (placeholder)\n",
    "sae_interface = SAEInterface(sae_model_path=config['analysis']['sae']['model_path'])\n",
    "\n",
    "print(\"Running SAE analysis (placeholder implementation)...\")\n",
    "sae_results = {}\n",
    "\n",
    "# Analyze one pattern as demonstration\n",
    "demo_pattern = list(activations_dict.keys())[0]\n",
    "demo_activations = activations_dict[demo_pattern]\n",
    "\n",
    "print(f\"\\nAnalyzing SAE features for: {demo_pattern}\")\n",
    "sae_result = sae_interface.analyze_pattern_features(\n",
    "    activations=demo_activations,\n",
    "    pattern_name=demo_pattern,\n",
    "    top_k=config['analysis']['sae']['top_k_features']\n",
    ")\n",
    "\n",
    "sae_results[demo_pattern] = sae_result\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nSAE Analysis Results for {demo_pattern}:\")\n",
    "for layer_key, layer_result in sae_result['layers'].items():\n",
    "    print(f\"\\n  {layer_key}:\")\n",
    "    print(f\"    Sparsity: {layer_result['sparsity']:.3f}\")\n",
    "    print(f\"    Reconstruction error: {layer_result['reconstruction_error']:.3f}\")\n",
    "    print(f\"    Top 3 features:\")\n",
    "    for i, (feat_idx, activation) in enumerate(layer_result['top_features'][:3]):\n",
    "        interpretation = layer_result['interpretations'].get(feat_idx, \"No interpretation\")\n",
    "        print(f\"      Feature {feat_idx}: {activation:.3f} - {interpretation}\")\n",
    "\n",
    "print(\"\\n✅ SAE analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Selfie Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize selfie interpreter\n",
    "selfie_interpreter = SelfieInterpreter(model=activation_capturer.model)\n",
    "\n",
    "print(\"Running selfie interpretation analysis...\")\n",
    "\n",
    "# Analyze one pattern with limited samples for demo\n",
    "demo_pattern = list(cognitive_patterns.keys())[0]\n",
    "demo_strings = cognitive_patterns[demo_pattern][:5]  # Limit to 5 samples\n",
    "demo_activations = activations_dict[demo_pattern]\n",
    "\n",
    "print(f\"\\nSelfie analysis for: {demo_pattern}\")\n",
    "print(f\"Analyzing {len(demo_strings)} samples...\")\n",
    "\n",
    "# Limit to one layer for demo\n",
    "layer_keys = list(demo_activations.keys())[:1]\n",
    "limited_activations = {k: demo_activations[k][:5] for k in layer_keys}\n",
    "\n",
    "selfie_interpretations = selfie_interpreter.batch_interpret_activations(\n",
    "    activations=limited_activations,\n",
    "    contexts=demo_strings,\n",
    "    pattern_name=demo_pattern\n",
    ")\n",
    "\n",
    "# Display interpretations\n",
    "print(f\"\\nSelfie Interpretations for {demo_pattern}:\")\n",
    "for layer_key, interpretations in selfie_interpretations.items():\n",
    "    print(f\"\\n  {layer_key}:\")\n",
    "    for i, interpretation in enumerate(interpretations[:3]):\n",
    "        print(f\"    Sample {i+1}: {interpretation[:200]}...\")\n",
    "\n",
    "# Validate interpretations\n",
    "validation_results = {}\n",
    "for layer_key, layer_interpretations in selfie_interpretations.items():\n",
    "    validation = selfie_interpreter.validate_interpretations(\n",
    "        layer_interpretations, demo_pattern\n",
    "    )\n",
    "    validation_results[layer_key] = validation\n",
    "    print(f\"\\nValidation for {layer_key}:\")\n",
    "    print(f\"  Keyword match ratio: {validation['keyword_match_ratio']:.2f}\")\n",
    "    print(f\"  Avg interpretation length: {validation['avg_interpretation_length']:.0f} chars\")\n",
    "\n",
    "print(\"\\n✅ Selfie interpretation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Activation Arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize activation arithmetic\n",
    "activation_arithmetic = ActivationArithmetic()\n",
    "\n",
    "print(\"Running activation arithmetic analysis...\")\n",
    "\n",
    "# Prepare data for arithmetic operations\n",
    "layers = config['model']['layers']\n",
    "pattern_activations = {}\n",
    "\n",
    "for pattern_name, activations in activations_dict.items():\n",
    "    pattern_activations[pattern_name] = {}\n",
    "    for layer_num in layers:\n",
    "        layer_key = f\"{pattern_name}_layer_{layer_num}\"\n",
    "        if layer_key in activations:\n",
    "            pattern_activations[pattern_name][layer_num] = activations[layer_key]\n",
    "\n",
    "print(f\"Prepared activations for {len(pattern_activations)} patterns\")\n",
    "\n",
    "# Compute similarity matrices for each layer\n",
    "arithmetic_results = {}\n",
    "for layer_num in layers:\n",
    "    print(f\"\\nAnalyzing layer {layer_num}:\")\n",
    "    \n",
    "    # Get patterns that have data for this layer\n",
    "    layer_patterns = {}\n",
    "    for pattern_name, pattern_data in pattern_activations.items():\n",
    "        if layer_num in pattern_data:\n",
    "            layer_patterns[pattern_name] = pattern_data[layer_num]\n",
    "    \n",
    "    if len(layer_patterns) > 1:\n",
    "        # Compute similarity matrix\n",
    "        similarities = activation_arithmetic.compute_similarity_matrix(layer_patterns)\n",
    "        \n",
    "        print(f\"  Pattern similarities (cosine):\")\n",
    "        for (p1, p2), sim in similarities.items():\n",
    "            if p1 < p2:  # Only show each pair once\n",
    "                print(f\"    {p1} <-> {p2}: {sim:.3f}\")\n",
    "        \n",
    "        arithmetic_results[f\"layer_{layer_num}\"] = {\n",
    "            'similarities': similarities\n",
    "        }\n",
    "    else:\n",
    "        print(f\"  Not enough patterns for comparison\")\n",
    "\n",
    "# Pattern arithmetic operations (if multiple patterns available)\n",
    "pattern_names = list(pattern_activations.keys())\n",
    "if len(pattern_names) >= 2:\n",
    "    print(f\"\\nPattern arithmetic operations:\")\n",
    "    \n",
    "    # Choose first layer and first two patterns\n",
    "    demo_layer = layers[0]\n",
    "    pattern1, pattern2 = pattern_names[0], pattern_names[1]\n",
    "    \n",
    "    if (demo_layer in pattern_activations[pattern1] and \n",
    "        demo_layer in pattern_activations[pattern2]):\n",
    "        \n",
    "        act1 = pattern_activations[pattern1][demo_layer]\n",
    "        act2 = pattern_activations[pattern2][demo_layer]\n",
    "        \n",
    "        # Compute difference vector\n",
    "        diff_vector = activation_arithmetic.compute_pattern_difference(act1, act2)\n",
    "        print(f\"  Difference vector ({pattern1} - {pattern2}): shape {diff_vector.shape}\")\n",
    "        print(f\"  Difference magnitude: {torch.norm(diff_vector).item():.3f}\")\n",
    "        \n",
    "        # Compute transition vector\n",
    "        transition = activation_arithmetic.find_transition_vector(act1, act2)\n",
    "        print(f\"  Transition vector ({pattern1} -> {pattern2}): magnitude {torch.norm(transition).item():.3f}\")\n",
    "        \n",
    "        # Create interpolation\n",
    "        interpolated = activation_arithmetic.interpolate_patterns(act1, act2, steps=5)\n",
    "        print(f\"  Created {len(interpolated)} interpolation steps\")\n",
    "\n",
    "print(\"\\n✅ Activation arithmetic complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary of all results\n",
    "print(\"=\" * 60)\n",
    "print(\"COGNITIVE PATTERN ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nModel: {config['model']['name']}\")\n",
    "print(f\"Local path: {config['model']['local_path']}\")\n",
    "print(f\"Layers analyzed: {config['model']['layers']}\")\n",
    "\n",
    "print(f\"\\nPatterns analyzed:\")\n",
    "for pattern_name, activations in activations_dict.items():\n",
    "    activation_keys = list(activations.keys())\n",
    "    sample_shape = list(activations.values())[0].shape if activations else \"N/A\"\n",
    "    print(f\"  - {pattern_name}: {len(activation_keys)} layer combinations, shape: {sample_shape}\")\n",
    "\n",
    "print(f\"\\nAnalysis methods completed:\")\n",
    "completed_methods = []\n",
    "if pca_results:\n",
    "    completed_methods.append(\"✅ PCA Analysis\")\n",
    "if sae_results:\n",
    "    completed_methods.append(\"✅ SAE Analysis (placeholder)\")\n",
    "if 'selfie_interpretations' in locals():\n",
    "    completed_methods.append(\"✅ Selfie Interpretation\")\n",
    "if arithmetic_results:\n",
    "    completed_methods.append(\"✅ Activation Arithmetic\")\n",
    "\n",
    "for method in completed_methods:\n",
    "    print(f\"  {method}\")\n",
    "\n",
    "print(f\"\\nKey findings:\")\n",
    "print(f\"  - Successfully captured activations from {len(layers)} layers\")\n",
    "print(f\"  - PCA analysis shows variance structure across cognitive patterns\")\n",
    "if len(pattern_names) >= 2 and arithmetic_results:\n",
    "    # Find highest similarity\n",
    "    all_similarities = []\n",
    "    for layer_result in arithmetic_results.values():\n",
    "        for (p1, p2), sim in layer_result['similarities'].items():\n",
    "            if p1 != p2:\n",
    "                all_similarities.append(sim)\n",
    "    if all_similarities:\n",
    "        max_sim = max(all_similarities)\n",
    "        min_sim = min(all_similarities)\n",
    "        print(f\"  - Pattern similarities range from {min_sim:.3f} to {max_sim:.3f}\")\n",
    "\n",
    "print(f\"\\n🎯 Analysis pipeline completed successfully!\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Download actual SAE models to replace placeholder\")\n",
    "print(f\"  2. Experiment with different layers and cognitive patterns\")\n",
    "print(f\"  3. Use activation arithmetic for pattern steering\")\n",
    "print(f\"  4. Validate interpretations with domain experts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save results to files\n",
    "save_results = input(\"Save results to files? (y/n): \").strip().lower() == 'y'\n",
    "\n",
    "if save_results:\n",
    "    # Create results directory\n",
    "    results_dir = Path(\"./notebook_results\")\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"Saving results to {results_dir}...\")\n",
    "    \n",
    "    # Save activations\n",
    "    for pattern_name, activations in activations_dict.items():\n",
    "        torch.save(activations, results_dir / f\"{pattern_name}_activations.pt\")\n",
    "    \n",
    "    # Save PCA results\n",
    "    if pca_results:\n",
    "        torch.save(pca_results, results_dir / \"pca_results.pt\")\n",
    "    \n",
    "    # Save arithmetic results\n",
    "    if arithmetic_results:\n",
    "        with open(results_dir / \"arithmetic_results.yaml\", 'w') as f:\n",
    "            yaml.dump(arithmetic_results, f)\n",
    "    \n",
    "    print(f\"✅ Results saved to {results_dir}\")\n",
    "else:\n",
    "    print(\"Results not saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}