{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAE Analysis: Cognitive Patterns → Features → Neuronpedia\n",
    "\n",
    "This notebook demonstrates how to analyze your pre-computed activations using Sparse Autoencoders (SAEs) and explore the results through Neuronpedia.\n",
    "\n",
    "## Overview\n",
    "1. **Load SAE**: Find and load appropriate SAE for your model/layer\n",
    "2. **Process Activations**: Run your activations through the SAE\n",
    "3. **Find Top Features**: Identify most active features\n",
    "4. **Neuronpedia Integration**: View feature interpretations\n",
    "5. **Advanced Analysis**: Steering, ablation, comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install sae-lens transformer-lens plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from IPython.display import IFrame, display, HTML\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Import our SAE analysis module\n",
    "from sae_analysis import SAEActivationAnalyzer, SAEAnalysisConfig\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {'MPS' if torch.backends.mps.is_available() else 'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure analysis\n",
    "config = SAEAnalysisConfig(\n",
    "    device=\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    sae_release=\"gpt2-small-res-jb\",  # Start with GPT-2 SAEs\n",
    "    sae_id=\"blocks.7.hook_resid_pre\",   # Layer 7 residual stream\n",
    "    top_k_features=20,\n",
    "    analysis_output_dir=\"sae_analysis_results\"\n",
    ")\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = SAEActivationAnalyzer(config)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Device: {config.device}\")\n",
    "print(f\"  SAE: {config.sae_release}/{config.sae_id}\")\n",
    "print(f\"  Output directory: {config.analysis_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Discover Available SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover all available SAEs\n",
    "available_saes = analyzer.discover_available_saes()\n",
    "\n",
    "# Show available models\n",
    "model_counts = available_saes['model_name'].value_counts()\n",
    "print(\"Available SAE models:\")\n",
    "for model, count in model_counts.head(10).items():\n",
    "    print(f\"  {model}: {count} SAEs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find SAEs for specific models\n",
    "gpt2_saes = analyzer.find_matching_saes(\"gpt2\", \"resid\")\n",
    "display(gpt2_saes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SAE\n",
    "sae = analyzer.load_sae()\n",
    "\n",
    "print(f\"SAE Configuration:\")\n",
    "print(f\"  Hook name: {sae.cfg.hook_name}\")\n",
    "print(f\"  Input dims (d_in): {sae.cfg.d_in}\")\n",
    "print(f\"  SAE dims (d_sae): {sae.cfg.d_sae}\")\n",
    "print(f\"  Expansion factor: {sae.cfg.d_sae / sae.cfg.d_in:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load Your Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find your activation files\n",
    "activation_files = list(Path(\"activations\").glob(\"*.pt\"))\n",
    "print(f\"Found {len(activation_files)} activation files:\")\n",
    "for i, file_path in enumerate(activation_files):\n",
    "    print(f\"  {i}: {file_path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which activation file to analyze\n",
    "file_index = 0  # Change this to select different files\n",
    "activation_path = activation_files[file_index]\n",
    "\n",
    "print(f\"Loading: {activation_path}\")\n",
    "activations = analyzer.load_activations(str(activation_path))\n",
    "\n",
    "print(f\"Activation statistics:\")\n",
    "print(f\"  Shape: {activations.shape}\")\n",
    "print(f\"  Device: {activations.device}\")\n",
    "print(f\"  Dtype: {activations.dtype}\")\n",
    "print(f\"  Min/Max: {activations.min():.4f} / {activations.max():.4f}\")\n",
    "print(f\"  Mean/Std: {activations.mean():.4f} ± {activations.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle dimension compatibility\n",
    "original_shape = activations.shape\n",
    "actual_dim = activations.shape[-1]\n",
    "expected_dim = sae.cfg.d_in\n",
    "\n",
    "print(f\"Dimension compatibility check:\")\n",
    "print(f\"  Activation dims: {actual_dim}\")\n",
    "print(f\"  SAE expects: {expected_dim}\")\n",
    "\n",
    "if actual_dim != expected_dim:\n",
    "    print(f\"  ⚠️  Dimension mismatch detected!\")\n",
    "    \n",
    "    if actual_dim > expected_dim:\n",
    "        print(f\"  Truncating to first {expected_dim} dimensions\")\n",
    "        activations = activations[..., :expected_dim]\n",
    "    else:\n",
    "        print(f\"  Padding to {expected_dim} dimensions with zeros\")\n",
    "        padding_shape = list(activations.shape)\n",
    "        padding_shape[-1] = expected_dim - actual_dim\n",
    "        padding = torch.zeros(padding_shape, device=activations.device, dtype=activations.dtype)\n",
    "        activations = torch.cat([activations, padding], dim=-1)\n",
    "        \n",
    "    print(f\"  New shape: {activations.shape}\")\n",
    "else:\n",
    "    print(f\"  ✅ Dimensions compatible!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Process Through SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process activations through SAE\n",
    "results = analyzer.process_activations(activations)\n",
    "\n",
    "print(\"Processing results:\")\n",
    "for key, tensor in results.items():\n",
    "    if isinstance(tensor, torch.Tensor):\n",
    "        print(f\"  {key}: {tensor.shape} - {tensor.dtype}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(tensor)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Find Top Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top activating features\n",
    "values, indices = analyzer.find_top_features(results['feature_activations'], \n",
    "                                           position=-1,  # Last token position\n",
    "                                           top_k=config.top_k_features)\n",
    "\n",
    "# Create a dataframe for easier viewing\n",
    "top_features_df = pd.DataFrame({\n",
    "    'rank': range(1, len(values) + 1),\n",
    "    'feature_idx': indices.cpu().numpy(),\n",
    "    'activation_value': values.cpu().numpy()\n",
    "})\n",
    "\n",
    "display(top_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Feature Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig = analyzer.visualize_feature_activations(\n",
    "    results['feature_activations'], \n",
    "    position=-1,\n",
    "    title=f\"Feature Analysis: {activation_path.name}\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional visualization: Top features bar chart\n",
    "fig_bar = go.Figure(data=[\n",
    "    go.Bar(\n",
    "        x=[f\"F{idx}\" for idx in indices[:10].cpu()],\n",
    "        y=values[:10].cpu(),\n",
    "        text=[f\"{val:.3f}\" for val in values[:10].cpu()],\n",
    "        textposition='outside'\n",
    "    )\n",
    "])\n",
    "\n",
    "fig_bar.update_layout(\n",
    "    title=\"Top 10 Feature Activations\",\n",
    "    xaxis_title=\"Feature Index\",\n",
    "    yaxis_title=\"Activation Value\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig_bar.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Neuronpedia Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Neuronpedia URLs for top features\n",
    "print(\"Neuronpedia Dashboard URLs:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i, (val, idx) in enumerate(zip(values[:10], indices[:10])):\n",
    "    url = analyzer.get_neuronpedia_dashboard_url(int(idx))\n",
    "    print(f\"{i+1:2d}. Feature {int(idx):4d} (activation: {float(val):6.4f})\")\n",
    "    print(f\"    {url}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display interactive Neuronpedia dashboards for top 3 features\n",
    "print(\"Interactive Neuronpedia Dashboards:\")\n",
    "\n",
    "for i in range(min(3, len(indices))):\n",
    "    feature_idx = int(indices[i])\n",
    "    activation_val = float(values[i])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Feature {feature_idx} - Activation: {activation_val:.4f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create and display iframe\n",
    "    dashboard = analyzer.display_feature_dashboard(feature_idx, width=1200, height=400)\n",
    "    if dashboard:\n",
    "        display(dashboard)\n",
    "    else:\n",
    "        url = analyzer.get_neuronpedia_dashboard_url(feature_idx)\n",
    "        display(HTML(f'<a href=\"{url}\" target=\"_blank\">Open Feature {feature_idx} in new tab</a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Download and Search Feature Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download feature explanations\n",
    "try:\n",
    "    explanations = analyzer.download_feature_explanations(\"gpt2-small\", \"7-res-jb\")\n",
    "    if explanations is not None:\n",
    "        print(f\"Successfully downloaded {len(explanations)} feature explanations\")\n",
    "        print(\"\\nSample explanations:\")\n",
    "        display(explanations[['feature', 'description']].head())\n",
    "    else:\n",
    "        print(\"Could not download explanations\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading explanations: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for features related to cognitive patterns\n",
    "if analyzer.feature_explanations is not None:\n",
    "    search_terms = [\n",
    "        \"thought\", \"thinking\", \"cognitive\", \"mental\", \"mind\", \n",
    "        \"emotion\", \"feeling\", \"pattern\", \"behavior\", \"psychology\",\n",
    "        \"negative\", \"positive\", \"anxiety\", \"depression\", \"stress\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Searching for cognitively relevant features:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    all_matches = []\n",
    "    for term in search_terms:\n",
    "        matches = analyzer.search_features_by_description(term)\n",
    "        if matches is not None and len(matches) > 0:\n",
    "            print(f\"\\n'{term}': {len(matches)} matches\")\n",
    "            if len(matches) > 0:\n",
    "                example = matches.iloc[0]\n",
    "                print(f\"  Example: Feature {example['feature']} - {example['description'][:100]}...\")\n",
    "                all_matches.extend(matches['feature'].tolist())\n",
    "    \n",
    "    # Check if any of your top features match cognitive patterns\n",
    "    top_feature_indices = [int(idx) for idx in indices[:10]]\n",
    "    cognitive_matches = [f for f in top_feature_indices if f in all_matches]\n",
    "    \n",
    "    if cognitive_matches:\n",
    "        print(f\"\\n🎯 Cognitive pattern matches in your top features:\")\n",
    "        for feature_idx in cognitive_matches:\n",
    "            explanation_row = analyzer.feature_explanations[\n",
    "                analyzer.feature_explanations['feature'] == feature_idx\n",
    "            ]\n",
    "            if not explanation_row.empty:\n",
    "                desc = explanation_row.iloc[0]['description']\n",
    "                rank = top_feature_indices.index(feature_idx) + 1\n",
    "                print(f\"  Rank {rank} - Feature {feature_idx}: {desc}\")\n",
    "else:\n",
    "    print(\"No explanations available for searching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Advanced Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Ablation Analysis\n",
    "print(\"Feature Ablation Analysis\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Ablate top 3 features\n",
    "top_3_features = [int(idx) for idx in indices[:3]]\n",
    "print(f\"Ablating features: {top_3_features}\")\n",
    "\n",
    "ablated_reconstructions = analyzer.perform_feature_ablation(\n",
    "    results['feature_activations'], \n",
    "    top_3_features\n",
    ")\n",
    "\n",
    "# Compare reconstruction quality\n",
    "original_error = torch.nn.functional.mse_loss(activations, results['reconstructions'])\n",
    "ablated_error = torch.nn.functional.mse_loss(activations, ablated_reconstructions)\n",
    "error_increase = ablated_error - original_error\n",
    "\n",
    "print(f\"\\nReconstruction Analysis:\")\n",
    "print(f\"  Original MSE: {original_error:.6f}\")\n",
    "print(f\"  Ablated MSE:  {ablated_error:.6f}\")\n",
    "print(f\"  Error increase: {error_increase:.6f} ({error_increase/original_error*100:.1f}% worse)\")\n",
    "\n",
    "if error_increase > 0.001:  # Significant increase\n",
    "    print(f\"  🔍 These features appear important for reconstruction!\")\n",
    "else:\n",
    "    print(f\"  💭 These features may be less critical for reconstruction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Steering Analysis\n",
    "print(\"Feature Steering Analysis\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Create steering interventions\n",
    "steering_configs = [\n",
    "    (int(indices[0]), 2.0),   # Amplify top feature\n",
    "    (int(indices[1]), -1.0),  # Suppress second feature\n",
    "    (int(indices[2]), 1.5)    # Moderately amplify third feature\n",
    "]\n",
    "\n",
    "print(f\"Steering interventions:\")\n",
    "for feature_idx, strength in steering_configs:\n",
    "    direction = \"Amplify\" if strength > 0 else \"Suppress\"\n",
    "    print(f\"  Feature {feature_idx}: {direction} by {abs(strength):.1f}x\")\n",
    "\n",
    "# Apply steering\n",
    "steered_activations = analyzer.apply_steering(activations, steering_configs)\n",
    "steered_results = analyzer.process_activations(steered_activations)\n",
    "\n",
    "# Compare before/after\n",
    "print(f\"\\nBefore/After Steering Comparison:\")\n",
    "print(f\"  Original avg L0:  {results['l0_norm'].mean():.2f}\")\n",
    "print(f\"  Steered avg L0:   {steered_results['l0_norm'].mean():.2f}\")\n",
    "print(f\"  Change in sparsity: {steered_results['l0_norm'].mean() - results['l0_norm'].mean():.2f}\")\n",
    "\n",
    "# Find top features after steering\n",
    "steered_values, steered_indices = analyzer.find_top_features(\n",
    "    steered_results['feature_activations'], top_k=10\n",
    ")\n",
    "\n",
    "print(f\"\\nTop features after steering:\")\n",
    "for i, (val, idx) in enumerate(zip(steered_values[:5], steered_indices[:5])):\n",
    "    original_rank = \"NEW\" if int(idx) not in indices[:10].tolist() else f\"#{indices[:10].tolist().index(int(idx)) + 1}\"\n",
    "    print(f\"  {i+1}. Feature {int(idx)} ({original_rank}): {float(val):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative visualization\n",
    "fig = analyzer.compare_activations(\n",
    "    results['feature_activations'][0, -1, :], \n",
    "    steered_results['feature_activations'][0, -1, :],\n",
    "    labels=(\"Original\", \"Steered\")\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Save Results and Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive results\n",
    "timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "file_stem = activation_path.stem\n",
    "\n",
    "# Save analysis results\n",
    "results_filename = f\"analysis_results_{file_stem}_{timestamp}.pt\"\n",
    "results_path = analyzer.save_analysis_results(results, results_filename)\n",
    "\n",
    "# Generate report\n",
    "report_filename = f\"analysis_report_{file_stem}_{timestamp}.md\"\n",
    "report_path = analyzer.generate_analysis_report(results, (values, indices), report_filename)\n",
    "\n",
    "# Create interactive summary\n",
    "summary = {\n",
    "    \"metadata\": {\n",
    "        \"activation_file\": str(activation_path),\n",
    "        \"original_shape\": list(original_shape),\n",
    "        \"processed_shape\": list(activations.shape),\n",
    "        \"analysis_timestamp\": timestamp,\n",
    "        \"sae_config\": {\n",
    "            \"release\": config.sae_release,\n",
    "            \"sae_id\": config.sae_id,\n",
    "            \"d_in\": sae.cfg.d_in,\n",
    "            \"d_sae\": sae.cfg.d_sae,\n",
    "            \"hook_name\": sae.cfg.hook_name\n",
    "        }\n",
    "    },\n",
    "    \"statistics\": {\n",
    "        \"avg_sparsity\": float(results['l0_norm'].mean()),\n",
    "        \"avg_reconstruction_error\": float(results['reconstruction_error'].mean()),\n",
    "        \"activation_range\": {\n",
    "            \"min\": float(activations.min()),\n",
    "            \"max\": float(activations.max()),\n",
    "            \"mean\": float(activations.mean()),\n",
    "            \"std\": float(activations.std())\n",
    "        }\n",
    "    },\n",
    "    \"top_features\": [\n",
    "        {\n",
    "            \"rank\": i + 1,\n",
    "            \"feature_idx\": int(idx),\n",
    "            \"activation_value\": float(val),\n",
    "            \"neuronpedia_url\": analyzer.get_neuronpedia_dashboard_url(int(idx))\n",
    "        }\n",
    "        for i, (val, idx) in enumerate(zip(values, indices))\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Add explanations if available\n",
    "if analyzer.feature_explanations is not None:\n",
    "    for feature_data in summary[\"top_features\"]:\n",
    "        feature_idx = feature_data[\"feature_idx\"]\n",
    "        explanation_row = analyzer.feature_explanations[\n",
    "            analyzer.feature_explanations['feature'] == feature_idx\n",
    "        ]\n",
    "        if not explanation_row.empty:\n",
    "            feature_data[\"description\"] = explanation_row.iloc[0]['description']\n",
    "\n",
    "# Save summary\n",
    "summary_path = Path(config.analysis_output_dir) / f\"interactive_summary_{file_stem}_{timestamp}.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"Analysis Complete! 🎉\")\n",
    "print(f\"\\nGenerated files:\")\n",
    "print(f\"  📊 Results: {results_path.name}\")\n",
    "print(f\"  📝 Report: {report_path.name}\")\n",
    "print(f\"  📋 Summary: {summary_path.name}\")\n",
    "print(f\"\\nAll files saved in: {config.analysis_output_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We Accomplished:\n",
    "1. ✅ Loaded a pre-trained SAE matching your model architecture\n",
    "2. ✅ Processed your pre-computed activations through the SAE  \n",
    "3. ✅ Identified the most active features for your cognitive patterns\n",
    "4. ✅ Generated Neuronpedia dashboard links for feature interpretation\n",
    "5. ✅ Performed advanced analysis (ablation, steering)\n",
    "6. ✅ Saved comprehensive results and generated reports\n",
    "\n",
    "### Key Insights:\n",
    "- Your activations were successfully processed through the SAE\n",
    "- Identified top active features that may relate to cognitive patterns\n",
    "- Generated interpretable visualizations and Neuronpedia links\n",
    "- Demonstrated feature manipulation techniques\n",
    "\n",
    "### Next Steps:\n",
    "1. **Explore Neuronpedia**: Click the dashboard URLs to understand what each feature represents\n",
    "2. **Compare Patterns**: Run this analysis on different activation files to compare cognitive patterns\n",
    "3. **Feature Analysis**: Investigate which features consistently activate across similar cognitive patterns\n",
    "4. **Model Understanding**: Use steering/ablation to understand which features are most important\n",
    "5. **Research Applications**: Use these insights to understand the neural basis of cognitive transformations\n",
    "\n",
    "### Troubleshooting:\n",
    "- If Neuronpedia dashboards don't load, try opening the URLs directly in a new browser tab\n",
    "- For dimension mismatches, adjust the SAE selection or activation preprocessing\n",
    "- If explanations fail to download, the Neuronpedia API might be temporarily unavailable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}