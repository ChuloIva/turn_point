{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Transition Capture with RepEng\n",
    "\n",
    "This notebook demonstrates how to capture the transition from negative (depressive/sad) to positive (happy/optimistic) sentiment using RepEng control vectors. The goal is to:\n",
    "\n",
    "1. Use depressive steering to generate partial responses to cognitive pattern questions\n",
    "2. Switch to positive steering to complete the responses\n",
    "3. Capture and record this transition for downstream analysis\n",
    "4. Export vectors in .gguf format for use with GGUF models\n",
    "\n",
    "Based on RepEng examples, we'll use the `pca_center` method for more precise steering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if running in Colab\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install torch transformers sklearn numpy tqdm gguf\n",
    "    !git clone https://github.com/vgel/repeng.git\n",
    "    sys.path.append('/content/repeng')\n",
    "    \n",
    "    # Download the cognitive pattern questions file\n",
    "    import urllib.request\n",
    "    urllib.request.urlretrieve(\n",
    "        'https://raw.githubusercontent.com/your-repo/cognitive_pattern_questions.md',\n",
    "        'cognitive_pattern_questions.md'\n",
    "    )\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from repeng import ControlVector, ControlModel, DatasetEntry\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "\n",
    "We'll use Mistral-7B-Instruct as our base model, following the RepEng emotion example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "device = (\n",
    "    \"cuda:0\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps:0\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Wrap with ControlModel - using layers -5 to -18 as in the emotion example\n",
    "model = ControlModel(model, list(range(-5, -18, -1)))\n",
    "\n",
    "# Chat templates\n",
    "user_tag, asst_tag = \"[INST]\", \"[/INST]\"\n",
    "\n",
    "print(f\"Model loaded on {device}\")\n",
    "print(f\"Control layers: {list(range(-5, -18, -1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Cognitive Pattern Questions\n",
    "\n",
    "We'll extract questions from the cognitive pattern assessment file to use as prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cognitive_questions(filepath: str) -> Dict[str, List[str]]:\n",
    "    \"\"\"Load and parse cognitive pattern questions from markdown file.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "    except FileNotFoundError:\n",
    "        # Fallback for Colab - create sample questions\n",
    "        print(\"Cognitive questions file not found. Using sample questions.\")\n",
    "        return {\n",
    "            \"Suicidal Ideation\": [\n",
    "                \"What thoughts occupy most of your mental space throughout the day?\",\n",
    "                \"How often do thoughts about death or dying enter your consciousness?\",\n",
    "                \"What happens when you attempt to push certain thoughts away?\"\n",
    "            ],\n",
    "            \"Hopelessness\": [\n",
    "                \"How difficult has it become to imagine any positive changes in your future?\",\n",
    "                \"What happens to your energy when you try to think about tomorrow?\",\n",
    "                \"How exhausting is it to contemplate trying to improve things?\"\n",
    "            ],\n",
    "            \"Executive Fatigue\": [\n",
    "                \"How difficult has it become to start simple daily tasks?\",\n",
    "                \"What happens when you try to make yourself do basic things like showering?\",\n",
    "                \"How much mental effort does it take to begin routine activities?\"\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    # Parse the markdown to extract questions by category\n",
    "    categories = {}\n",
    "    current_category = None\n",
    "    \n",
    "    lines = content.split('\\n')\n",
    "    for line in lines:\n",
    "        # Check for category headers (## format)\n",
    "        if line.startswith('## '):\n",
    "            # Extract category name (remove number and clean up)\n",
    "            category_match = re.search(r'##\\s*\\d+\\.\\s*(.+?)(?:\\s*\\*|$)', line)\n",
    "            if category_match:\n",
    "                current_category = category_match.group(1).strip()\n",
    "                categories[current_category] = []\n",
    "        \n",
    "        # Check for numbered questions\n",
    "        elif current_category and re.match(r'^\\d+\\. ', line):\n",
    "            question = re.sub(r'^\\d+\\. ', '', line).strip()\n",
    "            if question:\n",
    "                categories[current_category].append(question)\n",
    "    \n",
    "    return categories\n",
    "\n",
    "# Load the questions\n",
    "questions_by_category = load_cognitive_questions('cognitive_pattern_questions.md')\n",
    "\n",
    "print(\"Loaded question categories:\")\n",
    "for category, questions in questions_by_category.items():\n",
    "    print(f\"- {category}: {len(questions)} questions\")\n",
    "\n",
    "# Flatten all questions for easier access\n",
    "all_questions = []\n",
    "for questions in questions_by_category.values():\n",
    "    all_questions.extend(questions)\n",
    "\n",
    "print(f\"\\nTotal questions: {len(all_questions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Dataset for Control Vectors\n",
    "\n",
    "We'll create datasets for both depressive and positive steering using various response starters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the suffixes from RepEng data - same as in the emotion example\ndef load_truncated_outputs():\n    \"\"\"Load truncated outputs from RepEng data file.\"\"\"\n    try:\n        # Try to load from local RepEng installation\n        with open('/content/repeng/notebooks/data/all_truncated_outputs.json', 'r') as f:\n            return json.load(f)\n    except FileNotFoundError:\n        try:\n            # Try alternative path for local development\n            with open('repeng/repeng/notebooks/data/all_truncated_outputs.json', 'r') as f:\n                return json.load(f)\n        except FileNotFoundError:\n            # Fallback: create a subset based on the original data structure\n            print(\"RepEng data file not found. Using fallback dataset.\")\n            return [\n                \"\", \"That game\", \"I can see\", \"Hmm, this\", \"I can relate to\", \"Who is\",\n                \"I understand the\", \"Ugh,\", \"What the hell was\", \"Hey, did anyone\",\n                \"Although\", \"Thank you for choosing\", \"What are you\", \"Oh w\",\n                \"How dare you open\", \"It was my pleasure\", \"I'm hon\", \"I appreciate that you\",\n                \"Are you k\", \"Whoever left this\", \"It's always\", \"Ew,\", \"Hey, I l\",\n                \"Hello? Is someone\", \"I understand that\", \"That poem\", \"Aww, poor\",\n                \"Hey, it\", \"Alright, who\", \"I didn't\", \"Well, life\", \"The document\",\n                \"Oh no, this\", \"I'm concerned\", \"Hello, this is\", \"This art\",\n                \"Hmm, this drink\", \"Hi there!\", \"It seems\", \"Is\", \"Good\", \"I can't\",\n                \"Ex\", \"Who are\", \"I can see that\", \"Wow,\", \"Today is a\", \"Hey friend\",\n                \"Sometimes friends\", \"Oh, this old\", \"The weather outside\",\n                \"This place is sur\", \"I appreciate your input\", \"Thank you for the\",\n                \"Look at\", \"I'm disappoint\", \"To my\", \"How dare you\", \"That's an\",\n                \"This piece of art\", \"Eww\", \"This park is\", \"This is incredible\",\n                \"Oh no, someone\", \"Exc\", \"Well, it'\", \"I warned\", \"Hey, I understand\",\n                \"Hey, I saw\", \"How dare you go\", \"What the he\", \"Hey\", \"It's\",\n                \"Hello? Hello?\", \"It\", \"Oh no!\", \"This is the perfect\",\n                \"Good morning,\", \"Oh no, there\", \"It's so\", \"Yeah\", \"Uh,\",\n                \"Hello everyone\", \"Who turned off\", \"The weather\", \"Who'\", \"Hey, this\",\n                \"Wait,\", \"Eww, gross\", \"Excuse\", \"It seems like you\", \"Thank you so\",\n                \"What happened?\", \"Oh my g\", \"I am deeply sad\", \"I war\", \"Okay, let'\",\n                \"Hey, that\", \"That was a beautiful\", \"Oh no! That\", \"What happened\",\n                \"Hey there\", \"The artist'\", \"What?!\", \"Hey, it'\", \"I am disappoint\",\n                \"It seems like\", \"Oh no! The\", \"This park is a\", \"If you\", \"Yes! I did\",\n                \"It sounds\", \"What\", \"Who is it\", \"Hmm, that\", \"That's strange\",\n                \"Yeah, that was\", \"That's interesting\", \"This park\", \"What the hell\",\n                \"Who is that\", \"I feel like my\", \"Oh well\", \"What the hell is\",\n                \"Hello? Hello\", \"To my dearest\", \"Bless you!\\\"\", \"Thank you for\",\n                \"Oh, looks like\", \"Can you please\", \"This place is\", \"Eww, what\",\n                \"Bless you\", \"Is everything\", \"Hey, I just\", \"Whoever left these\",\n                \"Well, that'\", \"I feel\", \"Hey, do you\", \"It's sad\", \"Oh no, it\",\n                \"Hey, that'\", \"Oh my god,\", \"Thank you,\", \"Hello little one,\",\n                \"I apolog\", \"Hey team, I\", \"How dare you read\", \"Who is this and\"\n            ]\n\n# Load the response suffixes from RepEng data\nsuffixes = load_truncated_outputs()\nprint(f\"Loaded {len(suffixes)} response suffixes from RepEng data\")\nprint(f\"Sample suffixes: {suffixes[:10]}\")\n\n# Personas for steering - exactly like RepEng emotion example  \npositive_personas = [\"happy\", \"ecstatic\", \"delighted\"]\nnegative_personas = [\"sad\", \"depressed\", \"dismayed\"]\n\ndef template(persona: str, suffix: str) -> str:\n    \"\"\"Create template exactly like RepEng emotion example.\"\"\"\n    return f\"{user_tag} Act as if you're extremely {persona}. {asst_tag} {suffix}\"\n\ndef create_control_dataset() -> List[DatasetEntry]:\n    \"\"\"Create dataset using RepEng's exact approach from emotion example.\"\"\"\n    dataset = []\n    \n    # Follow the exact same pattern as RepEng emotion notebook\n    for suffix in suffixes:\n        tokens = tokenizer.tokenize(suffix)\n        for i in range(1, len(tokens)):\n            truncated = tokenizer.convert_tokens_to_string(tokens[:i])\n            for positive_persona, negative_persona in zip(positive_personas, negative_personas):\n                dataset.append(DatasetEntry(\n                    positive=template(positive_persona, truncated),\n                    negative=template(negative_persona, truncated)\n                ))\n    \n    return dataset\n\n# Create the dataset using RepEng approach\ncontrol_dataset = create_control_dataset()\nprint(f\"\\nCreated control dataset with {len(control_dataset)} entries\")\n\n# Show some examples (matching RepEng output format)\nprint(\"\\nSample training examples:\")\nfor i in range(3):\n    print(f\"dataset[{i}].positive:\", control_dataset[i].positive)\n    print(f\"dataset[{i}].negative:\", control_dataset[i].negative)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Control Vectors\n",
    "\n",
    "We'll train separate control vectors for depression and positivity using the `pca_center` method for more precise control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset model before training\n",
    "model.reset()\n",
    "\n",
    "print(\"Training control vector...\")\n",
    "control_vector = ControlVector.train(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    control_dataset,\n",
    "    method=\"pca_center\",  # More precise than pca_diff\n",
    "    max_batch_size=16     # Adjust based on your GPU memory\n",
    ")\n",
    "\n",
    "print(\"Control vector training completed!\")\n",
    "print(f\"Vector covers layers: {sorted(control_vector.directions.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Basic Steering\n",
    "\n",
    "Let's test our control vectors with a simple question to verify they work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_control(prompt: str, control_strength: float = 0.0, max_tokens: int = 100) -> str:\n",
    "    \"\"\"Generate text with optional control vector applied.\"\"\"\n",
    "    input_text = f\"{user_tag} {prompt} {asst_tag}\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    settings = {\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"do_sample\": False,  # Deterministic generation\n",
    "        \"max_new_tokens\": max_tokens,\n",
    "        \"repetition_penalty\": 1.1\n",
    "    }\n",
    "    \n",
    "    # Apply control\n",
    "    model.reset()\n",
    "    if control_strength != 0.0:\n",
    "        model.set_control(control_vector, control_strength)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**input_ids, **settings)\n",
    "    \n",
    "    # Decode and clean\n",
    "    full_text = tokenizer.decode(output.squeeze(), skip_special_tokens=True)\n",
    "    response_start = full_text.find(asst_tag) + len(asst_tag)\n",
    "    return full_text[response_start:].strip()\n",
    "\n",
    "# Test with a sample question\n",
    "test_question = \"How do you feel about your future?\"\n",
    "\n",
    "print(\"=== BASELINE (No Control) ===\")\n",
    "baseline_response = generate_with_control(test_question, control_strength=0.0)\n",
    "print(baseline_response)\n",
    "\n",
    "print(\"\\n=== DEPRESSIVE STEERING (Negative Control) ===\")\n",
    "depressive_response = generate_with_control(test_question, control_strength=-2.0)\n",
    "print(depressive_response)\n",
    "\n",
    "print(\"\\n=== POSITIVE STEERING (Positive Control) ===\")\n",
    "positive_response = generate_with_control(test_question, control_strength=2.0)\n",
    "print(positive_response)\n",
    "\n",
    "model.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Transition Capture System\n",
    "\n",
    "Now we'll implement the core functionality: generating partial responses with depressive steering, then completing them with positive steering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_sentiment_transition(\n",
    "    question: str,\n",
    "    depressive_strength: float = -2.0,\n",
    "    positive_strength: float = 2.0,\n",
    "    initial_tokens: int = 50,\n",
    "    completion_tokens: int = 80,\n",
    "    transition_point_range: Tuple[int, int] = (20, 40)\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Capture sentiment transition from depressive to positive.\n",
    "    \n",
    "    Args:\n",
    "        question: The question to ask\n",
    "        depressive_strength: Negative control strength \n",
    "        positive_strength: Positive control strength\n",
    "        initial_tokens: Max tokens for initial depressive generation\n",
    "        completion_tokens: Max tokens for positive completion\n",
    "        transition_point_range: Random range for transition point selection\n",
    "    \n",
    "    Returns:\n",
    "        Dict with 'question', 'depressive_start', 'positive_continuation', 'full_response'\n",
    "    \"\"\"\n",
    "    import random\n",
    "    \n",
    "    # Step 1: Generate initial response with depressive steering\n",
    "    input_text = f\"{user_tag} {question} {asst_tag}\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    model.reset()\n",
    "    model.set_control(control_vector, depressive_strength)\n",
    "    \n",
    "    settings = {\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_new_tokens\": initial_tokens,\n",
    "        \"repetition_penalty\": 1.1\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        depressive_output = model.generate(**input_ids, **settings)\n",
    "    \n",
    "    # Extract the depressive response\n",
    "    full_depressive = tokenizer.decode(depressive_output.squeeze(), skip_special_tokens=True)\n",
    "    response_start = full_depressive.find(asst_tag) + len(asst_tag)\n",
    "    depressive_text = full_depressive[response_start:].strip()\n",
    "    \n",
    "    # Step 2: Choose a random transition point in the depressive response\n",
    "    words = depressive_text.split()\n",
    "    if len(words) < transition_point_range[0]:\n",
    "        # If response is too short, use a smaller transition point\n",
    "        transition_point = len(words) // 2\n",
    "    else:\n",
    "        transition_point = random.randint(\n",
    "            min(transition_point_range[0], len(words)),\n",
    "            min(transition_point_range[1], len(words))\n",
    "        )\n",
    "    \n",
    "    # Split the response at transition point\n",
    "    depressive_start = ' '.join(words[:transition_point])\n",
    "    \n",
    "    # Step 3: Continue with positive steering from the transition point\n",
    "    continuation_prompt = f\"{input_text} {depressive_start}\"\n",
    "    continuation_ids = tokenizer(continuation_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    model.reset()\n",
    "    model.set_control(control_vector, positive_strength)\n",
    "    \n",
    "    settings[\"max_new_tokens\"] = completion_tokens\n",
    "    settings[\"temperature\"] = 0.8  # Slightly more creative for positive completion\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        positive_output = model.generate(**continuation_ids, **settings)\n",
    "    \n",
    "    # Extract the positive continuation\n",
    "    full_positive = tokenizer.decode(positive_output.squeeze(), skip_special_tokens=True)\n",
    "    \n",
    "    # Find where the new generation starts\n",
    "    continuation_start = len(continuation_prompt)\n",
    "    positive_continuation = full_positive[continuation_start:].strip()\n",
    "    \n",
    "    # Combine for full response\n",
    "    full_response = depressive_start + \" \" + positive_continuation\n",
    "    \n",
    "    model.reset()\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"depressive_start\": depressive_start,\n",
    "        \"positive_continuation\": positive_continuation,\n",
    "        \"full_response\": full_response,\n",
    "        \"transition_point\": transition_point,\n",
    "        \"total_words\": len(words)\n",
    "    }\n",
    "\n",
    "# Test the transition capture with a sample question\n",
    "sample_question = \"How do you feel about your future and what lies ahead?\"\n",
    "transition_result = capture_sentiment_transition(sample_question)\n",
    "\n",
    "print(f\"Question: {transition_result['question']}\")\n",
    "print(f\"\\nDepressive Start ({transition_result['transition_point']}/{transition_result['total_words']} words):\")\n",
    "print(f'\"{transition_result[\"depressive_start\"]}\"')\n",
    "print(f\"\\nPositive Continuation:\")\n",
    "print(f'\"{transition_result[\"positive_continuation\"]}\"')\n",
    "print(f\"\\nFull Transition:\")\n",
    "print(f'\"{transition_result[\"full_response\"]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Process Cognitive Pattern Questions\n",
    "\n",
    "Now let's process multiple questions from our cognitive pattern dataset to capture various sentiment transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_question_batch(\n",
    "    questions: List[str], \n",
    "    num_samples: int = 10,\n",
    "    save_results: bool = True\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Process a batch of questions to capture sentiment transitions.\n",
    "    \n",
    "    Args:\n",
    "        questions: List of questions to process\n",
    "        num_samples: Number of questions to sample and process\n",
    "        save_results: Whether to save results to JSON\n",
    "    \n",
    "    Returns:\n",
    "        List of transition results\n",
    "    \"\"\"\n",
    "    import random\n",
    "    \n",
    "    # Sample questions if we have more than requested\n",
    "    if len(questions) > num_samples:\n",
    "        selected_questions = random.sample(questions, num_samples)\n",
    "    else:\n",
    "        selected_questions = questions\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(f\"Processing {len(selected_questions)} questions...\")\n",
    "    \n",
    "    for i, question in enumerate(selected_questions):\n",
    "        print(f\"\\nProcessing question {i+1}/{len(selected_questions)}...\")\n",
    "        \n",
    "        try:\n",
    "            result = capture_sentiment_transition(\n",
    "                question=question,\n",
    "                depressive_strength=-2.5,  # Stronger depressive steering\n",
    "                positive_strength=2.0,     # Moderate positive steering\n",
    "                initial_tokens=60,         # Allow longer depressive responses\n",
    "                completion_tokens=100,     # Allow longer positive completions\n",
    "                transition_point_range=(15, 35)  # Transition in middle portion\n",
    "            )\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            # Show abbreviated result\n",
    "            print(f\"‚úì Transition captured (words: {result['transition_point']}/{result['total_words']})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error processing question: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if save_results and results:\n",
    "        output_file = \"sentiment_transitions.json\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\nResults saved to {output_file}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Process a sample of questions\n",
    "transition_results = process_question_batch(all_questions, num_samples=5)\n",
    "\n",
    "print(f\"\\nProcessed {len(transition_results)} questions successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Transition Results\n",
    "\n",
    "Let's examine the captured transitions in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_transitions(results: List[Dict[str, str]], max_display: int = 3):\n",
    "    \"\"\"\n",
    "    Display the transition results in a readable format.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"SENTIMENT TRANSITION RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, result in enumerate(results[:max_display]):\n",
    "        print(f\"\\n[EXAMPLE {i+1}]\")\n",
    "        print(f\"Question: {result['question']}\")\n",
    "        print(f\"Transition Point: {result['transition_point']}/{result['total_words']} words\")\n",
    "        \n",
    "        print(\"\\nüî¥ DEPRESSIVE START:\")\n",
    "        print(f'\"{result[\"depressive_start\"]}\"')\n",
    "        \n",
    "        print(\"\\nüü¢ POSITIVE CONTINUATION:\")\n",
    "        print(f'\"{result[\"positive_continuation\"]}\"')\n",
    "        \n",
    "        print(\"\\nüîÑ FULL TRANSITION:\")\n",
    "        print(f'\"{result[\"full_response\"]}\"')\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 60)\n",
    "\n",
    "# Display the results\n",
    "if transition_results:\n",
    "    display_transitions(transition_results, max_display=len(transition_results))\n",
    "else:\n",
    "    print(\"No transition results to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Control Vectors to GGUF Format\n",
    "\n",
    "Export the trained control vectors in GGUF format for use with other GGUF-compatible models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def export_control_vectors(control_vector: ControlVector, base_filename: str = \"sentiment_control\"):\n    \"\"\"\n    Export control vectors in GGUF format and save metadata.\n    \"\"\"\n    # Export to GGUF\n    gguf_path = f\"{base_filename}.gguf\"\n    \n    try:\n        control_vector.export_gguf(gguf_path)\n        print(f\"‚úì Control vector exported to {gguf_path}\")\n        \n        # Save metadata\n        metadata = {\n            \"model_type\": control_vector.model_type,\n            \"layers\": sorted(control_vector.directions.keys()),\n            \"training_method\": \"pca_center\",\n            \"base_model\": model_name,\n            \"positive_personas\": positive_personas,  # Updated to use RepEng personas\n            \"negative_personas\": negative_personas,  # Updated to use RepEng personas\n            \"description\": \"Control vector for sentiment steering from depressive to positive states\",\n            \"training_data_source\": \"RepEng all_truncated_outputs.json\",\n            \"training_approach\": \"Exact replication of RepEng emotion example methodology\"\n        }\n        \n        metadata_path = f\"{base_filename}_metadata.json\"\n        with open(metadata_path, 'w') as f:\n            json.dump(metadata, f, indent=2)\n        \n        print(f\"‚úì Metadata saved to {metadata_path}\")\n        \n        # File size info\n        if os.path.exists(gguf_path):\n            size_mb = os.path.getsize(gguf_path) / (1024 * 1024)\n            print(f\"‚úì GGUF file size: {size_mb:.2f} MB\")\n            \n        return gguf_path, metadata_path\n        \n    except Exception as e:\n        print(f\"‚úó Error exporting control vector: {e}\")\n        return None, None\n\n# Export the control vectors\ngguf_file, metadata_file = export_control_vectors(control_vector)\n\n# Test loading the exported vector\nif gguf_file and os.path.exists(gguf_file):\n    print(\"\\nTesting GGUF import...\")\n    try:\n        imported_vector = ControlVector.import_gguf(gguf_file)\n        print(f\"‚úì Successfully imported GGUF vector\")\n        print(f\"  Model type: {imported_vector.model_type}\")\n        print(f\"  Layers: {sorted(imported_vector.directions.keys())}\")\n        \n        # Verify vectors are identical\n        vectors_match = imported_vector == control_vector\n        print(f\"  Vectors match original: {vectors_match}\")\n        \n    except Exception as e:\n        print(f\"‚úó Error importing GGUF vector: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GGUF Model Integration Pipeline\n",
    "\n",
    "Provide a complete pipeline for using the exported vectors with GGUF models (conceptual implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gguf_integration_guide():\n",
    "    \"\"\"\n",
    "    Create a guide for using the exported vectors with GGUF models.\n",
    "    \"\"\"\n",
    "    guide = \"\"\"\n",
    "# GGUF Model Integration Guide\n",
    "\n",
    "This notebook has exported control vectors in GGUF format. Here's how to use them:\n",
    "\n",
    "## Files Created:\n",
    "- `sentiment_control.gguf`: The control vector in GGUF format\n",
    "- `sentiment_control_metadata.json`: Metadata about the vector\n",
    "- `sentiment_transitions.json`: Example transitions captured\n",
    "\n",
    "## Usage with GGUF Models:\n",
    "\n",
    "### 1. Loading the Control Vector:\n",
    "```python\n",
    "from repeng import ControlVector\n",
    "\n",
    "# Load the exported vector\n",
    "control_vector = ControlVector.import_gguf('sentiment_control.gguf')\n",
    "```\n",
    "\n",
    "### 2. Applying to Compatible Models:\n",
    "```python\n",
    "# For models with similar architecture to the training model\n",
    "from repeng import ControlModel\n",
    "\n",
    "# Load your GGUF model (conceptual - actual implementation may vary)\n",
    "model = load_gguf_model('your_model.gguf')\n",
    "control_model = ControlModel(model, list(range(-5, -18, -1)))\n",
    "\n",
    "# Apply control\n",
    "control_model.set_control(control_vector, strength=2.0)  # Positive\n",
    "control_model.set_control(control_vector, strength=-2.0) # Negative\n",
    "```\n",
    "\n",
    "### 3. Sentiment Transition Pipeline:\n",
    "```python\n",
    "def gguf_sentiment_transition(question, model, control_vector):\n",
    "    # 1. Generate with negative steering\n",
    "    model.set_control(control_vector, -2.0)\n",
    "    negative_start = model.generate(question, max_tokens=50)\n",
    "    \n",
    "    # 2. Continue with positive steering  \n",
    "    model.set_control(control_vector, 2.0)\n",
    "    positive_end = model.continue_generation(negative_start, max_tokens=50)\n",
    "    \n",
    "    return negative_start + positive_end\n",
    "```\n",
    "\n",
    "## Vector Characteristics:\n",
    "- Training Method: PCA Center (more precise than PCA diff)\n",
    "- Target Layers: -5 to -17 (relative to model end)\n",
    "- Effective Strength Range: -3.0 to +3.0\n",
    "- Best Results: Use -2.0 to -2.5 for depression, +1.5 to +2.5 for positivity\n",
    "\n",
    "## Notes:\n",
    "- Vectors work best with models similar to the training architecture\n",
    "- Fine-tune strength values for your specific model\n",
    "- Monitor for over-steering artifacts\n",
    "    \"\"\"\n",
    "    \n",
    "    with open('GGUF_Integration_Guide.md', 'w') as f:\n",
    "        f.write(guide.strip())\n",
    "    \n",
    "    print(\"üìã GGUF Integration Guide created: GGUF_Integration_Guide.md\")\n",
    "    print(guide)\n",
    "\n",
    "create_gguf_integration_guide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This notebook demonstrates the complete pipeline for capturing sentiment transitions using RepEng control vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_report(transition_results, control_vector):\n",
    "    \"\"\"\n",
    "    Create a summary report of the session.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"SESSION SUMMARY REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nüìä STATISTICS:\")\n",
    "    print(f\"  ‚Ä¢ Questions processed: {len(transition_results)}\")\n",
    "    print(f\"  ‚Ä¢ Control vector layers: {len(control_vector.directions)}\")\n",
    "    print(f\"  ‚Ä¢ Training method: PCA Center\")\n",
    "    print(f\"  ‚Ä¢ Base model: {model_name}\")\n",
    "    \n",
    "    if transition_results:\n",
    "        avg_transition_point = np.mean([r['transition_point'] for r in transition_results])\n",
    "        avg_total_words = np.mean([r['total_words'] for r in transition_results])\n",
    "        print(f\"  ‚Ä¢ Average transition point: {avg_transition_point:.1f} words\")\n",
    "        print(f\"  ‚Ä¢ Average response length: {avg_total_words:.1f} words\")\n",
    "    \n",
    "    print(f\"\\nüìÅ FILES CREATED:\")\n",
    "    files_created = [\n",
    "        \"sentiment_control.gguf (Control vector in GGUF format)\",\n",
    "        \"sentiment_control_metadata.json (Vector metadata)\", \n",
    "        \"sentiment_transitions.json (Captured transitions)\",\n",
    "        \"GGUF_Integration_Guide.md (Usage guide)\"\n",
    "    ]\n",
    "    \n",
    "    for file_desc in files_created:\n",
    "        print(f\"  ‚Ä¢ {file_desc}\")\n",
    "    \n",
    "    print(f\"\\nüéØ KEY ACHIEVEMENTS:\")\n",
    "    achievements = [\n",
    "        \"‚úì Trained control vectors using RepEng with PCA center method\",\n",
    "        \"‚úì Captured sentiment transitions from depressive to positive states\", \n",
    "        \"‚úì Exported vectors in GGUF format for broader compatibility\",\n",
    "        \"‚úì Processed cognitive pattern questions systematically\",\n",
    "        \"‚úì Created reusable pipeline for sentiment steering research\"\n",
    "    ]\n",
    "    \n",
    "    for achievement in achievements:\n",
    "        print(f\"  {achievement}\")\n",
    "    \n",
    "    print(f\"\\nüî¨ NEXT STEPS:\")\n",
    "    next_steps = [\n",
    "        \"1. Analyze transition patterns in downstream processing\",\n",
    "        \"2. Test exported vectors with different GGUF models\", \n",
    "        \"3. Fine-tune transition points for optimal effect\",\n",
    "        \"4. Scale up processing to full question dataset\",\n",
    "        \"5. Develop metrics for transition quality assessment\"\n",
    "    ]\n",
    "    \n",
    "    for step in next_steps:\n",
    "        print(f\"  {step}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SESSION COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Create the summary report\n",
    "create_summary_report(transition_results, control_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}