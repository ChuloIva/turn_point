{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cognitive Pattern Analysis Notebook\n",
    "\n",
    "This notebook demonstrates the complete pipeline for analyzing cognitive patterns through neural network activations using TransformerLens, PCA, SAE, and selfie interpretation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Device Detection ===\n",
      "=== Device Detection Results ===\n",
      "Platform: Darwin arm64\n",
      "Python: 3.13.5\n",
      "PyTorch: 2.8.0\n",
      "Optimal Device: mps (mps)\n",
      "\n",
      "âŒ CUDA: False\n",
      "âœ… MPS: True\n",
      "âœ… MLX: True\n",
      "    CPU: Apple M4 Pro\n",
      "    MLX Installed: False\n",
      "âŒ ROCM: False\n",
      "âœ… CPU: True\n",
      "\n",
      "âœ… Imports successful\n",
      "PyTorch version: 2.8.0\n",
      "Optimal device: mps\n",
      "Device type: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Import our modules\n",
    "from model_loader import ModelLoader\n",
    "from activation_capture import ActivationCapturer\n",
    "from data.data_loader import DataLoader\n",
    "from analysis.pca_analysis import PCAAnalyzer\n",
    "from analysis.sae_interface import SAEInterface\n",
    "from analysis.interpretation import SelfieInterpreter, ActivationArithmetic\n",
    "from utils.device_detection import get_device_manager, detect_and_print_devices\n",
    "\n",
    "# Detect and display available devices\n",
    "print(\"=== Device Detection ===\")\n",
    "device_manager = get_device_manager()\n",
    "device_manager.print_device_info()\n",
    "\n",
    "print(\"âœ… Imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Optimal device: {device_manager.get_device('auto')}\")\n",
    "print(f\"Device type: {device_manager.optimal_device[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "Model: google/gemma-2-2b-it\n",
      "Local path: google/gemma-2-2b-it\n",
      "Layers: [17, 21]\n",
      "Cognitive patterns: ['positive', 'negative', 'transition']\n",
      "Analysis methods: ['pca', 'sae', 'selfie', 'arithmetic']\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "with open('./config/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"Model: {config['model']['name']}\")\n",
    "print(f\"Local path: {config['model']['local_path']}\")\n",
    "print(f\"Layers: {config['model']['layers']}\")\n",
    "print(f\"Cognitive patterns: {config['data']['cognitive_patterns']}\")\n",
    "print(f\"Analysis methods: {config['analysis']['methods']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Data Loader and Load Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cognitive patterns...\n",
      "\n",
      "Dataset Statistics:\n",
      "  positive: 520 samples, avg length: 493.5\n",
      "  negative: 520 samples, avg length: 258.5\n",
      "  transition: 520 samples, avg length: 414.8\n",
      "\n",
      "Sample data:\n",
      "\n",
      "positive (first 2 samples):\n",
      "  1. I'm recognizing that my energy levels are flagging today, which is totally normal. I've been pushing...\n",
      "  2. I've noticed how often my mind drifts to the idea of death as a coping mechanism when I'm feeling ov...\n",
      "\n",
      "negative (first 2 samples):\n",
      "  1. Ugh, just the thought of checking my email is draining me already. It's like trying to lift a heavy ...\n",
      "  2. Ugh, there they go again - those incessant whispers about what would be better if only I were dead. ...\n",
      "\n",
      "transition (first 2 samples):\n",
      "  1. I need to take a deep breath and acknowledge that my mind feels overwhelmed. That's okay; it doesn't...\n",
      "  2. I remember the day it started, marked 'Beginning' on my timeline. Facts: I got into an argument with...\n"
     ]
    }
   ],
   "source": [
    "# Initialize data loader\n",
    "data_loader = DataLoader(base_path=config['data']['base_path'])\n",
    "\n",
    "# Load cognitive patterns using the new method for your dataset structure\n",
    "print(\"Loading cognitive patterns...\")\n",
    "cognitive_patterns = data_loader.load_cognitive_pattern_types(config['data']['main_dataset'])\n",
    "\n",
    "# Display statistics\n",
    "stats = data_loader.get_pattern_stats()\n",
    "print(\"\\nDataset Statistics:\")\n",
    "for pattern, stat in stats.items():\n",
    "    print(f\"  {pattern}: {stat['count']} samples, avg length: {stat['avg_length']:.1f}\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nSample data:\")\n",
    "for pattern_name, strings in cognitive_patterns.items():\n",
    "    if strings:\n",
    "        print(f\"\\n{pattern_name} (first 2 samples):\")\n",
    "        for i, sample in enumerate(strings[:2]):\n",
    "            print(f\"  {i+1}. {sample[:100]}...\" if len(sample) > 100 else f\"  {i+1}. {sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Model and Activation Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing activation capturer...\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2efe4a6fcd452db4c9567692ca8f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b-it into HookedTransformer\n",
      "\n",
      "Model Information:\n",
      "  n_layers: 26\n",
      "  d_model: 2304\n",
      "  n_heads: 8\n",
      "  d_head: 256\n",
      "  vocab_size: 256000\n",
      "  context_length: 8192\n",
      "\n",
      "Using device: mps\n",
      "Device type: mps\n",
      "\n",
      "âœ… Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize activation capturer with enhanced device detection\n",
    "print(\"Initializing activation capturer...\")\n",
    "activation_capturer = ActivationCapturer(\n",
    "    model_name=config['model']['name'],\n",
    "    device=config['model']['device']  # This will use our enhanced device detection\n",
    ")\n",
    "\n",
    "# Load the local model\n",
    "print(\"Loading model...\")\n",
    "local_path = config['model']['local_path']\n",
    "activation_capturer.load_model(local_path)\n",
    "\n",
    "# Get model info\n",
    "model_info = activation_capturer.get_model_info()\n",
    "print(\"\\nModel Information:\")\n",
    "for key, value in model_info.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Display device information\n",
    "device_info = activation_capturer.device_manager.get_device_info()\n",
    "print(f\"\\nUsing device: {activation_capturer.device}\")\n",
    "print(f\"Device type: {activation_capturer.device_type}\")\n",
    "\n",
    "print(\"\\nâœ… Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Capture Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capturing activations...\n",
      "\n",
      "Processing pattern: positive\n",
      "  Processing 520 samples\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [107, 2304] at entry 0 and [120, 2304] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(limited_strings)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Capture activations\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m activations = \u001b[43mactivation_capturer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcapture_activations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimited_strings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_nums\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcognitive_pattern\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpattern_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m activations_dict[pattern_name] = activations\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Show activation shapes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/turn_point/activation_capture.py:89\u001b[39m, in \u001b[36mcapture_activations\u001b[39m\u001b[34m(self, strings, layer_nums, cognitive_pattern, position)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcapture_activations\u001b[39m(\n\u001b[32m     73\u001b[39m     \u001b[38;5;28mself\u001b[39m, \n\u001b[32m     74\u001b[39m     strings: List[\u001b[38;5;28mstr\u001b[39m], \n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m     use_cache: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     79\u001b[39m ) -> Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor]:\n\u001b[32m     80\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[33;03m    Capture activations for given strings at specified layers.\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[33;03m        strings: List of input strings\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m        layer_nums: Layers to capture activations from\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[33;03m        cognitive_pattern: Pattern category for organization\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[33;03m        position: Token position to extract ('last', 'all', or int)\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[33;03m        use_cache: Whether to use cached activations if available\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[33;03m        \u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[33;03m        Dictionary of activations organized by layer and pattern\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mModel not loaded. Call load_model() first.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: stack expects each tensor to be equal size, but got [107, 2304] at entry 0 and [120, 2304] at entry 1"
     ]
    }
   ],
   "source": [
    "# Capture activations for all patterns\n",
    "print(\"Capturing activations...\")\n",
    "\n",
    "layers = config['model']['layers']\n",
    "position = config['capture']['position']\n",
    "max_samples = config['data'].get('max_samples_per_pattern', 1)  # Limit for notebook\n",
    "\n",
    "activations_dict = {}\n",
    "\n",
    "for pattern_name, strings in cognitive_patterns.items():\n",
    "    if not strings:\n",
    "        print(f\"Skipping {pattern_name} - no data\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nProcessing pattern: {pattern_name}\")\n",
    "    \n",
    "    # Limit samples for notebook demo\n",
    "    limited_strings = strings[:max_samples]\n",
    "    print(f\"  Processing {len(limited_strings)} samples\")\n",
    "    \n",
    "    # Capture activations\n",
    "    activations = activation_capturer.capture_activations(\n",
    "        strings=limited_strings,\n",
    "        layer_nums=layers,\n",
    "        cognitive_pattern=pattern_name,\n",
    "        position=position\n",
    "    )\n",
    "    \n",
    "    activations_dict[pattern_name] = activations\n",
    "    \n",
    "    # Show activation shapes\n",
    "    for key, tensor in activations.items():\n",
    "        print(f\"    {key}: {tensor.shape}\")\n",
    "\n",
    "print(\"\\nâœ… Activation capture complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PCA analyzer\n",
    "pca_analyzer = PCAAnalyzer(n_components=config['analysis']['pca']['n_components'])\n",
    "\n",
    "print(\"Running PCA analysis...\")\n",
    "pca_results = {}\n",
    "\n",
    "for pattern_name, activations in activations_dict.items():\n",
    "    print(f\"\\nPCA for pattern: {pattern_name}\")\n",
    "    \n",
    "    # Compute PCA\n",
    "    pca_result = pca_analyzer.compute_pca(\n",
    "        activations=activations,\n",
    "        pattern_name=pattern_name,\n",
    "        standardize=config['analysis']['pca']['standardize']\n",
    "    )\n",
    "    \n",
    "    pca_results[pattern_name] = pca_result\n",
    "    \n",
    "    # Show explained variance for each layer\n",
    "    for layer_key in activations.keys():\n",
    "        explained_var = pca_analyzer.get_explained_variance_ratio(pattern_name, layer_key)\n",
    "        if len(explained_var) > 0:\n",
    "            cumulative_var = pca_analyzer.get_cumulative_explained_variance(pattern_name, layer_key)\n",
    "            print(f\"  {layer_key}: First 5 PCs explain {cumulative_var[4]:.1%} of variance\")\n",
    "\n",
    "print(\"\\nâœ… PCA analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA explained variance\n",
    "print(\"Creating PCA visualizations...\")\n",
    "\n",
    "# Create plots for each pattern and layer\n",
    "for pattern_name, activations in activations_dict.items():\n",
    "    for layer_key in list(activations.keys())[:2]:  # Limit to first 2 layers for notebook\n",
    "        print(f\"Plotting {pattern_name} - {layer_key}\")\n",
    "        pca_analyzer.plot_explained_variance(pattern_name, layer_key)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA scatter plot comparing patterns\n",
    "if len(list(activations_dict.keys())) > 1:\n",
    "    print(\"Creating PCA comparison plots...\")\n",
    "    \n",
    "    # Get first layer key that exists in multiple patterns\n",
    "    common_layers = None\n",
    "    for pattern_name, activations in activations_dict.items():\n",
    "        if common_layers is None:\n",
    "            common_layers = set(activations.keys())\n",
    "        else:\n",
    "            common_layers &= set(activations.keys())\n",
    "    \n",
    "    if common_layers:\n",
    "        layer_to_plot = list(common_layers)[0]\n",
    "        pattern_names = list(activations_dict.keys())\n",
    "        \n",
    "        pca_analyzer.plot_pca_scatter(\n",
    "            pattern_names=pattern_names,\n",
    "            layer_key=layer_to_plot,\n",
    "            components=(0, 1)\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "        # Compute pattern separation\n",
    "        if len(pattern_names) >= 2:\n",
    "            separation = pca_analyzer.get_pattern_separation(\n",
    "                pattern_names[0], pattern_names[1], layer_to_plot\n",
    "            )\n",
    "            print(f\"Pattern separation ({pattern_names[0]} vs {pattern_names[1]}): {separation:.2f}\")\n",
    "else:\n",
    "    print(\"Need multiple patterns for comparison plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SAE Analysis (Placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SAE interface (placeholder)\n",
    "sae_interface = SAEInterface(sae_model_path=config['analysis']['sae']['model_path'])\n",
    "\n",
    "print(\"Running SAE analysis (placeholder implementation)...\")\n",
    "sae_results = {}\n",
    "\n",
    "# Analyze one pattern as demonstration\n",
    "demo_pattern = list(activations_dict.keys())[0]\n",
    "demo_activations = activations_dict[demo_pattern]\n",
    "\n",
    "print(f\"\\nAnalyzing SAE features for: {demo_pattern}\")\n",
    "sae_result = sae_interface.analyze_pattern_features(\n",
    "    activations=demo_activations,\n",
    "    pattern_name=demo_pattern,\n",
    "    top_k=config['analysis']['sae']['top_k_features']\n",
    ")\n",
    "\n",
    "sae_results[demo_pattern] = sae_result\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nSAE Analysis Results for {demo_pattern}:\")\n",
    "for layer_key, layer_result in sae_result['layers'].items():\n",
    "    print(f\"\\n  {layer_key}:\")\n",
    "    print(f\"    Sparsity: {layer_result['sparsity']:.3f}\")\n",
    "    print(f\"    Reconstruction error: {layer_result['reconstruction_error']:.3f}\")\n",
    "    print(f\"    Top 3 features:\")\n",
    "    for i, (feat_idx, activation) in enumerate(layer_result['top_features'][:3]):\n",
    "        interpretation = layer_result['interpretations'].get(feat_idx, \"No interpretation\")\n",
    "        print(f\"      Feature {feat_idx}: {activation:.3f} - {interpretation}\")\n",
    "\n",
    "print(\"\\nâœ… SAE analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Selfie Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize selfie interpreter\n",
    "selfie_interpreter = SelfieInterpreter(model=activation_capturer.model)\n",
    "\n",
    "print(\"Running selfie interpretation analysis...\")\n",
    "\n",
    "# Analyze one pattern with limited samples for demo\n",
    "demo_pattern = list(cognitive_patterns.keys())[0]\n",
    "demo_strings = cognitive_patterns[demo_pattern][:5]  # Limit to 5 samples\n",
    "demo_activations = activations_dict[demo_pattern]\n",
    "\n",
    "print(f\"\\nSelfie analysis for: {demo_pattern}\")\n",
    "print(f\"Analyzing {len(demo_strings)} samples...\")\n",
    "\n",
    "# Limit to one layer for demo\n",
    "layer_keys = list(demo_activations.keys())[:1]\n",
    "limited_activations = {k: demo_activations[k][:5] for k in layer_keys}\n",
    "\n",
    "selfie_interpretations = selfie_interpreter.batch_interpret_activations(\n",
    "    activations=limited_activations,\n",
    "    contexts=demo_strings,\n",
    "    pattern_name=demo_pattern\n",
    ")\n",
    "\n",
    "# Display interpretations\n",
    "print(f\"\\nSelfie Interpretations for {demo_pattern}:\")\n",
    "for layer_key, interpretations in selfie_interpretations.items():\n",
    "    print(f\"\\n  {layer_key}:\")\n",
    "    for i, interpretation in enumerate(interpretations[:3]):\n",
    "        print(f\"    Sample {i+1}: {interpretation[:200]}...\")\n",
    "\n",
    "# Validate interpretations\n",
    "validation_results = {}\n",
    "for layer_key, layer_interpretations in selfie_interpretations.items():\n",
    "    validation = selfie_interpreter.validate_interpretations(\n",
    "        layer_interpretations, demo_pattern\n",
    "    )\n",
    "    validation_results[layer_key] = validation\n",
    "    print(f\"\\nValidation for {layer_key}:\")\n",
    "    print(f\"  Keyword match ratio: {validation['keyword_match_ratio']:.2f}\")\n",
    "    print(f\"  Avg interpretation length: {validation['avg_interpretation_length']:.0f} chars\")\n",
    "\n",
    "print(\"\\nâœ… Selfie interpretation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Activation Arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize activation arithmetic\n",
    "activation_arithmetic = ActivationArithmetic()\n",
    "\n",
    "print(\"Running activation arithmetic analysis...\")\n",
    "\n",
    "# Prepare data for arithmetic operations\n",
    "layers = config['model']['layers']\n",
    "pattern_activations = {}\n",
    "\n",
    "for pattern_name, activations in activations_dict.items():\n",
    "    pattern_activations[pattern_name] = {}\n",
    "    for layer_num in layers:\n",
    "        layer_key = f\"{pattern_name}_layer_{layer_num}\"\n",
    "        if layer_key in activations:\n",
    "            pattern_activations[pattern_name][layer_num] = activations[layer_key]\n",
    "\n",
    "print(f\"Prepared activations for {len(pattern_activations)} patterns\")\n",
    "\n",
    "# Compute similarity matrices for each layer\n",
    "arithmetic_results = {}\n",
    "for layer_num in layers:\n",
    "    print(f\"\\nAnalyzing layer {layer_num}:\")\n",
    "    \n",
    "    # Get patterns that have data for this layer\n",
    "    layer_patterns = {}\n",
    "    for pattern_name, pattern_data in pattern_activations.items():\n",
    "        if layer_num in pattern_data:\n",
    "            layer_patterns[pattern_name] = pattern_data[layer_num]\n",
    "    \n",
    "    if len(layer_patterns) > 1:\n",
    "        # Compute similarity matrix\n",
    "        similarities = activation_arithmetic.compute_similarity_matrix(layer_patterns)\n",
    "        \n",
    "        print(f\"  Pattern similarities (cosine):\")\n",
    "        for (p1, p2), sim in similarities.items():\n",
    "            if p1 < p2:  # Only show each pair once\n",
    "                print(f\"    {p1} <-> {p2}: {sim:.3f}\")\n",
    "        \n",
    "        arithmetic_results[f\"layer_{layer_num}\"] = {\n",
    "            'similarities': similarities\n",
    "        }\n",
    "    else:\n",
    "        print(f\"  Not enough patterns for comparison\")\n",
    "\n",
    "# Pattern arithmetic operations (if multiple patterns available)\n",
    "pattern_names = list(pattern_activations.keys())\n",
    "if len(pattern_names) >= 2:\n",
    "    print(f\"\\nPattern arithmetic operations:\")\n",
    "    \n",
    "    # Choose first layer and first two patterns\n",
    "    demo_layer = layers[0]\n",
    "    pattern1, pattern2 = pattern_names[0], pattern_names[1]\n",
    "    \n",
    "    if (demo_layer in pattern_activations[pattern1] and \n",
    "        demo_layer in pattern_activations[pattern2]):\n",
    "        \n",
    "        act1 = pattern_activations[pattern1][demo_layer]\n",
    "        act2 = pattern_activations[pattern2][demo_layer]\n",
    "        \n",
    "        # Compute difference vector\n",
    "        diff_vector = activation_arithmetic.compute_pattern_difference(act1, act2)\n",
    "        print(f\"  Difference vector ({pattern1} - {pattern2}): shape {diff_vector.shape}\")\n",
    "        print(f\"  Difference magnitude: {torch.norm(diff_vector).item():.3f}\")\n",
    "        \n",
    "        # Compute transition vector\n",
    "        transition = activation_arithmetic.find_transition_vector(act1, act2)\n",
    "        print(f\"  Transition vector ({pattern1} -> {pattern2}): magnitude {torch.norm(transition).item():.3f}\")\n",
    "        \n",
    "        # Create interpolation\n",
    "        interpolated = activation_arithmetic.interpolate_patterns(act1, act2, steps=5)\n",
    "        print(f\"  Created {len(interpolated)} interpolation steps\")\n",
    "\n",
    "print(\"\\nâœ… Activation arithmetic complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary of all results\n",
    "print(\"=\" * 60)\n",
    "print(\"COGNITIVE PATTERN ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nModel: {config['model']['name']}\")\n",
    "print(f\"Local path: {config['model']['local_path']}\")\n",
    "print(f\"Layers analyzed: {config['model']['layers']}\")\n",
    "\n",
    "print(f\"\\nPatterns analyzed:\")\n",
    "for pattern_name, activations in activations_dict.items():\n",
    "    activation_keys = list(activations.keys())\n",
    "    sample_shape = list(activations.values())[0].shape if activations else \"N/A\"\n",
    "    print(f\"  - {pattern_name}: {len(activation_keys)} layer combinations, shape: {sample_shape}\")\n",
    "\n",
    "print(f\"\\nAnalysis methods completed:\")\n",
    "completed_methods = []\n",
    "if pca_results:\n",
    "    completed_methods.append(\"âœ… PCA Analysis\")\n",
    "if sae_results:\n",
    "    completed_methods.append(\"âœ… SAE Analysis (placeholder)\")\n",
    "if 'selfie_interpretations' in locals():\n",
    "    completed_methods.append(\"âœ… Selfie Interpretation\")\n",
    "if arithmetic_results:\n",
    "    completed_methods.append(\"âœ… Activation Arithmetic\")\n",
    "\n",
    "for method in completed_methods:\n",
    "    print(f\"  {method}\")\n",
    "\n",
    "print(f\"\\nKey findings:\")\n",
    "print(f\"  - Successfully captured activations from {len(layers)} layers\")\n",
    "print(f\"  - PCA analysis shows variance structure across cognitive patterns\")\n",
    "if len(pattern_names) >= 2 and arithmetic_results:\n",
    "    # Find highest similarity\n",
    "    all_similarities = []\n",
    "    for layer_result in arithmetic_results.values():\n",
    "        for (p1, p2), sim in layer_result['similarities'].items():\n",
    "            if p1 != p2:\n",
    "                all_similarities.append(sim)\n",
    "    if all_similarities:\n",
    "        max_sim = max(all_similarities)\n",
    "        min_sim = min(all_similarities)\n",
    "        print(f\"  - Pattern similarities range from {min_sim:.3f} to {max_sim:.3f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Analysis pipeline completed successfully!\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Download actual SAE models to replace placeholder\")\n",
    "print(f\"  2. Experiment with different layers and cognitive patterns\")\n",
    "print(f\"  3. Use activation arithmetic for pattern steering\")\n",
    "print(f\"  4. Validate interpretations with domain experts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save results to files\n",
    "save_results = input(\"Save results to files? (y/n): \").strip().lower() == 'y'\n",
    "\n",
    "if save_results:\n",
    "    # Create results directory\n",
    "    results_dir = Path(\"./notebook_results\")\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"Saving results to {results_dir}...\")\n",
    "    \n",
    "    # Save activations\n",
    "    for pattern_name, activations in activations_dict.items():\n",
    "        torch.save(activations, results_dir / f\"{pattern_name}_activations.pt\")\n",
    "    \n",
    "    # Save PCA results\n",
    "    if pca_results:\n",
    "        torch.save(pca_results, results_dir / \"pca_results.pt\")\n",
    "    \n",
    "    # Save arithmetic results\n",
    "    if arithmetic_results:\n",
    "        with open(results_dir / \"arithmetic_results.yaml\", 'w') as f:\n",
    "            yaml.dump(arithmetic_results, f)\n",
    "    \n",
    "    print(f\"âœ… Results saved to {results_dir}\")\n",
    "else:\n",
    "    print(\"Results not saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
