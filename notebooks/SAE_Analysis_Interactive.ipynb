{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAE Analysis: Cognitive Patterns ‚Üí Features ‚Üí Neuronpedia\n",
    "\n",
    "This notebook demonstrates how to analyze your pre-computed activations using Sparse Autoencoders (SAEs) and explore the results through Neuronpedia.\n",
    "\n",
    "## Overview\n",
    "1. **Load SAE**: Find and load appropriate SAE for your model/layer\n",
    "2. **Process Activations**: Run your activations through the SAE\n",
    "3. **Find Top Features**: Identify most active features\n",
    "4. **Neuronpedia Integration**: View feature interpretations\n",
    "5. **Advanced Analysis**: Steering, ablation, comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install sae-lens transformer-lens plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from IPython.display import IFrame, display, HTML\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Import our SAE analysis module\n",
    "from sae_analysis import SAEActivationAnalyzer, SAEAnalysisConfig\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {'MPS' if torch.backends.mps.is_available() else 'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure analysis\n",
    "config = SAEAnalysisConfig(\n",
    "    device=\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    sae_release=\"gpt2-small-res-jb\",  # Start with GPT-2 SAEs\n",
    "    sae_id=\"blocks.7.hook_resid_pre\",   # Layer 7 residual stream\n",
    "    top_k_features=20,\n",
    "    analysis_output_dir=\"sae_analysis_results\"\n",
    ")\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = SAEActivationAnalyzer(config)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Device: {config.device}\")\n",
    "print(f\"  SAE: {config.sae_release}/{config.sae_id}\")\n",
    "print(f\"  Output directory: {config.analysis_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Discover Available SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover all available SAEs\n",
    "available_saes = analyzer.discover_available_saes()\n",
    "\n",
    "# Show available models\n",
    "model_counts = available_saes['model_name'].value_counts()\n",
    "print(\"Available SAE models:\")\n",
    "for model, count in model_counts.head(10).items():\n",
    "    print(f\"  {model}: {count} SAEs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find SAEs for specific models\n",
    "gpt2_saes = analyzer.find_matching_saes(\"gpt2\", \"resid\")\n",
    "display(gpt2_saes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SAE\n",
    "sae = analyzer.load_sae()\n",
    "\n",
    "print(f\"SAE Configuration:\")\n",
    "print(f\"  Hook name: {sae.cfg.hook_name}\")\n",
    "print(f\"  Input dims (d_in): {sae.cfg.d_in}\")\n",
    "print(f\"  SAE dims (d_sae): {sae.cfg.d_sae}\")\n",
    "print(f\"  Expansion factor: {sae.cfg.d_sae / sae.cfg.d_in:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load Your Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find your activation files\n",
    "activation_files = list(Path(\"activations\").glob(\"*.pt\"))\n",
    "print(f\"Found {len(activation_files)} activation files:\")\n",
    "for i, file_path in enumerate(activation_files):\n",
    "    print(f\"  {i}: {file_path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which activation file to analyze\n",
    "file_index = 0  # Change this to select different files\n",
    "activation_path = activation_files[file_index]\n",
    "\n",
    "print(f\"Loading: {activation_path}\")\n",
    "activations = analyzer.load_activations(str(activation_path))\n",
    "\n",
    "print(f\"Activation statistics:\")\n",
    "print(f\"  Shape: {activations.shape}\")\n",
    "print(f\"  Device: {activations.device}\")\n",
    "print(f\"  Dtype: {activations.dtype}\")\n",
    "print(f\"  Min/Max: {activations.min():.4f} / {activations.max():.4f}\")\n",
    "print(f\"  Mean/Std: {activations.mean():.4f} ¬± {activations.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle dimension compatibility\n",
    "original_shape = activations.shape\n",
    "actual_dim = activations.shape[-1]\n",
    "expected_dim = sae.cfg.d_in\n",
    "\n",
    "print(f\"Dimension compatibility check:\")\n",
    "print(f\"  Activation dims: {actual_dim}\")\n",
    "print(f\"  SAE expects: {expected_dim}\")\n",
    "\n",
    "if actual_dim != expected_dim:\n",
    "    print(f\"  ‚ö†Ô∏è  Dimension mismatch detected!\")\n",
    "    \n",
    "    if actual_dim > expected_dim:\n",
    "        print(f\"  Truncating to first {expected_dim} dimensions\")\n",
    "        activations = activations[..., :expected_dim]\n",
    "    else:\n",
    "        print(f\"  Padding to {expected_dim} dimensions with zeros\")\n",
    "        padding_shape = list(activations.shape)\n",
    "        padding_shape[-1] = expected_dim - actual_dim\n",
    "        padding = torch.zeros(padding_shape, device=activations.device, dtype=activations.dtype)\n",
    "        activations = torch.cat([activations, padding], dim=-1)\n",
    "        \n",
    "    print(f\"  New shape: {activations.shape}\")\n",
    "else:\n",
    "    print(f\"  ‚úÖ Dimensions compatible!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Process Through SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process activations through SAE\n",
    "results = analyzer.process_activations(activations)\n",
    "\n",
    "print(\"Processing results:\")\n",
    "for key, tensor in results.items():\n",
    "    if isinstance(tensor, torch.Tensor):\n",
    "        print(f\"  {key}: {tensor.shape} - {tensor.dtype}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(tensor)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Find Top Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top activating features\n",
    "values, indices = analyzer.find_top_features(results['feature_activations'], \n",
    "                                           position=-1,  # Last token position\n",
    "                                           top_k=config.top_k_features)\n",
    "\n",
    "# Create a dataframe for easier viewing\n",
    "top_features_df = pd.DataFrame({\n",
    "    'rank': range(1, len(values) + 1),\n",
    "    'feature_idx': indices.cpu().numpy(),\n",
    "    'activation_value': values.cpu().numpy()\n",
    "})\n",
    "\n",
    "display(top_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Feature Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig = analyzer.visualize_feature_activations(\n",
    "    results['feature_activations'], \n",
    "    position=-1,\n",
    "    title=f\"Feature Analysis: {activation_path.name}\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional visualization: Top features bar chart\n",
    "fig_bar = go.Figure(data=[\n",
    "    go.Bar(\n",
    "        x=[f\"F{idx}\" for idx in indices[:10].cpu()],\n",
    "        y=values[:10].cpu(),\n",
    "        text=[f\"{val:.3f}\" for val in values[:10].cpu()],\n",
    "        textposition='outside'\n",
    "    )\n",
    "])\n",
    "\n",
    "fig_bar.update_layout(\n",
    "    title=\"Top 10 Feature Activations\",\n",
    "    xaxis_title=\"Feature Index\",\n",
    "    yaxis_title=\"Activation Value\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig_bar.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Neuronpedia Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Neuronpedia URLs for top features\n",
    "print(\"Neuronpedia Dashboard URLs:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i, (val, idx) in enumerate(zip(values[:10], indices[:10])):\n",
    "    url = analyzer.get_neuronpedia_dashboard_url(int(idx))\n",
    "    print(f\"{i+1:2d}. Feature {int(idx):4d} (activation: {float(val):6.4f})\")\n",
    "    print(f\"    {url}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display interactive Neuronpedia dashboards for top 3 features\n",
    "print(\"Interactive Neuronpedia Dashboards:\")\n",
    "\n",
    "for i in range(min(3, len(indices))):\n",
    "    feature_idx = int(indices[i])\n",
    "    activation_val = float(values[i])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Feature {feature_idx} - Activation: {activation_val:.4f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create and display iframe\n",
    "    dashboard = analyzer.display_feature_dashboard(feature_idx, width=1200, height=400)\n",
    "    if dashboard:\n",
    "        display(dashboard)\n",
    "    else:\n",
    "        url = analyzer.get_neuronpedia_dashboard_url(feature_idx)\n",
    "        display(HTML(f'<a href=\"{url}\" target=\"_blank\">Open Feature {feature_idx} in new tab</a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Download and Search Feature Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download feature explanations\n",
    "try:\n",
    "    explanations = analyzer.download_feature_explanations(\"gpt2-small\", \"7-res-jb\")\n",
    "    if explanations is not None:\n",
    "        print(f\"Successfully downloaded {len(explanations)} feature explanations\")\n",
    "        print(\"\\nSample explanations:\")\n",
    "        display(explanations[['feature', 'description']].head())\n",
    "    else:\n",
    "        print(\"Could not download explanations\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading explanations: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for features related to cognitive patterns\n",
    "if analyzer.feature_explanations is not None:\n",
    "    search_terms = [\n",
    "        \"thought\", \"thinking\", \"cognitive\", \"mental\", \"mind\", \n",
    "        \"emotion\", \"feeling\", \"pattern\", \"behavior\", \"psychology\",\n",
    "        \"negative\", \"positive\", \"anxiety\", \"depression\", \"stress\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Searching for cognitively relevant features:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    all_matches = []\n",
    "    for term in search_terms:\n",
    "        matches = analyzer.search_features_by_description(term)\n",
    "        if matches is not None and len(matches) > 0:\n",
    "            print(f\"\\n'{term}': {len(matches)} matches\")\n",
    "            if len(matches) > 0:\n",
    "                example = matches.iloc[0]\n",
    "                print(f\"  Example: Feature {example['feature']} - {example['description'][:100]}...\")\n",
    "                all_matches.extend(matches['feature'].tolist())\n",
    "    \n",
    "    # Check if any of your top features match cognitive patterns\n",
    "    top_feature_indices = [int(idx) for idx in indices[:10]]\n",
    "    cognitive_matches = [f for f in top_feature_indices if f in all_matches]\n",
    "    \n",
    "    if cognitive_matches:\n",
    "        print(f\"\\nüéØ Cognitive pattern matches in your top features:\")\n",
    "        for feature_idx in cognitive_matches:\n",
    "            explanation_row = analyzer.feature_explanations[\n",
    "                analyzer.feature_explanations['feature'] == feature_idx\n",
    "            ]\n",
    "            if not explanation_row.empty:\n",
    "                desc = explanation_row.iloc[0]['description']\n",
    "                rank = top_feature_indices.index(feature_idx) + 1\n",
    "                print(f\"  Rank {rank} - Feature {feature_idx}: {desc}\")\n",
    "else:\n",
    "    print(\"No explanations available for searching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Advanced Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Ablation Analysis\n",
    "print(\"Feature Ablation Analysis\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Ablate top 3 features\n",
    "top_3_features = [int(idx) for idx in indices[:3]]\n",
    "print(f\"Ablating features: {top_3_features}\")\n",
    "\n",
    "ablated_reconstructions = analyzer.perform_feature_ablation(\n",
    "    results['feature_activations'], \n",
    "    top_3_features\n",
    ")\n",
    "\n",
    "# Compare reconstruction quality\n",
    "original_error = torch.nn.functional.mse_loss(activations, results['reconstructions'])\n",
    "ablated_error = torch.nn.functional.mse_loss(activations, ablated_reconstructions)\n",
    "error_increase = ablated_error - original_error\n",
    "\n",
    "print(f\"\\nReconstruction Analysis:\")\n",
    "print(f\"  Original MSE: {original_error:.6f}\")\n",
    "print(f\"  Ablated MSE:  {ablated_error:.6f}\")\n",
    "print(f\"  Error increase: {error_increase:.6f} ({error_increase/original_error*100:.1f}% worse)\")\n",
    "\n",
    "if error_increase > 0.001:  # Significant increase\n",
    "    print(f\"  üîç These features appear important for reconstruction!\")\n",
    "else:\n",
    "    print(f\"  üí≠ These features may be less critical for reconstruction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Steering Analysis\n",
    "print(\"Feature Steering Analysis\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Create steering interventions\n",
    "steering_configs = [\n",
    "    (int(indices[0]), 2.0),   # Amplify top feature\n",
    "    (int(indices[1]), -1.0),  # Suppress second feature\n",
    "    (int(indices[2]), 1.5)    # Moderately amplify third feature\n",
    "]\n",
    "\n",
    "print(f\"Steering interventions:\")\n",
    "for feature_idx, strength in steering_configs:\n",
    "    direction = \"Amplify\" if strength > 0 else \"Suppress\"\n",
    "    print(f\"  Feature {feature_idx}: {direction} by {abs(strength):.1f}x\")\n",
    "\n",
    "# Apply steering\n",
    "steered_activations = analyzer.apply_steering(activations, steering_configs)\n",
    "steered_results = analyzer.process_activations(steered_activations)\n",
    "\n",
    "# Compare before/after\n",
    "print(f\"\\nBefore/After Steering Comparison:\")\n",
    "print(f\"  Original avg L0:  {results['l0_norm'].mean():.2f}\")\n",
    "print(f\"  Steered avg L0:   {steered_results['l0_norm'].mean():.2f}\")\n",
    "print(f\"  Change in sparsity: {steered_results['l0_norm'].mean() - results['l0_norm'].mean():.2f}\")\n",
    "\n",
    "# Find top features after steering\n",
    "steered_values, steered_indices = analyzer.find_top_features(\n",
    "    steered_results['feature_activations'], top_k=10\n",
    ")\n",
    "\n",
    "print(f\"\\nTop features after steering:\")\n",
    "for i, (val, idx) in enumerate(zip(steered_values[:5], steered_indices[:5])):\n",
    "    original_rank = \"NEW\" if int(idx) not in indices[:10].tolist() else f\"#{indices[:10].tolist().index(int(idx)) + 1}\"\n",
    "    print(f\"  {i+1}. Feature {int(idx)} ({original_rank}): {float(val):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative visualization\n",
    "fig = analyzer.compare_activations(\n",
    "    results['feature_activations'][0, -1, :], \n",
    "    steered_results['feature_activations'][0, -1, :],\n",
    "    labels=(\"Original\", \"Steered\")\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Save Results and Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive results\n",
    "timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "file_stem = activation_path.stem\n",
    "\n",
    "# Save analysis results\n",
    "results_filename = f\"analysis_results_{file_stem}_{timestamp}.pt\"\n",
    "results_path = analyzer.save_analysis_results(results, results_filename)\n",
    "\n",
    "# Generate report\n",
    "report_filename = f\"analysis_report_{file_stem}_{timestamp}.md\"\n",
    "report_path = analyzer.generate_analysis_report(results, (values, indices), report_filename)\n",
    "\n",
    "# Create interactive summary\n",
    "summary = {\n",
    "    \"metadata\": {\n",
    "        \"activation_file\": str(activation_path),\n",
    "        \"original_shape\": list(original_shape),\n",
    "        \"processed_shape\": list(activations.shape),\n",
    "        \"analysis_timestamp\": timestamp,\n",
    "        \"sae_config\": {\n",
    "            \"release\": config.sae_release,\n",
    "            \"sae_id\": config.sae_id,\n",
    "            \"d_in\": sae.cfg.d_in,\n",
    "            \"d_sae\": sae.cfg.d_sae,\n",
    "            \"hook_name\": sae.cfg.hook_name\n",
    "        }\n",
    "    },\n",
    "    \"statistics\": {\n",
    "        \"avg_sparsity\": float(results['l0_norm'].mean()),\n",
    "        \"avg_reconstruction_error\": float(results['reconstruction_error'].mean()),\n",
    "        \"activation_range\": {\n",
    "            \"min\": float(activations.min()),\n",
    "            \"max\": float(activations.max()),\n",
    "            \"mean\": float(activations.mean()),\n",
    "            \"std\": float(activations.std())\n",
    "        }\n",
    "    },\n",
    "    \"top_features\": [\n",
    "        {\n",
    "            \"rank\": i + 1,\n",
    "            \"feature_idx\": int(idx),\n",
    "            \"activation_value\": float(val),\n",
    "            \"neuronpedia_url\": analyzer.get_neuronpedia_dashboard_url(int(idx))\n",
    "        }\n",
    "        for i, (val, idx) in enumerate(zip(values, indices))\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Add explanations if available\n",
    "if analyzer.feature_explanations is not None:\n",
    "    for feature_data in summary[\"top_features\"]:\n",
    "        feature_idx = feature_data[\"feature_idx\"]\n",
    "        explanation_row = analyzer.feature_explanations[\n",
    "            analyzer.feature_explanations['feature'] == feature_idx\n",
    "        ]\n",
    "        if not explanation_row.empty:\n",
    "            feature_data[\"description\"] = explanation_row.iloc[0]['description']\n",
    "\n",
    "# Save summary\n",
    "summary_path = Path(config.analysis_output_dir) / f\"interactive_summary_{file_stem}_{timestamp}.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"Analysis Complete! üéâ\")\n",
    "print(f\"\\nGenerated files:\")\n",
    "print(f\"  üìä Results: {results_path.name}\")\n",
    "print(f\"  üìù Report: {report_path.name}\")\n",
    "print(f\"  üìã Summary: {summary_path.name}\")\n",
    "print(f\"\\nAll files saved in: {config.analysis_output_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We Accomplished:\n",
    "1. ‚úÖ Loaded a pre-trained SAE matching your model architecture\n",
    "2. ‚úÖ Processed your pre-computed activations through the SAE  \n",
    "3. ‚úÖ Identified the most active features for your cognitive patterns\n",
    "4. ‚úÖ Generated Neuronpedia dashboard links for feature interpretation\n",
    "5. ‚úÖ Performed advanced analysis (ablation, steering)\n",
    "6. ‚úÖ Saved comprehensive results and generated reports\n",
    "\n",
    "### Key Insights:\n",
    "- Your activations were successfully processed through the SAE\n",
    "- Identified top active features that may relate to cognitive patterns\n",
    "- Generated interpretable visualizations and Neuronpedia links\n",
    "- Demonstrated feature manipulation techniques\n",
    "\n",
    "### Next Steps:\n",
    "1. **Explore Neuronpedia**: Click the dashboard URLs to understand what each feature represents\n",
    "2. **Compare Patterns**: Run this analysis on different activation files to compare cognitive patterns\n",
    "3. **Feature Analysis**: Investigate which features consistently activate across similar cognitive patterns\n",
    "4. **Model Understanding**: Use steering/ablation to understand which features are most important\n",
    "5. **Research Applications**: Use these insights to understand the neural basis of cognitive transformations\n",
    "\n",
    "### Troubleshooting:\n",
    "- If Neuronpedia dashboards don't load, try opening the URLs directly in a new browser tab\n",
    "- For dimension mismatches, adjust the SAE selection or activation preprocessing\n",
    "- If explanations fail to download, the Neuronpedia API might be temporarily unavailable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}